[{"content":"In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.\nCLIP -\u0026gt; Alec Radford, et al. NeuraIPS 2021\nThe model architecture of CLIP (source from paper)\nCLIP has a quite simple design which consists of a joint-training image encoder and text encoder, but there are still some interesting details worth noting.\n1.1 Bag-of-words encoding In the text encoder, the output at last [EOS] token are used as the feature representation of the text (i.e. $[N,L]\\to[N,L,D]\\to[N,D]$). The authors refer to this process as \u0026ldquo;Bag-of-words Encoding\u0026rdquo; where the text of arbitrary length can be encoded into a D-dimensional vector.\nIn Stable Diffusion, CLIP\u0026rsquo;s text encoder is used to provide crucial text feature. People often take full-sequence features (skip last 1 or 2 layers) from CLIP\u0026rsquo;s text encoder into the cross-attention to realize a \u0026ldquo;word-by-word instruction\u0026rdquo;. But indeed, if you take only the feature at last [EOS] token as the instruction, the results will be fine due to its globality.\n1.2 Contrastive learning The contrastive learning is implemented by optimizing a symmetric cross entropy loss over the cosine similarity score. We say that an image-text pair is matched if the cosine similarity between their embeddings is high enough.\nIn this paper, they set a learnable temperature parameter $e^t$ to scale the similarities, but didn\u0026rsquo;t explain much. I speculate such exponentiation can make learning more sensitive (imagine that steep curve) than a pure scalar.\nBTW, the authors separate out l2_normalize() and refer to np.dot() as the cosine similarity, which is of course equivalent but might be confusing.\nBLIP -\u0026gt; Junnan Li, et al. ICML 2022 The model architecture of BLIP (source from paper)\nIn addition to the contrastive learning, BLIP introduces two more vision-language objectives: image-text matching and image-conditioned language modeling. Such multimodal mixture of encoder-decoder model is jointly trained with three objectives, enabling a wider range of downstream tasks.\n2.1 Coarse-grained alignment Similar to CLIP, BLIP includes the image-text contrastive learning by two unimodal encoders. We can think of such alignment is coarse-grained, since the interaction between the features from two modalities occurs only in the last shallow linear network.\nA few differences from CLIP are listed below:\nBLIP\u0026rsquo;s text encoder is BERT-like architecture which the global feature of the whole text is allocated at the first [CLS] token instead of the [EOS]. BLIP\u0026rsquo;s text encoder and image encoder is initialized from BERT-base and ViT (pre-trained on ImageNet), instead of training from scratch. (CLIP\u0026rsquo;s networks are heavily modified and scaled, so there are certainly no suitable pre-trained baselines for initialization.) 2.2 Fine-grained alignment The image-grounded (conditioned) text encoder aims to learn image-text multimodal representation with dense cross-attention layer, which can capture the fine-grained alignment. The objective is a binary classification task to predict whether an image-text pair is matched or not.\nNoting that here they adopt the hard negative mining strategy, where those negatives pairs with higher contrastive similarity in a batch will be more likely to be chosen to compute the 0-1 loss. It makes sense because some unmatched pairs may have quite high cosine scores. Such filtering strategy is obviously another sense of fine-grained.\n2.3 Language modeling Altering to the masked self-attention can preserve the ability of language modeling with a new auxiliary objective, and is the future work as mentioned in CLIP. Now BLIP realizes this thing with the image-grounded text decoder.\nThis decoder is inherited from the image-grounded text encoder, where bi-directional SAs are replaced with causal SAs (i.e. triangle) just like in the regular text decoder. Correspondingly, it optimizes a cross entropy loss which trains the model to maximize the likelihood of the text in an autoregressive manner. And the text decoder share all parameters with text encoder except for the SA layers, since the remaining layers serve a similar function.\nPreserving the ability of LM enforce the image feature to capture all the information about the text, therefore make it easy to transfer to those vision-language generation tasks, such as Image Captioning, Visual Question Answering, and etc. But this LM objective also makes the learned feature more \u0026ldquo;text-like\u0026rdquo;, whether from the text encoder, text decoder, or even image encoder.\n2.4 Dataset bootstrapping They introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (source from paper)\nBLIP-2 -\u0026gt; Junnan Li, et al. arXiv 2023 The model architecture of BLIP-2 (source from paper)\nBLIP-2 consists of (1) an image transformer that interacts with the frozen image encoder for visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder. These two transformers share the same SA layers.\n3.1 Learnable queries A set number of learnable queries is created as input to the image transformer. Instead of taking image patches as input, allocating learnable queries can output the smaller-length $L=32$ features independent of image resolution (since the encoder/decoder-only model outputs the same length sequence as input). The output embeddings from these queries will be forced to extract visual features that capture all the information about the text, by three objectives. Due to such query\u0026rsquo;s importance, BLIP-2\u0026rsquo;s network is also known as the Q-Former.\n3.2 Shared self-attention We have observed that BLIP-1\u0026rsquo;s sub-networks have many common points in architectures and even in weights. So why not merge them as possible? and how?\nThe brilliance of BLIP-2 is that they employ different SA masking strategies for different objective to restrict the interaction between the vision token and text token, so two branches in the Q-Former can share the same SA layers. The self-attention masking strategy (source from paper)\nAlthough the masking strategies and the joint-training for three objectives sounds very straightforward and reasonable, there are a lot of unclear details waiting to be revealed in the source code.\nFor the contrastive learning, the query and text will be passed into Q-Former, respectively. Noting that the key\u0026amp;value of SA layers in the image transformer will be cached for later use.\nquery_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, return_dict=True, ) For the image-text matching, the query will be concatenated with the text and passed into Q-Former. All queries and texts can attend to each other.\noutput_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) For the language modeling, there are only the text (first token is replaced by [DEC]) as input to the Q-Former. As the visual instruction, the previous key\u0026amp;value caches (with pure visual information) will be concatenated with the current key\u0026amp;value (from the texts), and then passed to the SA layers with a multimodal causal mask.\nlm_output = self.Qformer( decoder_input_ids, attention_mask=attention_mask, past_key_values=query_output.past_key_values, return_dict=True, labels=labels, ) Compared to BLIP-1, the shared SAs of BLIP2 make the output query embeddings more \u0026ldquo;text-like\u0026rdquo; inevitably, so the image encoder should be frozen to counteract such imbalance.\n3.3 Bootstrap LLM Benefit from the LM objective, the output query embedding contains rich image and text features. So them can function as soft visual prompts to LLM by a simple linear projection.\n(Top) Bootstrapping a decoder-based LLM. (Bottom) Bootstrapping an encoder-decoder-based LLM (source from paper)\n","permalink":"https://yuhaoo00.github.io/posts/snapshots/2308clip/","summary":"In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.\nCLIP -\u0026gt; Alec Radford, et al.","title":"Learning the Multi-modal Feature Space"},{"content":"Guided Generation Hybrid-condition by Fine-Tuning ","permalink":"https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/","summary":"Guided Generation Hybrid-condition by Fine-Tuning ","title":"Two Inpainting Pipelines in Diffusers"},{"content":"","permalink":"https://yuhaoo00.github.io/posts/analysis/2302contorlnet/","summary":"","title":"Detailed Design in ControlNet"},{"content":"StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.\nUnet Fig. 1: Overall of the Unet in Stable Diffusion 1.x \u0026amp; 2.x\nAs in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x \u0026amp; SD2.x, $l_1=l_2=l_3=l_4=2$.\nFig. 2: Skip-connection in the 2nd scale.\nThe skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer\u0026rsquo;s input as in Fig. 2.\nFig 3: Transformer\u0026rsquo;s Design\nIn the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follow\u001ds the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.\nFig. 4: Resnet block\u0026rsquo;s design.\nThe SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling. $$ \\text{SiLU}(z)=z*\\text{sigmoid}(z) $$\nFig. 5: Timestep Embedding (based on positional encoding)\nVAE The KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don\u0026rsquo;t have the same weight (SD2\u0026rsquo;s VAE might be fine-tuned for higher resolution $768\\times 768$).\nFig.6: VAE Encoder\nFig.6: VAE Decoder\nTiled Processing In inference, VAE decoding often occupies a lot of memory.\nIn the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.\nThis is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.\nReferences https://github.com/huggingface/diffusers https://github.com/Stability-AI/stablediffusion https://github.com/Stability-AI/generative-models ","permalink":"https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/","summary":"StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.\nUnet Fig. 1: Overall of the Unet in Stable Diffusion 1.","title":"Network Design in Stable Diffusion"},{"content":"DDRM -\u0026gt; Bahjat Kawar, et al. NeurIPS, 2022. Illustration of DDRM (source from paper)\nTransformation via SVD Similar to SNIPS, DDRM consider the singular value decomposition (SVD) of the sampling matrix $H$ as follows: $$ \\begin{aligned} y\u0026amp;=Hx+z\\ y\u0026amp;=U\\Sigma V^\\top x+z\\ \\Sigma^{†} U^{\\top}y\u0026amp;=V^\\top x+\\Sigma^{†} U^{\\top}z\\ \\bar{y}\u0026amp;=\\bar{x}+\\bar{z}\\ \\end{aligned} $$ Since $U$ is orthogonal matrix, we have $p(U^\\top z) = p(z) = \\mathcal{N}(0,\\sigma^2_y I)$, resulting $\\bar{z}^{(i)}=(\\Sigma^{†} U^{\\top}z)^{(i)} \\sim \\mathcal{N}(0, \\frac{\\sigma^2_y}{s_i^2}I)$. So after these, we transform $x$ and $y$ into the same field (spectral space), and these two only differ by the noise $\\bar{z}$, which can be drawn as follows: $$ q(\\bar{y}^{(i)}|x_0)=\\mathcal{N}(\\bar{x}_0^{(i)},\\sigma_y^2/s_i^2 ) $$\nDDRM might be unable to cope with the wild scene, cause the variance $\\sigma_y$ is often unknown.\nConditional Diffusion \u0026amp; Generation The desired conditional diffusion process should (a) be a tractable Gaussian distribution, (b) employ the known $y$ to construct noisy samples as possible and (c) ensure the original marginal: $$ \\begin{aligned} q(x_t|x_0) = q(\\bar{x}_t|x_0, y)\\cdot q(\\bar{y}|x_0)=\\mathcal{N}(\\bar{x}_0,\\sigma_t^2I) \\ \\end{aligned} $$ We note that $q(\\bar{y}^{(i)}|x_0)=\\mathcal{N}(\\bar{x}_0^{(i)},\\sigma_y^2/s_i^2 )$, so these conditional processes can be defined as follows: From the perspective of DDIM, we can deduce the corresponding generative process via replacing $x_0$ with \u0026ldquo;the predicted version\u0026rdquo; as follows: Intuitively, this construction considers different cases for each index of the spectral space. (i) If the corresponding singular value $s_i=0$, then $y$ does not directly provide any information to that index, and the update is similar to regular unconditional generation. (ii) If $s_i \u0026gt;0$, then the updates consider the information provided by $y$, which further depends on whether the measurements’ noise level $\\sigma_y/s_i$ in the spectral space is larger than the noise level in the diffusion model or not.\nIn the resulting generation, the initial sample carries a few information from $\\bar{y}$, then is updated eventually by the guidance of $p_\\theta(\\bar{x}_t|x_t+1,y)$, and is recovered to $x_0$ exactly by left multiplying $V$.\nAlthough this conditional process results in a more complex ELBO objective for training, the authors proof that an optimal solution to DDPM / DDIM can also be an optimal solution to a DDRM problem, under some similar assumptions as in DDIM. So DDRM can be training-free.\nOne more thing DDRM is applied in super-resolution, deblurring, inpainting, and colorization. There\u0026rsquo;re no much difference of the value space between original $x$ and degraded $y$ in these task. These data are almost all in the perceptible pixel space. So does SVD really work as claimed? This paper lacks relevant ablation study for that. Maybe we should introduce this into more tasks, such as compressive sensing, and see what will happen.\nReferences Kawar, Bahjat, Gregory Vaksman, and Michael Elad. \u0026ldquo;SNIPS: Solving noisy inverse problems stochastically.\u0026rdquo; Advances in Neural Information Processing Systems 34 (2021): 21757-21769. Kawar, Bahjat, et al. \u0026ldquo;Denoising diffusion restoration models.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 23593-23606. ","permalink":"https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/","summary":"DDRM -\u0026gt; Bahjat Kawar, et al. NeurIPS, 2022. Illustration of DDRM (source from paper)\nTransformation via SVD Similar to SNIPS, DDRM consider the singular value decomposition (SVD) of the sampling matrix $H$ as follows: $$ \\begin{aligned} y\u0026amp;=Hx+z\\ y\u0026amp;=U\\Sigma V^\\top x+z\\ \\Sigma^{†} U^{\\top}y\u0026amp;=V^\\top x+\\Sigma^{†} U^{\\top}z\\ \\bar{y}\u0026amp;=\\bar{x}+\\bar{z}\\ \\end{aligned} $$ Since $U$ is orthogonal matrix, we have $p(U^\\top z) = p(z) = \\mathcal{N}(0,\\sigma^2_y I)$, resulting $\\bar{z}^{(i)}=(\\Sigma^{†} U^{\\top}z)^{(i)} \\sim \\mathcal{N}(0, \\frac{\\sigma^2_y}{s_i^2}I)$. So after these, we transform $x$ and $y$ into the same field (spectral space), and these two only differ by the noise $\\bar{z}$, which can be drawn as follows: $$ q(\\bar{y}^{(i)}|x_0)=\\mathcal{N}(\\bar{x}_0^{(i)},\\sigma_y^2/s_i^2 ) $$","title":"Inverse Problem × Diffusion -- Part: B"},{"content":" ”An inverse problem seeks to recover an unknown signal from a set of observed measurements. Specifically, suppose $x\\in R^n$ is an unknown signal, and $y\\in R^m = Ax+z$ is a noisy observation given by m linear measurements, where the measurement acquisition process is represented by a linear operator $A\\in R^{m\\times n}$, and $z\\in R^n$ represents a noise vector. Solving a linear inverse problem amounts to recovering the signal $x$ from its measurement $y$. Without further assumptions, the problem is ill-defined when $m\u0026lt; n$, so we additionally assume that $x$ is sampled from a prior distribution $p(x)$“\nsource from Dr. Yang Song\u0026rsquo;s paper\nAs unconditional generative model, Diffusion models (or Score models) can play a prior term in optimization for various ill-posed image inverse problems. Such methods are often labeled as training-free, zero-shot, unsupervised, etc.\nOn the other hand, Diffusion models also can be trained as conditional one $s_\\theta(x_t,y,t)$, where $y$ is the condition from other mode. However, this will consume a lot of resources both in the computing power and the collection of paired data $\\lbrace x_i,y_i\\rbrace$.\nMedical Score-SDE -\u0026gt; Yang Song, et al. in ICLR, 2022.\nThe medical imaging problem (source from paper)\nstep1: Perturbing measurements Since $x_t=\\alpha(t)x_0+\\beta(t)z$, and we set $y_t=Ax_t+\\alpha(t)\\epsilon$, so we have $y_t=\\alpha(t)y+\\beta(t)Az$.\nSmart and reasonable assumption for the coefficient $+\\alpha(t)\\epsilon$ to avoid the subsequent diffusion of random measurement noise $\\epsilon$.\nstep2: Generation with Score-SDE Using the Euler-Maruyama sampler, the generative process given by: $$ x_{t-\\Delta t} = x_t-f(t)x_t\\Delta t+g(t)^2s_\\theta(x_t,t)\\Delta t+g(t)\\sqrt{\\Delta t}z $$ where $s_\\theta(\\cdot)$ is the trained score model, $\\Delta t = 1/N$ is the set discrete time, and $z\\sim \\mathcal{N}(0,1)$.\nstep3: Consistent Guidance The pipeline of medical Score-SDE\nWe set the guided intermediate result as $x_t^{\\prime}$, For simultaneously minimizing the distance between $x_t$ and $x_t^{\\prime}$, and the distance between $x_t^{\\prime}$ and the hyperplane $\\lbrace x\\in R^n| Ax=y_t \\rbrace$, we build an optimization problem as: $$ \\begin{aligned} x_t^{\\prime}\u0026amp;=\\arg \\min_{v\\in R^n}\\lbrace (1-\\lambda)\\Vert v-x_t \\Vert_{T}^2 + \\min_{u\\in R^n} \\lambda \\Vert v-u \\Vert_{T}^2 \\rbrace \\ s.t.\\quad Au\u0026amp;=y_t \\end{aligned} $$ which has the closed-form solution as: $$ x_t^{\\prime}=T^{-1}[\\lambda \\Lambda \\mathcal{P} ^{-1}(\\Lambda)y_t+(1-\\lambda)\\Lambda Tx_t+(1-\\Lambda)Tx_t] $$ When the measurement process is noisy, we can choose $0\u0026lt;\\lambda\u0026lt;1$ to allow slackness in $Ax_t^{\\prime}=y_t$ (more affected by $p(x)$). When the measurement process contains no noise, the authors chosse $\\lambda=1$ at the last sampling step to guarantee $Ax_0^{\\prime}=y$.\nScore-MRI -\u0026gt; Hyungjin Chung, et al. Medical Image Analysis, 2022.\nSimilar to the above, Score-MRI inserts a more simple consistency mapping after every \u0026ldquo;Predictor\u0026rdquo; and \u0026ldquo;Corrector\u0026rdquo; iteration. $$ x_i=x_i+\\lambda A^\\ast(y-Ax_i)=(I-\\lambda A^\\ast A)x_i+A^\\ast y $$ where $A^\\ast$ denotes the Hermitian adjoint, and $+\\lambda A^\\ast (y-Ax_i)$ can be viewed as a rectification for minimizing the distance between $y$ and $Ax_i$. In the hybridtype Algorithm 5 (complex domain and multi-coils for MRI), the authors start with $\\lambda = 1.0$ in the first iteration, and linearly decrease the value to $\\lambda=0.2$ at the last iteration.\nGANCS has an similar rectification $(I-\\Phi^{†}\\Phi)x+\\Phi^{†}y$, where pseudo-inverse $\\Phi^{†}$ satisfies $\\Phi\\Phi^{†}\\Phi=\\Phi$, but without the scaling factor $\\lambda$. In this paper, the authors view $(I-\\Phi^{†}\\Phi)x$ as a projection onto the nullspace of $\\Phi$.\nDDNM -\u0026gt; Yinhuai Wang, et al. in ICLR, 2023.\nNoise-free Similar to GANCS, DDNM adopts a rectified estimation as: $$ x_{0|t}^{\\prime}=A^{†}y+(I-A^{†}A)x_{0|t} $$ where $x_{0|t}$ is the predicted version at timestep $t$. Based on DDIM, the generative process can be driven by $x_{t-1}\\sim \\mathcal{p}(x_{t-1}|x_t,x_{0|t}^{\\prime})$. So it might provide a more reliable rectification to human-perceptible $x_{0|t}$, rather than noisy $x_t$ like in Score-MRI.\nAlthough this paper mentioned that this consistency mapping was inspired by range-null space decomposition $x=A^{†}Ax+(I-A^{†}A)x$, but it seems to contradict with the subsequent DDNM+ for the noisy inverse problem.\nNoisy $$ x_{0|t}^{\\prime}=A^†y+(I−A^†A)x_{0|t} =x_{0|t}-A^†(Ax_{0|t}-y) $$ In short, the authors employ two scale factors $\\Sigma_t$ \u0026amp; $\\Phi_t$ into range-space correction \u0026amp; generative variance, respectively. $$ \\begin{aligned} x_{0|t}^{\\prime}\u0026amp;= x_{0|t}-\\Sigma_tA^†(Ax_{0|t}-y)\\ p^{\\prime}(x_{t-1}|x_t,x_{0|t}^{\\prime})\u0026amp;=\\mathcal{N}(x_{t-1};\\mu_t(x_t, x_{0|t}^{\\prime}),\\Phi_t I) \\end{aligned} $$\nHowever, the performance gains from these factor are not clear, due to the absence of ablations experiments. It looks like the \u0026ldquo;Time-Travel\u0026rdquo; (a kind of redundant computing) helped a lot.\nTime-Travel (source from paper)\nReferences Yang Song and others, ‘Solving Inverse Problems in Medical Imaging with Score-Based Generative Models’ in International Conference on Learning Representations, 2022. Hyungjin Chung and Jong Chul Ye, ‘Score-Based Diffusion Models for Accelerated MRI’, Medical Image Analysis, 80 (2022), 102479. Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, ‘Deep Null Space Learning for Inverse Problems: Convergence Analysis and Rates’, Inverse Problems, 35.2 (2019), 025008. Morteza Mardani and others, ‘Deep Generative Adversarial Networks for Compressed Sensing Automates MRI’ arXiv, 2017. Yinhuai Wang, Jiwen Yu, and Jian Zhang, ‘Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model’ in International Conference on Learning Representations, 2023. ","permalink":"https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/","summary":"”An inverse problem seeks to recover an unknown signal from a set of observed measurements. Specifically, suppose $x\\in R^n$ is an unknown signal, and $y\\in R^m = Ax+z$ is a noisy observation given by m linear measurements, where the measurement acquisition process is represented by a linear operator $A\\in R^{m\\times n}$, and $z\\in R^n$ represents a noise vector. Solving a linear inverse problem amounts to recovering the signal $x$ from its measurement $y$.","title":"Inverse Problem × Diffusion -- Part: A"},{"content":" Although Diffusion Model is a new generative framework, it still has many shades of other methods.\nBayes\u0026rsquo; rule is all you need\nGeneration \u0026amp; Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta\\left(x_{t-1} \\vert x_t\\right)$$ $$p_\\theta\\left(x_{t-1} \\vert x_t\\right):=N(x_{t-1};\\mu_{\\theta}(x_t,t),\\Sigma_\\theta(x_t,t))$$ Markov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\\vert x_{t:T})=p(x_{t-1}\\vert x_{t})$\nSimilar to VAE, we can use the posterior $q(x_{1:t} \\vert x_0)$ to do the estimation for $\\theta$. The difference is that $x_1,\\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \\vert x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\\alpha_{1:T}\\in [0,1]^T$: $$ q(x_{1:T} \\vert x_0) := \\prod_{t=1}^T q\\left(x_{t} \\vert x_{t-1}\\right) $$ $$ q(x_t \\vert x_{t-1}):=N(\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}}x_{t-1}, (1-\\frac{\\alpha_t}{\\alpha_{t-1}})I) $$ A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$: $$ q(x_t\\vert x_0)=N(x_t;\\sqrt{\\alpha_t}x_0, (1-\\alpha_t)I) $$\nTraining Objective We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood: $$ \\max_{\\theta}E_{q}[\\log{p_\\theta(x_0)}]\\leq \\max_{\\theta}E_{q}[\\log{p_{\\theta} (x_{0:T})}-\\log{q(x_{1:T} \\vert x_0)}] $$ which also can be driven by Jensen’s inequality as in Lil\u0026rsquo;log. And we can further rewrite this object as:\nUsing Bayes\u0026rsquo; rule, we can deduce the fact that $q(x_{t-1}\\vert x_t, x_0)$ is also a gaussian distribution.\nwhere $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs. If generative variances is all fixed $\\Sigma_t = \\sigma^2_t$, using parameterization (fit distribution $\\to$ fit mean $\\to$ predict noise) and reweighting based on the empirical results, we can simplify this objective as follows: $$ L_t=E_{x_0\\sim q, \\epsilon\\sim N(0,1)}\\left[|| \\epsilon_\\theta(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon, t)-\\epsilon {||}_2^2 \\right] $$\nNCSN vs DDPM, different ways lead to almost the same objective!\nFor last $L_0$, DDPMs treat it as an independent discrete decoder derived from $N(x_0;\\mu_{\\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.\nAt the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\\theta(x_{t-1}\\vert x_t)$ using predicted $\\epsilon_\\theta(\\cdot)$ (Alg. 2).\nDDPM+ Finding 1: Why fixing $\\sigma^2$ to $\\beta$ or$\\tilde{\\beta}$ achieve similar sample quality? As $\\beta \\approx \\tilde{\\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\\frac{\\beta_t}{\\beta_{t-1}} \\to 0$.\nFinding 2: The diffusion samples from linear schedule lose information very quickly. In the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggests that so much full noisy samples might do not contribute to generation.\nImprovements Learnable variances with an interpolation between $\\beta$ and $\\tilde{\\beta}$, driven by loss $+ \\lambda L_{vlb}$. Cosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end. Resampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting) DDIM More free diffusion chain In DDIM, the authors introduce a extended version $q(x_{t-1}\\vert x_t, x_0)$, which has the same marginal noise distribution $q(x_{t}\\vert x_0)$： $$ q_{\\sigma}(x_{t-1} \\vert x_t, x_0) = N(x_{t-1}; \\sqrt{\\alpha_{t-1}} x_0 + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\frac{x_t - \\sqrt{\\alpha_t} x_0}{\\sqrt{1 - \\alpha_t}}, \\sigma_t^2 I) $$ Therefore, the corresponding generative process can be exported as: If we set $\\sigma_t^2=\\tilde{\\beta}_t$, the diffusion process becomes Markovian, and the generative process becomes DDPM. And if $\\sigma_t=0$, there is a deterministic generation, called DDIM.\nAcceleration via subsampling In the generation, we sample a subset of S steps ${\\tau_1,\\dots,\\tau_S}$ to form a new chain as: $$ q_{\\sigma, \\tau}(x_{\\tau_{i-1}} \\vert x_{\\tau_i}, x_0) = N(x_{\\tau_{i-1}}; \\sqrt{\\alpha_{\\tau_{i-1}}} x_0 + \\sqrt{1 - \\alpha_{\\tau_{i-1}} - \\sigma_t^2} \\frac{x_{\\tau_i} - \\sqrt{\\alpha_{\\tau_i}} x_0}{\\sqrt{1 - \\alpha_{\\tau_i}}}, \\sigma_{\\tau_i}^2 I) $$ which can still provide high-quality samples using a much fewer number of steps.\nReferences Jonathan Ho, Ajay Jain, and Pieter Abbeel, ‘Denoising Diffusion Probabilistic Models’, in Advances in Neural Information Processing Systems, 2020. Alexander Quinn Nichol and Prafulla Dhariwal, ‘Improved Denoising Diffusion Probabilistic Models’, in Proceedings of the 38th International Conference on Machine Learning, 2021. Jiaming Song, Chenlin Meng, and Stefano Ermon, ‘Denoising Diffusion Implicit Models’, in International Conference on Learning Representations, 2021. ","permalink":"https://yuhaoo00.github.io/posts/snapshots/2212ddpm/","summary":"Although Diffusion Model is a new generative framework, it still has many shades of other methods.\nBayes\u0026rsquo; rule is all you need\nGeneration \u0026amp; Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta\\left(x_{t-1} \\vert x_t\\right)$$ $$p_\\theta\\left(x_{t-1} \\vert x_t\\right):=N(x_{t-1};\\mu_{\\theta}(x_t,t),\\Sigma_\\theta(x_t,t))$$ Markov chain: What happens next depends only on the state of affairs now.","title":"DDPM and Early Variants"},{"content":" Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function) may be a better choice than learning the data density directly.\nScore Estimation (for training) We want to train a network $s_{\\theta}(x)$ to estimate $\\nabla_{ x} \\log p_{\\text {data }}( x)$, but how can we get the ground truth (real score)? In this paper, the objective $\\frac{1}{2} E_{p_{\\text{data}}} \\lbrack\\lVert s_{\\theta}( x)-\\nabla_{ x} \\log p_{\\text{data}}( x)\\rVert_2^2\\rbrack$ is equivalent to the following by Score Matching:\n$$ E_{p_{\\text{data}}}\\left[tr(\\nabla_{ x}s_{\\theta}( x))+\\frac{1}{2}\\left|s_{\\theta}( x))\\right|_2^2\\right] $$\nUnfortunately, it is not easy to compute $tr(\\cdot)$ for a large-scale problem. Both Denoising score matching and Sliced score matching are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q_\\sigma$) as follows:\n$$ \\frac{1}{2} E_{q_\\sigma(\\tilde{ x} \\mid x) p_{\\text {data }}( x)}\\left[\\left| s_{\\theta}(\\tilde{ x})-\\nabla_{\\tilde{ x}} \\log q_\\sigma(\\tilde{ x} \\mid x)\\right|_2^2\\right] $$\nMinimizing this objective, we can get $s_{\\theta}( x)=\\nabla_{ x} \\log q_{\\sigma}( x)$. And if the noise is small enough $q_\\sigma( x) \\approx p_{\\text {data }}( x)$, we will get $s_{\\theta}( x)=\\nabla_{ x} \\log q_\\sigma( x) \\approx \\nabla_{ x} \\log p_{\\text {data }}( x)$. As $q_\\sigma(\\tilde{x}| x)$ can be defined as a known simple distribution, this minimization is easier than regular score matching. In this paper, they choose $q_\\sigma(\\tilde{x}| x) =N\\left(\\tilde{x} \\mid x, \\sigma^2 I\\right)$, which leads to: $$\\ell(\\theta ; \\sigma) \\triangleq \\frac{1}{2} E_{p_{\\text {data }}(x)} E_{\\tilde{x} \\sim N\\left(x, \\sigma^2 I\\right)}\\left[\\left|\\mathbf{s}_{\\theta}(\\tilde{x}, \\sigma)+\\frac{\\tilde{x}-x}{\\sigma^2}\\right|_2^2\\right]$$\nLangevin dynamics (for inference) How can we do sampling from $p_{\\text {data }}( x)$ when we get a nice estimation of $\\nabla_{ x} \\log p_{\\text {data }}( x)$? Just along the direction of this gradient? Yes, but plus more tricks.\n$$ x_t = x_{t-1}+\\frac{\\epsilon}{2} \\nabla_{x} \\log p\\left(x_{t-1}\\right)+\\sqrt{\\epsilon} z_t $$\nThe Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide more stable distribution! Briefly, the random noise term $\\mathbf{z}_t \\thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\\epsilon \\to 0$), the iterative $x_t$ will approach the distribution $p(x)$.\nPractical Challenges The manifold hypothesis The data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).\nThe score is undefined. The estimation by Score Matching isn\u0026rsquo;t consistent. If we perturb the data with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast \u0026amp; faithful) will converge (Fig. 1).\nFigure 1: Left: Sliced score matching (SSM) loss w.r.t. iterations. No noise is added to data. Right: Same but data are perturbed with N(0, 0.0001).\nLow data density regions Our training is based on the data in high density ($\\thicksim p_{\\text {data }}( x)$).\nThe estimation in low density regions is inaccurate. Regular Langevin Dynamics can\u0026rsquo;t be able to correctly recover the relative weights of the multi-modal distribution in reasonable time. If we perturb the data by using multiple noise levels (anneal down), we can fill the low density regions.\nSo naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions. (Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)\nOne More Thing In the score matching, the authors approximately have $\\left| s_{\\theta}( x, \\sigma)\\right|_2 \\propto 1 / \\sigma$, so they choose $\\lambda (\\sigma) = \\sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\\sigma$.\n$$ \\mathcal{L}\\left(\\theta ;\\lbrace\\sigma_i \\rbrace_{i=1}^{L}\\right) \\triangleq \\frac{1}{L} \\sum_{i=1}^L \\lambda\\left(\\sigma_i\\right) \\ell\\left(\\theta ; \\sigma_i\\right) $$\nCorrespondingly, in the langevin dynamics, they choose $\\alpha_i \\propto \\sigma^2$ to make the order magnitude of \u0026ldquo;signal-to-noise ratio\u0026rdquo; independent of $\\sigma$.\nReferences Yang Song and Stefano Ermon, ‘Generative Modeling by Estimating Gradients of the Data Distribution’, in Advances in Neural Information Processing Systems, 2019. Yang Song and others, ‘Score-Based Generative Modeling through Stochastic Differential Equations’, in International Conference on Learning Representations, 2021. ","permalink":"https://yuhaoo00.github.io/posts/snapshots/2212ncsn/","summary":"Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function) may be a better choice than learning the data density directly.\nScore Estimation (for training) We want to train a network $s_{\\theta}(x)$ to estimate $\\nabla_{ x} \\log p_{\\text {data }}( x)$, but how can we get the ground truth (real score)? In this paper, the objective $\\frac{1}{2} E_{p_{\\text{data}}} \\lbrack\\lVert s_{\\theta}( x)-\\nabla_{ x} \\log p_{\\text{data}}( x)\\rVert_2^2\\rbrack$ is equivalent to the following by Score Matching:","title":"Image Generation based on Score Model"},{"content":"","permalink":"https://yuhaoo00.github.io/posts/analysis/2307a111features/","summary":"","title":""},{"content":"","permalink":"https://yuhaoo00.github.io/posts/idea/2206maeandtransferlearning/","summary":"","title":""},{"content":"","permalink":"https://yuhaoo00.github.io/posts/snapshots/2309aeforvisiongeneration/","summary":"","title":""},{"content":"","permalink":"https://yuhaoo00.github.io/posts/snapshots/2310maeforvisiongeneration/","summary":"","title":""}]