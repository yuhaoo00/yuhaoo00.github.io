{
    "version": "https://jsonfeed.org/version/1.1",
    "title": "Encapsulating what I think. on Yu's MemoCapsule",
    "description": "Recent content in Encapsulating what I think. on Yu's MemoCapsule",
    "home_page_url": "https://yuhaoo00.github.io",
    "feed_url": "https://yuhaoo00.github.io/index.json",
    "language": "en-GB",
    "icon": "https://yuhaoo00.github.io/favicon-192.png",
    "favicon": "https://yuhaoo00.github.io/favicon_clip.ico",
    "items": [
        {
            "title": "Generating Images Like Texts",
            "date_published": "2023-09-26T00:00:00Z",
            "date_modified": "2023-09-26T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/",
            "content_html": "\u003cp\u003eCan we generate images in the same way as autoregressive language model?\u003c/p\u003e\n\u003cp\u003eAlthough this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don\u0026rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.\u003c/p\u003e\n\u003ch2 id=\"taming-transformer\" class=\"icon-inline\" id=\"taming-transformer\"\u003eTaming Transformer\u003ca class=\"icon-link\" href=\"#taming-transformer\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com\"\u003e-\u0026gt; Patrick Esser, et al. CVPR 2021\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts. For this sake, the Taming Transformer is designed as a two-stage approach including a VQGAN and an Autoregressive Transformer.\u003c/p\u003e\n\u003ch3 id=\"11-vq-gan\" class=\"icon-inline\" id=\"11-vq-gan\"\u003e1.1 VQ-GAN\u003ca class=\"icon-link\" href=\"#11-vq-gan\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026163628.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nSummary of taming transformer\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf\"\u003eVector quantization\u003c/a\u003e is a brilliant idea to provide the compression and discrete representation for images (like a image tokenizer). Inspired by that, VQ-GAN realizes more effective representation with \u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html\"\u003epatchGAN\u003c/a\u003e, compressing an image into a learnable space (codebook). More precisely, any image $x\\in R^{H\\times W\\times 3}$ can be represented by \u003cstrong\u003ea discrete vector\u003c/strong\u003e $s \\in R^{h×w}$ (an index set of the closest codebook entries).\u003c/p\u003e\n\u003cp\u003eThe training objective for finding the optimal compression model can be expressed as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\u0026amp;L_{VQ}=L_{perceptual}+||sg(E(x))-z_q||^2_2+\\beta||sg(z_q)-E(x)||^2_2 \\\\\n\u0026amp;L_{GAN}=\\log{D(x)}+\\log{(1-D(\\hat{x}))} \\\\\n\u0026amp;\\arg{\\min_{E,G,Z}\\max_{D}}\\ \\mathbb{E}\\lbrack L_{VQ}(E,G,Z)+\\lambda L_{GAN}(E,G,Z,D)\\rbrack\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003ewhere the adaptive weight $\\lambda=\\nabla_{G_L}[L_{rec}]/(\\nabla_{G_L}[L_{GAN}]+10^{−6})$ tends to focus on the smaller one of $L_{rec}$ and $L_{GAN}$.\u003c/p\u003e\n\u003ch3 id=\"12-autoregression\" class=\"icon-inline\" id=\"12-autoregression\"\u003e1.2 Autoregression\u003ca class=\"icon-link\" href=\"#12-autoregression\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFor the autoregressive transformer, it can be implemented by the \u0026ldquo;decoder-only\u0026rdquo; structure and the casual self-attention mask. And we can directly maximize the log-likelihood of the data representations as:\u003c/p\u003e\n\u003cp\u003e$$\nL_{Transformer} = \\mathbb{E}[-\\log(\\Pi_i{p(s_i|s_{\u0026lt;i})})]\n$$\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026184452.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nSliding attention window\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhen generating images in the megapixel regime (\u0026gt;256^2), that transformer will adopt the local attention in a sliding-window manner for efficiency. Since transformer is a kind of network without inductive bias, we need the spatial conditioning information or the training dataset with spatial invariance to ensure such local strategy work well.\u003c/p\u003e\n\u003ch2 id=\"parti\" class=\"icon-inline\" id=\"parti\"\u003eParti\u003ca class=\"icon-link\" href=\"#parti\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2206.10789\"\u003e-\u0026gt; Jiahui Yu, et al. arXiv 2022\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eParti (Pathways Autoregressive Text-to-Image) shows that above two-stage autoregressive method is able to realize high-fidelity generation for text-to-image with additional fine-tuning and upsampling.\u003c/p\u003e\n\u003ch3 id=\"21-vit-vqgan\" class=\"icon-inline\" id=\"21-vit-vqgan\"\u003e2.1 ViT-VQGAN\u003ca class=\"icon-link\" href=\"#21-vit-vqgan\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThey first train a stronger \u003ca href=\"https://arxiv.org/abs/2110.04627\"\u003eViT-VQGAN-Small\u003c/a\u003e encoder (30M) as a image tokenizer on their training data, which achieves 4x downsampling (i.e., $256\\to 32$) and learns 8192 image token classes for the codebook.\u003c/p\u003e\n\u003ch3 id=\"22-text-to-image\" class=\"icon-inline\" id=\"22-text-to-image\"\u003e2.2 Text-to-Image\u003ca class=\"icon-link\" href=\"#22-text-to-image\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113357.png\" loading=\"lazy\" width=\"600\"\u003e\n\u003cfigcaption\u003e\nSummary of Parti\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe text-to-image model is based on a classical encoder-decoder architecture in multi-modal tasks.\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027131920.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThey build a \u003ca href=\"https://arxiv.org/abs/1808.06226\"\u003eSentencePiece model\u003c/a\u003e as the text encoder which provides text tokens of length 128. This text encoder is first \u003cstrong\u003epretrain on two datasets\u003c/strong\u003e: the C4 datasets with BERT loss, and their image-text datasets with contrastive loss.\u003c/p\u003e\n\u003cp\u003eAfter pretraining, they continue \u003cstrong\u003etraining both encoder and decoder\u003c/strong\u003e for text-to-image generation with softmax cross-entropy loss. Nothing that the decoder uses conv-shaped masked sparse attention (like the sliding window in Taming Transformer).\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe ability of text encoder after pretraining performs comparably to BERT, but degrades after the full encoder-decoder training, which indicates the difference between language representation and image-grounded language representation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eClassifier-free guidance\u003c/strong\u003e has been adopted to great effect for Parti. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt.\u003c/p\u003e\n\u003ch3 id=\"23-fine-tune--super-resolution\" class=\"icon-inline\" id=\"23-fine-tune--super-resolution\"\u003e2.3 Fine-tune \u0026amp; Super-Resolution\u003ca class=\"icon-link\" href=\"#23-fine-tune--super-resolution\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAfter two-stage training, they freeze the image tokenizer and codebook, and fine-tune a larger-size image detokenizer (600M) to further improve visual acuity.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113410.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nA learned super-resolution module to upsample images\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eMoreover, they employ a simple super-resolution network (15M~30M) on top of the image detokenizer. This SR-network is based on \u003ca href=\"https://arxiv.org/abs/1808.08718\"\u003eWDSR\u003c/a\u003e and trained with the same losses of \u003ca href=\"https://arxiv.org/abs/2110.04627\"\u003eViT-VQGAN\u003c/a\u003e (perceptual loss, StyleGAN loss and l2 loss). It has about 15M parameters for 2x upsampling and 30M parameters for 4x upsampling.\u003c/p\u003e\n\u003ch2 id=\"muse\" class=\"icon-inline\" id=\"muse\"\u003eMuse\u003ca class=\"icon-link\" href=\"#muse\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2301.00704\"\u003e-\u0026gt; Huiwen Chang, et al. arXiv 2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlthough above solutions alleviate high training cost to some extent, the autoregressive paradigm still slows down inference significantly. Can we adopt parallel iteration instead of one by one? Muse give that answer which employs a \u003cstrong\u003erandom masking strategy\u003c/strong\u003e (like MLM in NLP) to facilitate predicting multiple tokens at once.\u003c/p\u003e\n\u003ch3 id=\"31-frozen-llm-as-text-encoder\" class=\"icon-inline\" id=\"31-frozen-llm-as-text-encoder\"\u003e3.1 Frozen LLM as Text Encoder\u003ca class=\"icon-link\" href=\"#31-frozen-llm-as-text-encoder\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eRecent works show that the conceptual representations learned by LLMs are roughly linearly mappable to those learned by models trained on vision tasks. Fueled by these observations, Muse adopts frozen \u003ca href=\"https://dl.acm.org/doi/abs/10.5555/3455716.3455856\"\u003eT5-XXL\u003c/a\u003e as the text encoder and tries to map those rich visual and semantic concepts in the LLM embeddings to the generated images. These embedding vectors are linearly projected to the hidden size of later Transformer models (base and super-res).\u003c/p\u003e\n\u003ch3 id=\"32-base-model\" class=\"icon-inline\" id=\"32-base-model\"\u003e3.2 Base Model\u003ca class=\"icon-link\" href=\"#32-base-model\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142154.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe base model of Muse\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eMuse\u0026rsquo;s base model has the same encoder-decoder architecture as Parti, but it employs a random masking strategy to ensure learning more expressive and robust.\u003c/p\u003e\n\u003cp\u003eThey leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens and replace them with a special [MASK] token. The masking rate is a variable based on a cosine scheduling $r\\sim p(r)=\\frac{2}{\\pi} (1 − r^2)^{-1/2}$, where the bias towards higher masking rates makes the prediction problem harder.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNoting that this masking functions on input rather than attention layers.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn this way, the base model is trained to predict all masked tokens at once.\u003c/p\u003e\n\u003ch3 id=\"33-super-resolution-model\" class=\"icon-inline\" id=\"33-super-resolution-model\"\u003e3.3 Super-Resolution Model\u003ca class=\"icon-link\" href=\"#33-super-resolution-model\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142223.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe super-resolution model of Muse\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eFor the high-resolution generation, the authors found that directly predicting $512\\times 512$ resolution leads the model to focus on low-level details over large-scale semantics. To this end, they trained another decoder to predict masked tokens in higher resolution with the help of low-res conditioning and text conditioning.\u003c/p\u003e\n\u003ch3 id=\"34-fine-tune\" class=\"icon-inline\" id=\"34-fine-tune\"\u003e3.4 Fine-tune\u003ca class=\"icon-link\" href=\"#34-fine-tune\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFollowing the Parti model, Muse increases the capacity of the VQGAN decoder by the addition of more residual layers and channels, and then fine-tune the new decoder layers while other modules frozen.\u003c/p\u003e\n\u003ch3 id=\"35-iterative-parallel-decoding\" class=\"icon-inline\" id=\"35-iterative-parallel-decoding\"\u003e3.5 Iterative Parallel Decoding\u003ca class=\"icon-link\" href=\"#35-iterative-parallel-decoding\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027153559.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nInference samples in Muse\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe above masked learning support us to decode multiple tokens at each step, so this inference is called as iterative parallel decoding. Based on a cosine schedule, we predict all masked tokens at each step, and choose a fraction of the highest confidence tokens as unmasked, and continue next step to predict remaining masked tokens.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027155432.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nPer-batch inference time for several models.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eUsing this procedure, Muse is able to perform high-fidelity inference using only 24 steps in base model and 8 steps in super-resolution model, and is significantly faster than competing diffusion or other autoregressive models.\u003c/p\u003e\n"
        },
        {
            "title": "Learning the Multi-modal Feature Space",
            "date_published": "2023-09-11T00:00:00Z",
            "date_modified": "2023-09-11T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2309clip/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2309clip/",
            "content_html": "\u003cp\u003eIn multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as \u003cstrong\u003ehow to learn a better multi-modal feature space\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"clip\" class=\"icon-inline\" id=\"clip\"\u003eCLIP\u003ca class=\"icon-link\" href=\"#clip\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://proceedings.mlr.press/v139/radford21a.html\"\u003e-\u0026gt; Alec Radford, et al. NeuraIPS 2021\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023154456.png\" loading=\"lazy\" width=\"600\"\u003e\n\u003cfigcaption\u003e\nThe model architecture of CLIP (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\nCLIP has a quite simple design which consists of a \u003cstrong\u003ejoint-training\u003c/strong\u003e image encoder and text encoder, but there are still some interesting details worth noting.\u003c/p\u003e\n\u003ch3 id=\"11-bag-of-words-encoding\" class=\"icon-inline\" id=\"11-bag-of-words-encoding\"\u003e1.1 Bag-of-words encoding\u003ca class=\"icon-link\" href=\"#11-bag-of-words-encoding\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIn the text encoder, the output at last [EOS] token are used as the feature representation of the text (i.e. $[N,L]\\to[N,L,D]\\to[N,D]$).  The authors refer to this process as \u0026ldquo;Bag-of-words Encoding\u0026rdquo; where the text of arbitrary length can be encoded into a D-dimensional vector.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn Stable Diffusion, CLIP\u0026rsquo;s text encoder is used to provide crucial text feature. People often take full-sequence features (skip last 1 or 2 layers) from CLIP\u0026rsquo;s text encoder into the cross-attention to realize a \u0026ldquo;word-by-word instruction\u0026rdquo;. But indeed, if you take only the feature at last [EOS] token as the instruction, the results will be fine due to its globality.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"12-contrastive-learning\" class=\"icon-inline\" id=\"12-contrastive-learning\"\u003e1.2 Contrastive learning\u003ca class=\"icon-link\" href=\"#12-contrastive-learning\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe contrastive learning is implemented by optimizing a symmetric cross entropy loss over the cosine similarity score. We say that an image-text pair is matched if the cosine similarity between their embeddings is high enough.\u003c/p\u003e\n\u003cp\u003eIn this paper, they set a learnable temperature parameter $e^t$ to scale the similarities, but didn\u0026rsquo;t explain much. I speculate such exponentiation can make learning more sensitive (imagine that steep curve) than a pure scalar.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBTW, the authors separate out l2_normalize() and refer to np.dot() as the cosine similarity, which is of course equivalent but might be confusing.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"blip\" class=\"icon-inline\" id=\"blip\"\u003eBLIP\u003ca class=\"icon-link\" href=\"#blip\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://proceedings.mlr.press/v162/li22n.html\"\u003e-\u0026gt; Junnan Li, et al. ICML 2022\u003c/a\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023171329.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe model architecture of BLIP (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eIn addition to the contrastive learning, BLIP introduces two more vision-language objectives: image-text matching and image-conditioned language modeling. Such multimodal mixture of encoder-decoder model is jointly trained with three objectives, enabling a wider range of downstream tasks.\u003c/p\u003e\n\u003ch3 id=\"21-coarse-grained-alignment\" class=\"icon-inline\" id=\"21-coarse-grained-alignment\"\u003e2.1 Coarse-grained alignment\u003ca class=\"icon-link\" href=\"#21-coarse-grained-alignment\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to CLIP, BLIP includes the image-text contrastive learning by two unimodal encoders. We can think of such alignment is coarse-grained, since the interaction between the features from two modalities occurs only in the last shallow linear network.\u003c/p\u003e\n\u003cp\u003eA few differences from CLIP are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe global feature of the whole text is allocated at the first [CLS] token instead of the [EOS].\u003c/li\u003e\n\u003cli\u003eBLIP\u0026rsquo;s text encoder and image encoder is initialized from BERT-base and ViT (pre-trained on ImageNet), instead of training from scratch. (CLIP\u0026rsquo;s networks are heavily modified and scaled, so there are certainly no suitable pre-trained baselines for initialization.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"22-fine-grained-alignment\" class=\"icon-inline\" id=\"22-fine-grained-alignment\"\u003e2.2 Fine-grained alignment\u003ca class=\"icon-link\" href=\"#22-fine-grained-alignment\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe image-grounded (conditioned) text encoder aims to learn image-text multimodal representation with dense cross-attention layer, which can capture the fine-grained alignment. The objective is a binary classification task to predict whether an image-text pair is matched or not.\u003c/p\u003e\n\u003cp\u003eNoting that here they adopt the \u003cstrong\u003ehard negative mining strategy\u003c/strong\u003e, where those negatives pairs with higher contrastive similarity in a batch will \u003cstrong\u003ebe more likely to be chosen\u003c/strong\u003e to compute the 0-1 loss. It makes sense because some unmatched pairs may have quite high cosine scores. Such filtering strategy is obviously another sense of fine-grained.\u003c/p\u003e\n\u003ch3 id=\"23-language-modeling\" class=\"icon-inline\" id=\"23-language-modeling\"\u003e2.3 Language modeling\u003ca class=\"icon-link\" href=\"#23-language-modeling\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAltering to the masked self-attention can preserve the ability of language modeling with a new auxiliary objective, and is the future work as mentioned in CLIP. Now BLIP realizes this thing with the image-grounded text decoder.\u003c/p\u003e\n\u003cp\u003eThis decoder is inherited from the image-grounded text encoder, where bi-directional SAs are replaced with causal SAs (i.e. triangle) just like in the regular text decoder. Correspondingly, it optimizes a cross entropy loss which trains the model to maximize the likelihood of the text in an \u003cstrong\u003eautoregressive manner\u003c/strong\u003e. And the text decoder share all parameters with text encoder except for the SA layers, since the remaining layers serve a similar function.\u003c/p\u003e\n\u003cp\u003ePreserving the ability of LM enforce the image feature to capture all the information about the text, therefore make it easy to transfer to those vision-language generation tasks, such as Image Captioning, Visual Question Answering, and etc. But this LM objective also makes the learned feature more \u0026ldquo;text-like\u0026rdquo;, whether from the text encoder, text decoder, or even image encoder.\u003c/p\u003e\n\u003ch3 id=\"24-dataset-bootstrapping\" class=\"icon-inline\" id=\"24-dataset-bootstrapping\"\u003e2.4 Dataset bootstrapping\u003ca class=\"icon-link\" href=\"#24-dataset-bootstrapping\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023173348.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThey introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"blip-2\" class=\"icon-inline\" id=\"blip-2\"\u003eBLIP-2\u003ca class=\"icon-link\" href=\"#blip-2\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2301.12597\"\u003e-\u0026gt; Junnan Li, et al. arXiv 2023\u003c/a\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025153821.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe model architecture of BLIP-2 (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eBLIP-2 consists of (1) an image transformer that interacts with the frozen image encoder for visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder. These two transformers share the same SA layers.\u003c/p\u003e\n\u003ch3 id=\"31-learnable-queries\" class=\"icon-inline\" id=\"31-learnable-queries\"\u003e3.1 Learnable queries\u003ca class=\"icon-link\" href=\"#31-learnable-queries\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA set number of learnable queries is created as input to the image transformer. Instead of taking image patches as input, allocating learnable queries can output the smaller-length $L=32$ features independent of image resolution (since the encoder/decoder-only model outputs the same length sequence as input). The output embeddings from these queries will be forced to extract visual features that capture all the information about the text, by three objectives. Due to such query\u0026rsquo;s importance, BLIP-2\u0026rsquo;s network is also known as the \u003cstrong\u003eQ-Former\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"32-shared-self-attention\" class=\"icon-inline\" id=\"32-shared-self-attention\"\u003e3.2 Shared self-attention\u003ca class=\"icon-link\" href=\"#32-shared-self-attention\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe have observed that BLIP-1\u0026rsquo;s sub-networks have many common points in architectures and even in weights. So why not merge them as possible? and how?\u003c/p\u003e\n\u003cp\u003eThe brilliance of BLIP-2 is that they employ different SA masking strategies for different objective to restrict the interaction between the vision token and text token, so two branches in the Q-Former can share the same SA layers.\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025162242.png\" loading=\"lazy\" width=\"600\"\u003e\n\u003cfigcaption\u003e\nThe self-attention masking strategy (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eAlthough the masking strategies and the joint-training for three objectives sounds very straightforward and reasonable, there are a lot of unclear details waiting to be revealed in the source code.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFor the contrastive learning\u003c/strong\u003e, the query and text will be passed into Q-Former, respectively. Noting that the key\u0026amp;value of SA layers in the image transformer will be cached for later use.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003equery_output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eQformer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebert(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            query_embeds\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003equery_tokens,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            encoder_hidden_states\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimage_embeds,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            encoder_attention_mask\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimage_atts,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            use_cache\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_dict\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etext_output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eQformer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebert(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            text_tokens\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003einput_ids,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            attention_mask\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etext_tokens\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eattention_mask,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_dict\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eFor the image-text matching\u003c/strong\u003e, the query will be concatenated with the text and passed into Q-Former. All queries and texts can attend to each other.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eoutput_itm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eQformer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebert(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            text_ids_all,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            query_embeds\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003equery_tokens_itm,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            attention_mask\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eattention_mask_all,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            encoder_hidden_states\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimage_embeds_all,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            encoder_attention_mask\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimage_atts_all,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_dict\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eFor the language modeling\u003c/strong\u003e, there are only the text (first token is replaced by [DEC]) as input to the Q-Former. As the visual instruction, the previous key\u0026amp;value caches (with pure visual information) will be concatenated with the current key\u0026amp;value (from the texts), and then passed to the SA layers with a multimodal causal mask.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elm_output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eQformer(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            decoder_input_ids,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            attention_mask\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eattention_mask,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            past_key_values\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003equery_output\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epast_key_values,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_dict\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            labels\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003elabels,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cblockquote\u003e\n\u003cp\u003eCompared to BLIP-1, the shared SAs of BLIP2 make the output query embeddings more \u0026ldquo;text-like\u0026rdquo; inevitably, so the image encoder should be frozen to counteract such imbalance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"33-bootstrap-llm\" class=\"icon-inline\" id=\"33-bootstrap-llm\"\u003e3.3 Bootstrap LLM\u003ca class=\"icon-link\" href=\"#33-bootstrap-llm\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eBenefit from the LM objective, the output query embedding contains rich image and text features. So them can function as soft visual prompts to LLM by a simple linear projection.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025213420.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\n(Top) Bootstrapping a decoder-based LLM. (Bottom) Bootstrapping an encoder-decoder-based LLM (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n"
        },
        {
            "title": "Controllable Text-To-Image Diffusion Models —— Explicit Control",
            "date_published": "2023-08-17T00:00:00Z",
            "date_modified": "2023-08-17T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/",
            "content_html": "\u003cp\u003eControllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.\u003c/p\u003e\n\u003cp\u003eAt present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control.\u003c/p\u003e\n\u003cp\u003eThis blog, as the first part of this series, will summarize representative researches related to Explicit Control, which can be further divided into two technical routes: forward guidance and backward guidance. The discussion of implicit control will be updated in another blog post.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePS: This explicit or implicit statement comes from my personal preference and is not yet widely accepted in the academic community.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2 id=\"early-explorations-before-the-stable-diffusion\" class=\"icon-inline\" id=\"early-explorations-before-the-stable-diffusion\"\u003eEarly Explorations before the Stable Diffusion\u003ca class=\"icon-link\" href=\"#early-explorations-before-the-stable-diffusion\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"image-to-image--inpainting\" class=\"icon-inline\" id=\"image-to-image--inpainting\"\u003eImage-to-Image \u0026amp; Inpainting\u003ca class=\"icon-link\" href=\"#image-to-image--inpainting\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2108.01073\"\u003e-\u0026gt; Chenlin Meng, et al. arXiv 2021\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html\"\u003e-\u0026gt; Andreas Lugmayr, et al. CVPR 2022\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe idea of explicit control can date back to before the Stable Diffusion, with two classic examples being SDEdit (Image-to-Image) and RePaint (Inpainting). Due to their relatively simple principles,  here we just briefly summarize them:\u003c/p\u003e\n\u003cp\u003eSDEdit uses a noisy version of a reference image as the starting point for the denoising process, ensuring that the generated image maintains a similar color layout to the reference image. Repaint, during the denoising process, combines the masked background image with the predicted foreground image to realize inpainting new content in the masked regions.\u003c/p\u003e\n\u003cp\u003eBoth of these classic methods achieve controllable generation through modify denoising process (without any training), demonstrating the strong robustness of the diffusion model and the significant potential for modifying the denoising process manually!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn the \u003cstrong\u003eDiffusers\u003c/strong\u003e library, the \u003cem\u003eStableDiffusionInpaintPipeline\u003c/em\u003e has two versions. The unfinetuned version is based on the idea of Repaint without training, while the finetuned version involves concatenating the \u003cem\u003eMask\u003c/em\u003e and \u003cem\u003eMasked_image_latents\u003c/em\u003e as additional 5 channels to the input of Unet and fine-tuning all layers. The latter version achieves better results.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"blended-diffusion\" class=\"icon-inline\" id=\"blended-diffusion\"\u003eBlended Diffusion\u003ca class=\"icon-link\" href=\"#blended-diffusion\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html\"\u003e-\u0026gt; Omri Avrahami, et al. CVPR 2022\u003c/a\u003e\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/DFD12FE9-4E0D-436D-8767-86E51992E1B6_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nA simple diagram of the Blended Diffusion\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWith the introduction of CLIP technology, Blended Diffusion achieves text-driven inpainting. The basic approach can be summarized as above figure. In each iteration,  $\\hat{x_0}$ is predicted based on the inverse equation of the known $p(x_t|x_0)$, and is masked accordingly. Then, the masked image and the text prompt are jointly input into CLIP to obtain a cosine similarity score $\\mathcal{L}$ (with the trick of data augmentation to improve sensitivity). The gradient of this score is used to update the latent $\\epsilon(x_t)+\\nabla_{\\hat{x_0}}\\mathcal{L}$. Moreover, the foreground and background images are fused based on the given mask to update the latent $x_{t-1}$ for next iteration.\u003c/p\u003e\n\u003cp\u003eThis approach is essentially a combination of CLIP-based T2I and Repaint method. In fact, the CLIP-based generation is a kind of classifier-based guidance, where the iteration values are updated based on the gradient of a certain criterion. Although this gradient-based approach may sound a little old-school in the days of Classifier-Free Guidance (CFG), we can still see its effectiveness in serval cutting-edge studies later on.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"forward-guidance\" class=\"icon-inline\" id=\"forward-guidance\"\u003eForward Guidance\u003ca class=\"icon-link\" href=\"#forward-guidance\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"prompt-to-prompt-image-editing-with-cross-attention\" class=\"icon-inline\" id=\"prompt-to-prompt-image-editing-with-cross-attention\"\u003ePrompt-to-Prompt Image Editing with Cross Attention\u003ca class=\"icon-link\" href=\"#prompt-to-prompt-image-editing-with-cross-attention\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2208.01626\"\u003e-\u0026gt; Amir Hertz, et al. arXiv 2022\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePrompt-to-Prompt (P2P) focuses on editing already generated images, which may not be relevant to controllable generation, but it shows great potential of manipulating Cross Attention (CA) maps and brings quite a lot of inspiration to controllable generation techniques!\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106181510.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nCross-Attention visualization in Unet (from the paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAs shown above, P2P first found that CA corresponding to text tokens had strong semantic information (even in the early stages of generation). This finding seems obvious when you realize that CA is the similarity between image features and text features. But no one had visualized them before, and such strong semantic connections were not well known!\u003c/p\u003e\n\u003cp\u003eAt the same time, this observation also reveals how the CA mechanism drives T2I generation. Let\u0026rsquo;s imagine the data flow: the sequence of text features from CLIP is first mapped to base values, then fused at different locations based on attention weights (the similarity between images and texts), and finally makes up the image we see (magical multimodal learning)!\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231107163401.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nImage editing effect achieved by P2P (from the paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eMore excitingly, the authors successfully controlled generation by manipulating CA values:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReplacing the CA corresponding to a text token for content replacement.\u003c/li\u003e\n\u003cli\u003eDeleting the CA corresponding to a text token for content erasure.\u003c/li\u003e\n\u003cli\u003eAdding the CA corresponding to a text token for content addition.\u003c/li\u003e\n\u003cli\u003eWeighting the CA corresponding to a text token to effectively enhance or weaken content.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNow we can enhance or weaken the content by weighting the text features from CLIP directly. This \u003cstrong\u003e\u0026ldquo;weighted prompt\u0026rdquo;\u003c/strong\u003e trick is widely integrated into various T2I tools (such as compel, A1111-webui, Midjourney, etc.).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"compositional-visual-generation-with-composable-diffusion-models\" class=\"icon-inline\" id=\"compositional-visual-generation-with-composable-diffusion-models\"\u003eCompositional Visual Generation with Composable Diffusion Models\u003ca class=\"icon-link\" href=\"#compositional-visual-generation-with-composable-diffusion-models\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2206.01714\"\u003e-\u0026gt; Nan Liu, et al. ECCV 2022\u003c/a\u003e\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/9F007937-5B9C-4D33-A13E-DEC5BC7F7683_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nA simple diagram of the Composed Diffusion\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eComposed Diffusion supports the generation of more content or objects. Although this paper is inspired by composing multiple EBMs, this idea is equivalent to the advanced CFG formally. In short, multiple text prompts separated by the \u0026ldquo;AND\u0026rdquo; symbol are fed into the diffusion model separately and then weighted to produce the results:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{\\epsilon}(x_t,t)=\\epsilon(x_t,t)+\\sum_{i=1}^{n}w_i(\\epsilon(x_t,t|c_i)-\\epsilon(x_t,t))\n$$\u003c/p\u003e\n\u003cp\u003eThis synthesis is a bit crude and often less than expected, because different content will merge globally without explicit location constraints. Therefore, Composed Diffusion is more suitable for those multi-content with large spatial differences to avoid unexpected conflicts.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106202519.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nComposing effect achieved by Composed Diffusion (from the paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"multidiffusion-fusing-diffusion-paths-for-controlled-image-generation\" class=\"icon-inline\" id=\"multidiffusion-fusing-diffusion-paths-for-controlled-image-generation\"\u003eMultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation\u003ca class=\"icon-link\" href=\"#multidiffusion-fusing-diffusion-paths-for-controlled-image-generation\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2302.08113\"\u003e-\u0026gt; Omer Bar-Tal, et al. arXiv 2023\u003c/a\u003e\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/0172B6F3-2F6E-49E8-82EF-07F6EB37DEF0_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nA simple diagram of the MultiDiffusion\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIf we want more control over spatial position, the most direct way is to draw something in a certain region by specifying a target mask or box. While this sounds like a multi-region inpainting task, the question is how to ensure that they are independent and harmonious with each other?\u003c/p\u003e\n\u003cp\u003eMultiDiffusion gives the answer. The authors first build a mathematical optimization model for harmonious multi-region generation and derive a closed-form solution:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\n\\Psi\\left(J_t \\mid z\\right)=\\arg\\min_{J\\in\\mathcal{J}}\\sum_{i=1}^n\\left|W_i \\otimes\\left[F_i(J)-\\Phi\\left(I_t^i \\mid y_i\\right)\\right]\\right|^2 \\\\\n\\Psi\\left(J_t \\mid z\\right)=\\sum_{i=1}^n \\frac{F_i^{-1}\\left(W_i\\right)}{\\sum_{j=1}^n F_j^{-1}\\left(W_j\\right)} \\otimes F_i^{-1}\\left(\\Phi\\left(I_t^i \\mid y_i\\right)\\right)\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eIn fact, MultiDiffusion can be understood as a multi-region extension of Repaint. It fuses multiple images generated from different regions (depending on the degree of overlap) in each step, and then eliminates dissonance at the boundaries solely by the robustness of the denoising process. Obviously, due to the lack of interaction between the central features of different regions, the final generated image has obvious style variations across regions.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe derived fusion equation is intuitive and can be obtained even without establishing the optimization problem. But it does make the whole paper more academic\u0026hellip; -_-\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis\" class=\"icon-inline\" id=\"training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis\"\u003eTraining-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis\u003ca class=\"icon-link\" href=\"#training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2212.05032\"\u003e-\u0026gt; Weixi Feng, et al. ICLR 2023\u003c/a\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/C5A36A3D-3958-4026-B90B-791D289DE33C_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nA simple diagram of the Structured Diffusion\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eSimilar to the goal of Composed Diffusion, Structured Diffusion seeks to synthesize more content through text input alone. The approach derives from two key observations: first, the Cross-Attention of the input text token is very semantic (also observed by P2P); Second, CLIP\u0026rsquo;s text encoder (based on Transformer) will inevitably incorporate contextual information for each token.\u003c/p\u003e\n\u003cp\u003eTherefore, Structured Diffusion attempts to ensure that specific text is not affected by context so that the corresponding content is properly synthesized. They first strip all the noun phrases from a sentence using a parser like Constituency Tree or Scene Graph; Then input these noun phrases into the CLIP text encoder separately to avoid mutual influence; And fill in the rest with the features of the whole sentence $W_p$; Finally, in the Unet, these new text features $W_i$ will be mapped into new values $V_i$, and fused according to the old CA $M^t$:\u003c/p\u003e\n\u003cp\u003e$$\nO^t=\\frac{1}{n}\\sum_{i=1}^{n}(M^tV_i)\n$$\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote that attention is still computed from the original whole-sentence query, which contribute to the spatial layout allocation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThus, we enhance the features of all noun phrases and reduce their interference with each other. Unfortunately, the actual improvement from this approach is not obvious.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"backward-guidance\" class=\"icon-inline\" id=\"backward-guidance\"\u003eBackward Guidance\u003ca class=\"icon-link\" href=\"#backward-guidance\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"training-free-layout-control-with-cross-attention-guidance\" class=\"icon-inline\" id=\"training-free-layout-control-with-cross-attention-guidance\"\u003eTraining-Free Layout Control with Cross-Attention Guidance\u003ca class=\"icon-link\" href=\"#training-free-layout-control-with-cross-attention-guidance\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2304.03373\"\u003e-\u0026gt; Minghao Chen, et al. arXiv 2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis paper attempts to implement control generation for multiple regions with a single diffusion model, called layout control. Interestingly, the authors explore two different methods of guidance: 1. a region constraint imposed in the generation iteration, also known as forward guidance; 2. a gradient-based update idea similar to Blended Diffusion, also known as backward guidance.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/98AB3A71-A285-471A-A6AD-ADF1A157F1AF_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nA simple diagram of the Layout Control\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn particular, the forward guidance will modify the Cross-Attention of the specified text tokens (in each layer) to cluster them into the specified box:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{A_{p,i}}=(1-\\lambda)A_{p,i}+\\lambda g_p\\sum_{j}A_{j,i}\n$$\u003c/p\u003e\n\u003cp\u003eWhere $p$ is the position coordinate, $i$ is the index of the text token, $g_p$ is the gaussian weight in a specified box (sum of 1), and $\\sum_{j}A_{j,i}$ is the sum of the entire CA Map. It means that the latter term in the equation will redistribute the attention weights according to the gaussian box.\u003c/p\u003e\n\u003cp\u003eBackward guidance defines a function to measure the CA concentration of a given token over a given box $B$:\u003c/p\u003e\n\u003cp\u003e$$\nE(A,B,i)=\\left(1-\\frac{\\sum_{p\\in B}A_{p,i}}{\\sum_{p}A_{p,i}}\\right)^2\n$$\u003c/p\u003e\n\u003cp\u003eWe update the latent $x_t$ at the current time step with the gradient of this metric as shown in the figure.\u003c/p\u003e\n\u003cp\u003eAccording to the experimental results, the backward guidance is more effective (and of course consumes more running time). In addition, the author also integrates this method with Dreambooth and Text Inversion, which still achieve a good control effect.\u003c/p\u003e\n\u003ch3 id=\"attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models\" class=\"icon-inline\" id=\"attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models\"\u003eAttend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models\u003ca class=\"icon-link\" href=\"#attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.1145/3592116\"\u003e-\u0026gt; Hila Chefer et al. ACM Trans. Graph. 2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to the observations in P2P, this paper assumes that if something is not generated as expected, the corresponding Cross-Attention should be increased. The difference, however, is that Attend-and-Excite establishes a certain metric as a loss function and backpropagates the gradient to excite the update of latents, rather than forcing up the corresponding value by direct re-weighting.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/90C7799C-B173-4CF9-B8FA-07BB9958EEA6_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nAttend-and-Excite\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eSpecifically, we first need to select a few text tokens as the target to be enhanced (for example, the 2nd and 5th), calculate their corresponding average cross-attention ($A_t^2,A_t^5$), and then take the smallest peak of them as the loss:\u003c/p\u003e\n\u003cp\u003e$$\nLoss=max\\left(1-max(A_t^2),1-max(A_t^5)\\right)\n$$\u003c/p\u003e\n\u003cp\u003eFinally, the latent is updated according to the gradient as $x_t^{\\prime}=x_t-\\alpha \\nabla_{x_t} Loss$, which encourages the peak cross-attention of all selected text tokens to be as high as possible. Of course, there are several limitations of Attend-and-Excite as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eManually selecting\u003c/strong\u003e the index of the enhanced token is not a convenient and perfect solution.\u003c/li\u003e\n\u003cli\u003eIf and only the excitation in \u003cstrong\u003ethe middle scale of Unet\u003c/strong\u003e is effective. Because not all scales of cross-attention have semantic information and contribute semantically to the final generation.\u003c/li\u003e\n\u003cli\u003eThis involves \u003cstrong\u003etoo many hyperparameter settings\u003c/strong\u003e, such as artificial thresholds that determine the number of backward updates during early iterations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing\" class=\"icon-inline\" id=\"dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing\"\u003eDragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing\u003ca class=\"icon-link\" href=\"#dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2306.14435\"\u003e-\u0026gt; Yujun Shi, et al. arXiv 2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDragDiffusion is a direct extension of the famous DragGAN. Similarly, DragDiffusion can achieve image editing by dragging multiple points, and the principle behind it is the backward guidance for the generated path by gradient-based update.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/42F9A785-50FF-4340-8262-2EE338776B96_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nAttend-and-Excite\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAs shown above, in addition to the regular diffusion denoising, the generated path in DragDiffusion contains two other mappings: Motion Supervision and Point Tracking. The loss function of Motion Supervision is shown as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}\\left(z_t^k\\right)= \\sum_{i=1}^n \\sum_{q \\in \\Omega\\left(h_i^k, r_1\\right)}\\lVert F_{q+d_i}\\left(z_t^k\\right)-\\operatorname{sg}\\left(F_q\\left(z_t^k\\right)\\right) \\rVert_1  +\\lambda \\lVert \\left(z_{t-1}^k-\\operatorname{sg}\\left(z_{t-1}^0\\right)\\right) \\odot(1-M) \\rVert_1\n$$\u003c/p\u003e\n\u003cp\u003eIn simple terms, the former of this loss encourages feature movement at the patch level $F_{\\Omega+d_i}(z_t^k) \\gets F_{\\Omega}(z_t^k)$, while the latter keeps the pixel values outside the mask nearly constant $(1-M)z_t^k \\approx (1-M)z_t^{k+1}$.\u003c/p\u003e\n\u003cp\u003eHere $sg(\\cdot)$ is the stop gradient operator, which makes the pixel values $F_{\\Omega}(z_t^k)$ in the original patch are not affected by gradient descent directly. We can calculate the gradient and update the latent as follows:\u003c/p\u003e\n\u003cp\u003e$$\nz_t^{k+1}=z_t^k-\\eta \\cdot \\nabla_{z_t^k} \\mathcal{L}\n$$\u003c/p\u003e\n\u003cp\u003ePoint Tracking is used to rematch the location of the handle points, which is implemented directly using a nearest neighbor search:\u003c/p\u003e\n\u003cp\u003e$$\nh_i^{k+1}=\\underset{q \\in \\Omega\\left(h_i^k, r_2\\right)}{\\arg \\min }\\left|F_q\\left(z_t^{k+1}\\right)-F_{h_i^k}\\left(z_t\\right)\\right|_1\n$$\u003c/p\u003e\n\u003cp\u003eAlthough this method directly follows DragGAN\u0026rsquo;s idea, the non-stationary iterative process of the Diffusion Models makes many hyperparameter different, and the author also abandons CFG to avoid large numerical errors.\u003c/p\u003e\n"
        },
        {
            "title": "Two Inpainting Pipelines in Diffusers",
            "date_published": "2023-07-20T00:00:00Z",
            "date_modified": "2023-07-20T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/",
            "url": "https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/",
            "content_html": "\u003ch2 id=\"guided-generation\" class=\"icon-inline\" id=\"guided-generation\"\u003eGuided Generation\u003ca class=\"icon-link\" href=\"#guided-generation\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch2 id=\"hybrid-condition-by-fine-tuning\" class=\"icon-inline\" id=\"hybrid-condition-by-fine-tuning\"\u003eHybrid-condition by Fine-Tuning\u003ca class=\"icon-link\" href=\"#hybrid-condition-by-fine-tuning\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n"
        },
        {
            "title": "Network Design in Stable Diffusion",
            "date_published": "2023-02-01T00:00:00Z",
            "date_modified": "2023-02-01T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/",
            "url": "https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/",
            "content_html": "\u003cp\u003eStabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on \u003ca href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html\"\u003elatent diffusion\u003c/a\u003e, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.\u003c/p\u003e\n\u003ch2 id=\"unet\" class=\"icon-inline\" id=\"unet\"\u003eUnet\u003ca class=\"icon-link\" href=\"#unet\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/4320ED37-C913-487C-AF89-3563E40F5828_1_201_a.jpeg\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nFig. 1: Overall of the Unet in Stable Diffusion 1.x \u0026amp; 2.x\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAs in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x \u0026amp; SD2.x, $l_1=l_2=l_3=l_4=2$.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/B431465F-81A1-404F-9AA0-1474625D7B44_1_201_a.jpeg\" loading=\"lazy\" width=\"600\"\u003e\n\u003cfigcaption\u003e\nFig. 2: Skip-connection in the 2nd scale.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer\u0026rsquo;s input as in Fig. 2.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/A012E8AE-8131-4160-8158-03EEC2A01A61_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nFig 3: Transformer\u0026rsquo;s Design\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follow\u001ds the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BA68D415-0DF9-4CB5-B0F4-59EC7C8033E5_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nFig. 4: Resnet block\u0026rsquo;s design.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling.\u003c/p\u003e\n\u003cp\u003e$$\n\\text{SiLU}(z)=z*\\text{sigmoid}(z)\n$$\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BFE0937A-36CA-4CC4-859F-BFC0F9F3EC51_1_201_a.jpeg\" loading=\"lazy\" width=\"500\"\u003e\n\u003cfigcaption\u003e\nFig. 5: Timestep Embedding (based on positional encoding)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"vae\" class=\"icon-inline\" id=\"vae\"\u003eVAE\u003ca class=\"icon-link\" href=\"#vae\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don\u0026rsquo;t have the same weight (SD2\u0026rsquo;s VAE might be fine-tuned for higher resolution $768\\times 768$).\u003cbr\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/CB9453C4-4906-4188-B99B-1186CDEDA771_1_201_a.jpeg\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nFig.6: VAE Encoder\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/774E90D6-50FD-4AD7-800D-D0B903C6549A_1_201_a.jpeg\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nFig.6: VAE Decoder\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"tiled-processing\" class=\"icon-inline\" id=\"tiled-processing\"\u003eTiled Processing\u003ca class=\"icon-link\" href=\"#tiled-processing\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn inference, VAE decoding often occupies a lot of memory.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.\u003c/p\u003e\n\u003cp\u003eThis is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.\u003c/p\u003e\n\u003ch1 id=\"references\" class=\"icon-inline\" id=\"references\"\u003eReferences\u003ca class=\"icon-link\" href=\"#references\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/huggingface/diffusers\"\u003ehttps://github.com/huggingface/diffusers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Stability-AI/stablediffusion\"\u003ehttps://github.com/Stability-AI/stablediffusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Stability-AI/generative-models\"\u003ehttps://github.com/Stability-AI/generative-models\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "Inverse Problem × Diffusion -- Part: B",
            "date_published": "2022-12-20T00:00:00Z",
            "date_modified": "2022-12-20T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/",
            "content_html": "\u003ch2 id=\"ddrm\" class=\"icon-inline\" id=\"ddrm\"\u003eDDRM\u003ca class=\"icon-link\" href=\"#ddrm\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2201.11793\"\u003e-\u0026gt; Bahjat Kawar, et al. NeurIPS, 2022.\u003c/a\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018160625.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nIllustration of DDRM (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003ch3 id=\"transformation-via-svd\" class=\"icon-inline\" id=\"transformation-via-svd\"\u003eTransformation via SVD\u003ca class=\"icon-link\" href=\"#transformation-via-svd\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2021/hash/b5c01503041b70d41d80e3dbe31bbd8c-Abstract.html\"\u003eSNIPS\u003c/a\u003e, DDRM consider the singular value decomposition (SVD) of the sampling matrix $H$ as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\ny\u0026amp;=Hx+z\\\ny\u0026amp;=U\\Sigma V^\\top x+z\\\n\\Sigma^{†} U^{\\top}y\u0026amp;=V^\\top x+\\Sigma^{†} U^{\\top}z\\\n\\bar{y}\u0026amp;=\\bar{x}+\\bar{z}\\\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eSince $U$ is orthogonal matrix, we have $p(U^\\top z) = p(z) = \\mathcal{N}(0,\\sigma^2_y I)$, resulting $\\bar{z}^{(i)}=(\\Sigma^{†} U^{\\top}z)^{(i)} \\sim \\mathcal{N}(0, \\frac{\\sigma^2_y}{s_i^2}I)$. So after these, we transform $x$ and $y$ into the same field (\u003cstrong\u003espectral space\u003c/strong\u003e), and these two only differ by the noise $\\bar{z}$, which can be drawn as follows:\u003c/p\u003e\n\u003cp\u003e$$\nq(\\bar{y}^{(i)}|x_0)=\\mathcal{N}(\\bar{x}_0^{(i)},\\sigma_y^2/s_i^2 )\n$$\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDDRM might be unable to cope with the wild scene, cause the variance $\\sigma_y$ is often unknown.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"conditional-diffusion--generation\" class=\"icon-inline\" id=\"conditional-diffusion--generation\"\u003eConditional Diffusion \u0026amp; Generation\u003ca class=\"icon-link\" href=\"#conditional-diffusion--generation\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe desired conditional diffusion process should (a) be a tractable Gaussian distribution, (b) employ the known $y$ to construct noisy samples as possible and (c) ensure the original marginal:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\nq(x_t|x_0) = q(\\bar{x}_t|x_0, y)\\cdot q(\\bar{y}|x_0)=\\mathcal{N}(\\bar{x}_0,\\sigma_t^2I) \\\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eWe note that $q(\\bar{y}^{(i)}|x_0)=\\mathcal{N}(\\bar{x}_0^{(i)},\\sigma_y^2/s_i^2 )$, so these conditional processes can be defined as follows:\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018141254.png\" alt=\"image.png\"\u003e\nFrom the perspective of DDIM, we can deduce the corresponding generative process via replacing $x_0$ with \u0026ldquo;the predicted version\u0026rdquo; as follows:\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018153006.png\" alt=\"image.png\"\u003e\nIntuitively, this construction considers different cases for each index of the spectral space. (i) If the corresponding singular value $s_i=0$, then $y$ does not directly provide any information to that index, and the update is similar to regular unconditional generation. (ii) If $s_i \u0026gt;0$, then the updates consider the information provided by $y$, which further depends on whether the measurements’ noise level $\\sigma_y/s_i$ in the spectral space is larger than the noise level in the diffusion model or not.\u003c/p\u003e\n\u003cp\u003eIn the resulting generation, the initial sample carries a few information from $\\bar{y}$, then is updated eventually by the guidance of $p_\\theta(\\bar{x}_t|x_t+1,y)$,  and is recovered to $x_0$ exactly by left multiplying $V$.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAlthough this conditional process results in a more complex ELBO objective for training, the authors proof that an optimal solution to DDPM / DDIM can also be an optimal solution to a DDRM problem, under some similar assumptions as in DDIM. So \u003cstrong\u003eDDRM can be training-free\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"one-more-thing\" class=\"icon-inline\" id=\"one-more-thing\"\u003eOne more thing\u003ca class=\"icon-link\" href=\"#one-more-thing\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eDDRM is applied in super-resolution, deblurring, inpainting, and colorization. There\u0026rsquo;re no much difference of the value space between original $x$ and degraded $y$ in these task. These data are almost all in the perceptible pixel space. So does SVD really work as claimed? This paper lacks relevant ablation study for that. Maybe we should introduce this into more tasks, such as compressive sensing, and see what will happen.\u003c/p\u003e\n\u003ch1 id=\"references\" class=\"icon-inline\" id=\"references\"\u003eReferences\u003ca class=\"icon-link\" href=\"#references\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eKawar, Bahjat, Gregory Vaksman, and Michael Elad. \u0026ldquo;SNIPS: Solving noisy inverse problems stochastically.\u0026rdquo; \u003cem\u003eAdvances in Neural Information Processing Systems\u003c/em\u003e 34 (2021): 21757-21769.\u003c/li\u003e\n\u003cli\u003eKawar, Bahjat, et al. \u0026ldquo;Denoising diffusion restoration models.\u0026rdquo; \u003cem\u003eAdvances in Neural Information Processing Systems\u003c/em\u003e 35 (2022): 23593-23606.\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "Inverse Problem × Diffusion -- Part: A",
            "date_published": "2022-12-19T00:00:00Z",
            "date_modified": "2022-12-19T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/",
            "content_html": "\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;An inverse problem seeks to recover an unknown signal from a set of observed measurements. Specifically, suppose $x\\in R^n$ is an unknown signal, and $y\\in R^m = Ax+z$ is a noisy observation given by m linear measurements, where the measurement acquisition process is represented by a linear operator $A\\in R^{m\\times n}$, and $z\\in R^n$ represents a noise vector. Solving a linear inverse problem amounts to recovering the signal $x$ from its measurement $y$. Without further assumptions, the problem is ill-defined when $m\u0026lt; n$, so we additionally assume that $x$ is sampled from a prior distribution $p(x)$\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003esource from \u003ca href=\"http://arxiv.org/abs/2111.08005\"\u003eDr. Yang Song\u0026rsquo;s paper\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAs unconditional generative model, Diffusion models (or Score models) can play a prior term in optimization for various ill-posed image inverse problems. Such methods are often labeled as training-free, zero-shot, unsupervised, etc.\u003c/p\u003e\n\u003cp\u003eOn the other hand, Diffusion models also can be trained as conditional one $s_\\theta(x_t,y,t)$, where $y$ is the condition from other mode. However, this will consume a lot of resources both in the computing power and the collection of paired data $\\lbrace x_i,y_i\\rbrace$.\u003c/p\u003e\n\u003ch2 id=\"medical-score-sde\" class=\"icon-inline\" id=\"medical-score-sde\"\u003eMedical Score-SDE\u003ca class=\"icon-link\" href=\"#medical-score-sde\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2111.08005\"\u003e-\u0026gt; Yang Song, et al. in ICLR, 2022.\u003c/a\u003e\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017172654.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe medical imaging problem (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"step1-perturbing-measurements\" class=\"icon-inline\" id=\"step1-perturbing-measurements\"\u003estep1: Perturbing measurements\u003ca class=\"icon-link\" href=\"#step1-perturbing-measurements\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSince $x_t=\\alpha(t)x_0+\\beta(t)z$, and we set $y_t=Ax_t+\\alpha(t)\\epsilon$, so we have $y_t=\\alpha(t)y+\\beta(t)Az$.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSmart and reasonable assumption for the coefficient $+\\alpha(t)\\epsilon$ to avoid the subsequent diffusion of random measurement noise $\\epsilon$.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"step2-generation-with-score-sde\" class=\"icon-inline\" id=\"step2-generation-with-score-sde\"\u003estep2: Generation with Score-SDE\u003ca class=\"icon-link\" href=\"#step2-generation-with-score-sde\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eUsing the Euler-Maruyama sampler, the generative process given by:\u003c/p\u003e\n\u003cp\u003e$$\nx_{t-\\Delta t} = x_t-f(t)x_t\\Delta t+g(t)^2s_\\theta(x_t,t)\\Delta t+g(t)\\sqrt{\\Delta t}z\n$$\u003c/p\u003e\n\u003cp\u003ewhere $s_\\theta(\\cdot)$ is the trained score model, $\\Delta t = 1/N$ is the set discrete time, and $z\\sim \\mathcal{N}(0,1)$.\u003c/p\u003e\n\u003ch3 id=\"step3-consistent-guidance\" class=\"icon-inline\" id=\"step3-consistent-guidance\"\u003estep3: Consistent Guidance\u003ca class=\"icon-link\" href=\"#step3-consistent-guidance\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017164345.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nThe pipeline of medical Score-SDE\u003c/figcaption\u003e\n\u003c/figure\u003e\n\nWe set the guided intermediate result as $x_t^{\\prime}$, For simultaneously minimizing the distance between $x_t$ and $x_t^{\\prime}$, and the distance between $x_t^{\\prime}$ and the hyperplane $\\lbrace x\\in R^n| Ax=y_t \\rbrace$, we build an  optimization problem as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\nx_t^{\\prime}\u0026amp;=\\arg \\min_{v\\in R^n}\\lbrace (1-\\lambda)\\Vert v-x_t \\Vert_{T}^2 + \\min_{u\\in R^n} \\lambda \\Vert v-u \\Vert_{T}^2 \\rbrace \\\ns.t.\\quad Au\u0026amp;=y_t\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003ewhich has the closed-form solution as:\u003c/p\u003e\n\u003cp\u003e$$\nx_t^{\\prime}=T^{-1}[\\lambda \\Lambda \\mathcal{P} ^{-1}(\\Lambda)y_t+(1-\\lambda)\\Lambda Tx_t+(1-\\Lambda)Tx_t]\n$$\u003c/p\u003e\n\u003cp\u003eWhen the measurement process is noisy, we can choose $0\u0026lt;\\lambda\u0026lt;1$ to allow slackness in $Ax_t^{\\prime}=y_t$ (more affected by $p(x)$). When the measurement process contains no noise, the authors chosse $\\lambda=1$ at the last sampling step to guarantee $Ax_0^{\\prime}=y$.\u003c/p\u003e\n\u003ch2 id=\"score-mri\" class=\"icon-inline\" id=\"score-mri\"\u003eScore-MRI\u003ca class=\"icon-link\" href=\"#score-mri\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S1361841522001268\"\u003e-\u0026gt; Hyungjin Chung, et al.  Medical Image Analysis, 2022.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to the above, Score-MRI inserts a more simple consistency mapping after every \u0026ldquo;Predictor\u0026rdquo; and \u0026ldquo;Corrector\u0026rdquo; iteration.\u003c/p\u003e\n\u003cp\u003e$$\nx_i=x_i+\\lambda A^\\ast(y-Ax_i)=(I-\\lambda A^\\ast A)x_i+A^\\ast y\n$$\u003c/p\u003e\n\u003cp\u003ewhere $A^\\ast$ denotes the Hermitian adjoint, and $+\\lambda A^\\ast (y-Ax_i)$ can be viewed as a rectification for minimizing the distance between $y$ and $Ax_i$. In the hybridtype Algorithm 5 (complex domain and multi-coils for MRI), the authors start with $\\lambda = 1.0$ in the first iteration, and linearly decrease the value to $\\lambda=0.2$ at the last iteration.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/1706.00051\"\u003eGANCS\u003c/a\u003e has an similar rectification $(I-\\Phi^{†}\\Phi)x+\\Phi^{†}y$, where pseudo-inverse $\\Phi^{†}$ satisfies $\\Phi\\Phi^{†}\\Phi=\\Phi$, but without the scaling factor $\\lambda$.  In this paper, the authors view $(I-\\Phi^{†}\\Phi)x$ as a projection onto the \u003ca href=\"https://dx.doi.org/10.1088/1361-6420/aaf14a\"\u003enullspace\u003c/a\u003e of $\\Phi$.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017182641.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"ddnm\" class=\"icon-inline\" id=\"ddnm\"\u003eDDNM\u003ca class=\"icon-link\" href=\"#ddnm\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://arxiv.org/abs/2212.00490\"\u003e-\u0026gt; Yinhuai Wang, et al. in ICLR, 2023.\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"noise-free\" class=\"icon-inline\" id=\"noise-free\"\u003eNoise-free\u003ca class=\"icon-link\" href=\"#noise-free\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to \u003ca href=\"http://arxiv.org/abs/1706.00051\"\u003eGANCS\u003c/a\u003e, DDNM adopts a rectified estimation as:\u003c/p\u003e\n\u003cp\u003e$$\nx_{0|t}^{\\prime}=A^{†}y+(I-A^{†}A)x_{0|t}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $x_{0|t}$ is the predicted version at timestep $t$. Based on \u003ca href=\"http://arxiv.org/abs/2010.02502\"\u003eDDIM\u003c/a\u003e, the generative process can be driven by $x_{t-1}\\sim \\mathcal{p}(x_{t-1}|x_t,x_{0|t}^{\\prime})$. So it might provide a more reliable rectification to human-perceptible $x_{0|t}$, rather than noisy $x_t$ like in \u003ca href=\"http://arxiv.org/abs/2010.02502\"\u003eScore-MRI\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAlthough this paper mentioned that this consistency mapping was inspired by \u003cstrong\u003erange-null space decomposition\u003c/strong\u003e $x=A^{†}Ax+(I-A^{†}A)x$, but it seems to contradict with the subsequent DDNM+ for the noisy inverse problem.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"noisy\" class=\"icon-inline\" id=\"noisy\"\u003eNoisy\u003ca class=\"icon-link\" href=\"#noisy\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e$$\nx_{0|t}^{\\prime}=A^†y+(I−A^†A)x_{0|t} =x_{0|t}-A^†(Ax_{0|t}-y)\n$$\u003c/p\u003e\n\u003cp\u003eIn short, the authors employ two scale factors $\\Sigma_t$ \u0026amp; $\\Phi_t$ into range-space correction \u0026amp; generative variance, respectively.\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\nx_{0|t}^{\\prime}\u0026amp;= x_{0|t}-\\Sigma_tA^†(Ax_{0|t}-y)\\\\\np^{\\prime}(x_{t-1}|x_t,x_{0|t}^{\\prime})\u0026amp;=\\mathcal{N}(x_{t-1};\\mu_t(x_t, x_{0|t}^{\\prime}),\\Phi_t I)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHowever, the performance gains from these factor are not clear, due to the absence of ablations experiments. It looks like the \u0026ldquo;Time-Travel\u0026rdquo; (a kind of redundant computing) helped a lot.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017212649.png\" loading=\"lazy\" width=\"300\"\u003e\n\u003cfigcaption\u003e\nTime-Travel (source from paper)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch1 id=\"references\" class=\"icon-inline\" id=\"references\"\u003eReferences\u003ca class=\"icon-link\" href=\"#references\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eYang Song and others, ‘Solving Inverse Problems in Medical Imaging with Score-Based Generative Models’  in \u003cem\u003eInternational Conference on Learning Representations\u003c/em\u003e, 2022.\u003c/li\u003e\n\u003cli\u003eHyungjin Chung and Jong Chul Ye, ‘Score-Based Diffusion Models for Accelerated MRI’, \u003cem\u003eMedical Image Analysis\u003c/em\u003e, 80 (2022), 102479.\u003c/li\u003e\n\u003cli\u003eJohannes Schwab, Stephan Antholzer, and Markus Haltmeier, ‘Deep Null Space Learning for Inverse Problems: Convergence Analysis and Rates’, \u003cem\u003eInverse Problems\u003c/em\u003e, 35.2 (2019), 025008.\u003c/li\u003e\n\u003cli\u003eMorteza Mardani and others, ‘Deep Generative Adversarial Networks for Compressed Sensing Automates MRI’ arXiv, 2017.\u003c/li\u003e\n\u003cli\u003eYinhuai Wang, Jiwen Yu, and Jian Zhang, ‘Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model’ in \u003cem\u003eInternational Conference on Learning Representations\u003c/em\u003e, 2023.\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "DDPM and Early Variants",
            "date_published": "2022-12-13T00:00:00Z",
            "date_modified": "2022-12-13T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2212ddpm/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2212ddpm/",
            "content_html": "\u003cblockquote\u003e\n\u003cp\u003eAlthough Diffusion Model is a new generative framework, it still has many shades of other methods.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nBayes\u0026rsquo; rule is all you need\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"generation--diffusion\" class=\"icon-inline\" id=\"generation--diffusion\"\u003eGeneration \u0026amp; Diffusion\u003ca class=\"icon-link\" href=\"#generation--diffusion\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eJust like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following \u003cstrong\u003eMarkov chain\u003c/strong\u003e with learnable Gaussian transitions:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\np_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}\\\\\np_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta\\left(x_{t-1} \\vert x_t\\right)\\\\\np_\\theta\\left(x_{t-1} \\vert x_t\\right):=N(x_{t-1};\\mu_{\\theta}(x_t,t),\\Sigma_\\theta(x_t,t))\n\\end{align}\n$$\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eMarkov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\\vert x_{t:T})=p(x_{t-1}\\vert x_{t})$\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSimilar to VAE, we can use the posterior $q(x_{1:t} \\vert x_0)$ to do the estimation for $\\theta$. The difference is that $x_1,\\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \\vert x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\\alpha_{1:T}\\in [0,1]^T$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align}\nq(x_{1:T} \\vert x_0) := \\prod_{t=1}^T q\\left(x_{t} \\vert x_{t-1}\\right) \\\\\nq(x_t \\vert x_{t-1}):=N(\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}}x_{t-1}, (1-\\frac{\\alpha_t}{\\alpha_{t-1}})I)\n\\end{align}\n$$\u003c/p\u003e\n\u003cp\u003eA nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$:\u003c/p\u003e\n\u003cp\u003e$$\nq(x_t\\vert x_0)=N(x_t;\\sqrt{\\alpha_t}x_0, (1-\\alpha_t)I)\n$$\u003c/p\u003e\n\u003ch2 id=\"training-objective\" class=\"icon-inline\" id=\"training-objective\"\u003eTraining Objective\u003ca class=\"icon-link\" href=\"#training-objective\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood:\u003c/p\u003e\n\u003cp\u003e$$\n\\max_{\\theta}E_{q}[\\log{p_\\theta(x_0)}]\\leq \\max_{\\theta}E_{q}[\\log{p_{\\theta} (x_{0:T})}-\\log{q(x_{1:T} \\vert x_0)}]\n$$\u003c/p\u003e\n\u003cp\u003ewhich also can be driven by Jensen’s inequality as in \u003ca href=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\" title=\"Lil'log\"\u003eLil\u0026rsquo;log\u003c/a\u003e. And we can further rewrite this object as:\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eUsing Bayes\u0026rsquo; rule, we can deduce the fact that $q(x_{t-1}\\vert x_t, x_0)$ is also a gaussian distribution.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ewhere $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs.  \u003cstrong\u003eIf generative variances is all fixed  $\\Sigma_t = \\sigma^2_t$\u003c/strong\u003e, using parameterization (fit distribution $\\to$ fit mean $\\to$ predict noise) and reweighting based on the empirical results, we can simplify this objective as follows:\u003c/p\u003e\n\u003cp\u003e$$\nL_t=E_{x_0\\sim q, \\epsilon\\sim N(0,1)}\\left[|| \\epsilon_\\theta(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon, t)-\\epsilon {||}_2^2 \\right]\n$$\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNCSN vs DDPM, different ways lead to almost the same objective!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor last $L_0$, DDPMs treat it as an independent discrete decoder derived from $N(x_0;\\mu_{\\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.\u003c/p\u003e\n\u003cp\u003eAt the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\\theta(x_{t-1}\\vert x_t)$ using predicted $\\epsilon_\\theta(\\cdot)$ (Alg. 2).\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"ddpm\" class=\"icon-inline\" id=\"ddpm\"\u003eDDPM+\u003ca class=\"icon-link\" href=\"#ddpm\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality\" class=\"icon-inline\" id=\"finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality\"\u003eFinding 1: Why fixing $\\sigma^2$ to $\\beta$ or$\\tilde{\\beta}$ achieve similar sample quality?\u003ca class=\"icon-link\" href=\"#finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAs $\\beta \\approx \\tilde{\\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\\frac{\\beta_t}{\\beta_{t-1}} \\to 0$.\u003c/p\u003e\n\u003ch3 id=\"finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly\" class=\"icon-inline\" id=\"finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly\"\u003eFinding 2: The diffusion samples from linear schedule lose information very quickly.\u003ca class=\"icon-link\" href=\"#finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png\" loading=\"lazy\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggests that so much full noisy samples might do not contribute to generation.\u003c/p\u003e\n\u003ch3 id=\"improvements\" class=\"icon-inline\" id=\"improvements\"\u003eImprovements\u003ca class=\"icon-link\" href=\"#improvements\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLearnable variances with an interpolation between $\\beta$ and $\\tilde{\\beta}$, driven by loss $+ \\lambda L_{vlb}$.\u003c/li\u003e\n\u003cli\u003eCosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end.\u003c/li\u003e\n\u003cli\u003eResampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"ddim\" class=\"icon-inline\" id=\"ddim\"\u003eDDIM\u003ca class=\"icon-link\" href=\"#ddim\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"more-free-diffusion-chain\" class=\"icon-inline\" id=\"more-free-diffusion-chain\"\u003eMore free diffusion chain\u003ca class=\"icon-link\" href=\"#more-free-diffusion-chain\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIn DDIM, the authors introduce a extended version $q(x_{t-1}\\vert x_t, x_0)$, which has the same marginal noise distribution $q(x_{t}\\vert x_0)$：\u003c/p\u003e\n\u003cp\u003e$$\nq_{\\sigma}(x_{t-1} \\vert x_t, x_0) = N(x_{t-1}; \\sqrt{\\alpha_{t-1}} x_0 + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\frac{x_t - \\sqrt{\\alpha_t} x_0}{\\sqrt{1 - \\alpha_t}}, \\sigma_t^2 I)\n$$\u003c/p\u003e\n\u003cp\u003eTherefore, the corresponding generative process can be exported as:\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013230807.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\nIf we set $\\sigma_t^2=\\tilde{\\beta}_t$,  the diffusion process becomes Markovian, and the generative process becomes DDPM. And if $\\sigma_t=0$, there is a deterministic generation, called DDIM.\u003c/p\u003e\n\u003ch3 id=\"acceleration-via-subsampling\" class=\"icon-inline\" id=\"acceleration-via-subsampling\"\u003eAcceleration via subsampling\u003ca class=\"icon-link\" href=\"#acceleration-via-subsampling\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIn the generation, we sample a subset of S steps ${\\tau_1,\\dots,\\tau_S}$ to form a new chain as:\u003c/p\u003e\n\u003cp\u003e$$\nq_{\\sigma, \\tau}(x_{\\tau_{i-1}} \\vert x_{\\tau_i}, x_0) = N(x_{\\tau_{i-1}}; \\sqrt{\\alpha_{\\tau_{i-1}}} x_0 + \\sqrt{1 - \\alpha_{\\tau_{i-1}} - \\sigma_t^2} \\frac{x_{\\tau_i} - \\sqrt{\\alpha_{\\tau_i}} x_0}{\\sqrt{1 - \\alpha_{\\tau_i}}}, \\sigma_{\\tau_i}^2 I)\n$$\u003c/p\u003e\n\u003cp\u003ewhich can still provide high-quality samples using a much fewer number of steps.\u003c/p\u003e\n\u003ch1 id=\"references\" class=\"icon-inline\" id=\"references\"\u003eReferences\u003ca class=\"icon-link\" href=\"#references\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eJonathan Ho, Ajay Jain, and Pieter Abbeel, ‘Denoising Diffusion Probabilistic Models’, in \u003cem\u003eAdvances in Neural Information Processing Systems\u003c/em\u003e, 2020.\u003c/li\u003e\n\u003cli\u003eAlexander Quinn Nichol and Prafulla Dhariwal, ‘Improved Denoising Diffusion Probabilistic Models’, in \u003cem\u003eProceedings of the 38th International Conference on Machine Learning\u003c/em\u003e, 2021.\u003c/li\u003e\n\u003cli\u003eJiaming Song, Chenlin Meng, and Stefano Ermon, ‘Denoising Diffusion Implicit Models’, in \u003cem\u003eInternational Conference on Learning Representations\u003c/em\u003e, 2021.\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "Image Generation based on Score Model",
            "date_published": "2022-12-08T00:00:00Z",
            "date_modified": "2022-12-08T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/snapshots/2212ncsn/",
            "url": "https://yuhaoo00.github.io/posts/snapshots/2212ncsn/",
            "content_html": "\u003cblockquote\u003e\n\u003cp\u003eBoth likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function $\\nabla_{ x} \\log p_{\\text {data }}( x)$) may be a better choice than learning the data density directly.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"score-estimation-for-training\" class=\"icon-inline\" id=\"score-estimation-for-training\"\u003eScore Estimation (for training)\u003ca class=\"icon-link\" href=\"#score-estimation-for-training\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe want to train a network $s_{\\theta}(x)$ to estimate $\\nabla_{ x} \\log p_{\\text {data }}( x)$, but how can we get the ground truth (the real score)? In this paper, the objective $\\frac{1}{2} E_{p_{\\text{data}}} \\lbrack\\lVert s_{\\theta}( x)-\\nabla_{ x} \\log p_{\\text{data}}( x)\\rVert_2^2\\rbrack$ is equivalent to the following by Score Matching:\u003c/p\u003e\n\u003cp\u003e$$\nE_{p_{\\text{data}}}\\left[tr(\\nabla_{ x}s_{\\theta}( x))+\\frac{1}{2}\\left|s_{\\theta}( x))\\right|_2^2\\right]\n$$\u003c/p\u003e\n\u003cp\u003eUnfortunately, it is not easy to compute $tr(\\cdot)$ for a large-scale problem. Both \u003cstrong\u003eDenoising score matching\u003c/strong\u003e and \u003cstrong\u003eSliced score matching\u003c/strong\u003e are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q_\\sigma$) as follows:\u003c/p\u003e\n\u003cp\u003e$$\n\\frac{1}{2} E_{q_\\sigma(\\tilde{ x} \\mid  x) p_{\\text {data }}( x)}\\left[\\left| s_{\\theta}(\\tilde{ x})-\\nabla_{\\tilde{ x}} \\log q_\\sigma(\\tilde{ x} \\mid  x)\\right|_2^2\\right]\n$$\u003c/p\u003e\n\u003cp\u003eMinimizing this objective, we can get $s_{\\theta}( x)=\\nabla_{ x} \\log q_{\\sigma}( x)$. And if the noise is small enough $q_\\sigma( x) \\approx p_{\\text {data }}( x)$, we will get $s_{\\theta}( x)=\\nabla_{ x} \\log q_\\sigma( x) \\approx \\nabla_{ x} \\log p_{\\text {data }}( x)$.\nAs $q_\\sigma(\\tilde{x}| x)$ can be defined as a known simple distribution, this minimization is easier than regular score matching. In this paper, they choose $q_\\sigma(\\tilde{x}| x) =N\\left(\\tilde{x} \\mid x, \\sigma^2 I\\right)$, which leads to:\u003c/p\u003e\n\u003cp\u003e$$\n\\ell(\\theta ; \\sigma) \\triangleq \\frac{1}{2} E_{p_{\\text {data }}(x)} E_{\\tilde{x} \\sim N\\left(x, \\sigma^2 I\\right)}\\left[\\left|\\mathbf{s}_{\\theta}(\\tilde{x}, \\sigma)+\\frac{\\tilde{x}-x}{\\sigma^2}\\right|_2^2\\right]\n$$\u003c/p\u003e\n\u003ch2 id=\"langevin-dynamics-for-inference\" class=\"icon-inline\" id=\"langevin-dynamics-for-inference\"\u003eLangevin dynamics (for inference)\u003ca class=\"icon-link\" href=\"#langevin-dynamics-for-inference\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eHow can we do sampling from $p_{\\text {data }}( x)$ when we get a nice estimation of $\\nabla_{ x} \\log p_{\\text {data }}( x)$? Just along the direction of this gradient? Yes, but plus more tricks.\u003c/p\u003e\n\u003cp\u003e$$\nx_t = x_{t-1}+\\frac{\\epsilon}{2} \\nabla_{x} \\log p\\left(x_{t-1}\\right)+\\sqrt{\\epsilon} z_t\n$$\u003c/p\u003e\n\u003cp\u003eThe Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide \u003cem\u003emore stable distribution\u003c/em\u003e! Briefly, the random noise term $\\mathbf{z}_t \\thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\\epsilon \\to 0$), the iterative $x_t$ will approach the distribution $p(x)$.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012150556.png\" loading=\"lazy\" width=\"400\"\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"practical-challenges\" class=\"icon-inline\" id=\"practical-challenges\"\u003ePractical Challenges\u003ca class=\"icon-link\" href=\"#practical-challenges\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"the-manifold-hypothesis\" class=\"icon-inline\" id=\"the-manifold-hypothesis\"\u003eThe manifold hypothesis\u003ca class=\"icon-link\" href=\"#the-manifold-hypothesis\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eThe score is \u003cem\u003eundefined\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThe estimation by Score Matching \u003cem\u003eisn\u0026rsquo;t consistent\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf we \u003cstrong\u003eperturb the data\u003c/strong\u003e with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast \u0026amp; faithful) will converge (Fig. 1).\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192631.png\" loading=\"lazy\"\u003e\n\u003cfigcaption\u003e\nFigure 1: Left: Sliced score matching (SSM) loss w.r.t. iterations. No noise is added to data. Right: Same but data are perturbed with N(0, 0.0001).\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"low-data-density-regions\" class=\"icon-inline\" id=\"low-data-density-regions\"\u003eLow data density regions\u003ca class=\"icon-link\" href=\"#low-data-density-regions\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOur training is based on the data in high density ($\\thicksim p_{\\text {data }}( x)$).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eThe estimation in low density regions is \u003cem\u003einaccurate.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eRegular Langevin Dynamics \u003cem\u003ecan\u0026rsquo;t be able to correctly recover\u003c/em\u003e the relative weights of the multi-modal distribution \u003cem\u003ein reasonable time.\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf we \u003cstrong\u003eperturb the data by using multiple noise levels\u003c/strong\u003e (anneal down), we can fill the low density regions.\u003c/p\u003e\n\u003cfigure class=\"image\"\u003e\n\u003cimg src=\"https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192901.png\" loading=\"lazy\"\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eSo naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions.\n(Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)\u003c/p\u003e\n\u003ch2 id=\"one-more-thing\" class=\"icon-inline\" id=\"one-more-thing\"\u003eOne More Thing\u003ca class=\"icon-link\" href=\"#one-more-thing\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn the score matching, the authors approximately have $\\left| s_{\\theta}( x, \\sigma)\\right|_2 \\propto 1 / \\sigma$, so they choose $\\lambda (\\sigma) = \\sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\\sigma$.\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}\\left(\\theta ;\\lbrace\\sigma_i \\rbrace_{i=1}^{L}\\right) \\triangleq \\frac{1}{L} \\sum_{i=1}^L \\lambda\\left(\\sigma_i\\right) \\ell\\left(\\theta ; \\sigma_i\\right)\n$$\u003c/p\u003e\n\u003cp\u003eCorrespondingly, in the langevin dynamics, they choose $\\alpha_i \\propto \\sigma^2$ to make the order magnitude of \u0026ldquo;signal-to-noise ratio\u0026rdquo; independent of $\\sigma$.\u003c/p\u003e\n\u003ch1 id=\"references\" class=\"icon-inline\" id=\"references\"\u003eReferences\u003ca class=\"icon-link\" href=\"#references\" aria-hidden=\"true\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\" /\u003e\n  \u003cpath d=\"M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5\" /\u003e\n  \u003cpath d=\"M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5\" /\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eYang Song and Stefano Ermon, ‘Generative Modeling by Estimating Gradients of the Data Distribution’, in \u003cem\u003eAdvances in Neural Information Processing Systems\u003c/em\u003e, 2019.\u003c/li\u003e\n\u003cli\u003eYang Song and others, ‘Score-Based Generative Modeling through Stochastic Differential Equations’, in \u003cem\u003eInternational Conference on Learning Representations\u003c/em\u003e, 2021.\u003c/li\u003e\n\u003c/ul\u003e\n"
        },
        {
            "title": "",
            "date_published": "0001-01-01T00:00:00Z",
            "date_modified": "0001-01-01T00:00:00Z",
            "id": "https://yuhaoo00.github.io/posts/idea/2206maeandtransferlearning/",
            "url": "https://yuhaoo00.github.io/posts/idea/2206maeandtransferlearning/",
            "content_html": ""
        },
        {
            "title": "Contact",
            "date_published": "0001-01-01T00:00:00Z",
            "date_modified": "0001-01-01T00:00:00Z",
            "id": "https://yuhaoo00.github.io/contact/",
            "url": "https://yuhaoo00.github.io/contact/",
            "content_html": "\u003cp class=\"error message js-hidden\"\u003eYou must have Javascript enabled to use this function.\u003c/p\u003e\n\u003cp class=\"contact-submitted status message hidden\"\u003eYour message was sent.\u003c/p\u003e\n\u003cp class=\"contact-error error message hidden\"\u003eThere was an error sending the message.\u003c/p\u003e\n\n\u003cform class=\"contact-form hidden\" data-protect=\"/php/contact.php\" action=\"#\" method=\"post\" accept-charset=\"UTF-8\"\u003e\n  \u003clabel for=\"edit-name\"\u003eName\u003c/label\u003e\n  \u003cinput type=\"text\" id=\"edit-name\" name=\"name\" placeholder=\"Your name\" required autofocus\u003e\n  \u003clabel for=\"edit-mail\"\u003eE-mail address\u003c/label\u003e\n  \u003cinput type=\"email\" id=\"edit-mail\" name=\"email\" placeholder=\"Your e-mail address\" required\u003e\n  \u003cinput type=\"text\" id=\"edit-url\" class=\"hidden\" name=\"url\" placeholder=\"Skip if you are a human\"\u003e\n  \u003clabel for=\"edit-subject\"\u003eSubject\u003c/label\u003e\n  \u003cinput type=\"text\" id=\"edit-subject\" name=\"subject\" placeholder=\"A short subject\" required\u003e\n  \u003clabel for=\"edit-message\"\u003eMessage\u003c/label\u003e\n  \u003ctextarea id=\"edit-message\" name=\"message\" rows=\"5\" placeholder=\"The messages goes here…\" required\u003e\u003c/textarea\u003e\n  \u003cbutton type=\"submit\" name=\"submit\" disabled\u003eSend message\u003c/button\u003e\n\u003c/form\u003e\n\n"
        },
        {
            "title": "Search",
            "date_published": "0001-01-01T00:00:00Z",
            "date_modified": "0001-01-01T00:00:00Z",
            "id": "https://yuhaoo00.github.io/search/",
            "url": "https://yuhaoo00.github.io/search/",
            "content_html": "\u003cp class=\"error message js-hidden\"\u003eYou must have Javascript enabled to use this function.\u003c/p\u003e\n\u003cp class=\"search-loading status message hidden\"\u003eLoading search index…\u003c/p\u003e\n\n\u003cdiv class=\"search-input hidden\"\u003e\n  \u003cform id=\"search-form\" class=\"search-form\" action=\"#\" method=\"post\" accept-charset=\"UTF-8\" role=\"search\"\u003e\n    \u003clabel for=\"query\" class=\"visually-hidden\"\u003eSearch\u003c/label\u003e\n    \u003cinput type=\"search\" id=\"query\" name=\"query\" class=\"search-text\" placeholder=\"Enter the terms you wish to search for.\" maxlength=\"128\"\u003e\n  \u003c/form\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"search-results\"\u003e\u003c/div\u003e\n\n\u003ctemplate\u003e\n  \u003carticle class=\"search-result list-view\"\u003e\n    \u003cheader\u003e\n      \u003ch2 class=\"title mt--s mb--xxs\"\u003e\u003ca href=\"#\"\u003eTitle here\u003c/a\u003e\u003c/h2\u003e\n      \u003cdiv class=\"submitted\"\u003e\u003ctime class=\"created-date\"\u003eDate here\u003c/time\u003e\u003c/div\u003e\n    \u003c/header\u003e\n    \u003cp class=\"content\"\u003eSummary here\u003c/p\u003e\n  \u003c/article\u003e\n\u003c/template\u003e\n\n"
        }
        ]
}
