<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DDPMs and Early Variants | Mem.Capsule</title><meta name=keywords content="Diffusion,ImageGeneration,ClassicPaper"><meta name=description content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follows: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta^{(t)}\left(x_{t-1} \mid x_t\right)$$ Similar to VAE, we can use the posterior $q(x_{1:t} \mid x_0)$ to do the estimation for $\theta$."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/collections/2212ddpm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8386e9a51327ebe0742181d4e05b9214ae1eca0a767c79f534b2349a31d68296.css integrity="sha256-g4bppRMn6+B0IYHU4FuSFK4eygp2fHn1NLI0mjHWgpY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="DDPMs and Early Variants"><meta property="og:description" content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follows: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta^{(t)}\left(x_{t-1} \mid x_t\right)$$ Similar to VAE, we can use the posterior $q(x_{1:t} \mid x_0)$ to do the estimation for $\theta$."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/collections/2212ddpm/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-09T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="DDPMs and Early Variants"><meta name=twitter:description content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follows: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta^{(t)}\left(x_{t-1} \mid x_t\right)$$ Similar to VAE, we can use the posterior $q(x_{1:t} \mid x_0)$ to do the estimation for $\theta$."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"DDPMs and Early Variants","item":"https://yuhaoo00.github.io/posts/collections/2212ddpm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DDPMs and Early Variants","name":"DDPMs and Early Variants","description":"Although Diffusion Model is a new generative framework, it still has many shades of other methods.\nGenerative \u0026amp; Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follow\u001ds: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta^{(t)}\\left(x_{t-1} \\mid x_t\\right)$$ Similar to VAE, we can use the posterior $q(x_{1:t} \\mid x_0)$ to do the estimation for $\\theta$.","keywords":["Diffusion","ImageGeneration","ClassicPaper"],"articleBody":" Although Diffusion Model is a new generative framework, it still has many shades of other methods.\nGenerative \u0026 Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follow\u001ds: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta^{(t)}\\left(x_{t-1} \\mid x_t\\right)$$ Similar to VAE, we can use the posterior $q(x_{1:t} \\mid x_0)$ to do the estimation for $\\theta$. The difference is that $x_1,\\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \\mid x_0)$ is fixed to a Markov chain, which can be defined as Gaussian transitions parameterized by a decreasing sequence $\\alpha_{1:T}\\in [0,1]^T$ : $$ q(x_{1:T} \\mid x_0) := \\prod_{t=1}^T q\\left(x_{t} \\mid x_{t-1}\\right) $$ $$ q(x_t \\mid x_{t-1}):=N(\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}}x_{t-1}, (1-\\frac{\\alpha_t}{\\alpha_{t-1}})I) $$\nMarkov chain: What happens next depends only on the state of affairs now. So we have $q(x_t\\mid x_{0:t-1})=q(x_t\\mid x_{t-1})$\nA nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$: $$ q(x_t\\mid x_0)=N(x_t;\\sqrt{\\alpha_t}x_0, (1-\\alpha_t)I) $$\nTraining Objective We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood: $$ \\max_{\\theta}E_{q}[\\log{p_\\theta(x_0)}]\\leq \\max_{\\theta}E_{q}[\\log{p_{\\theta} (x_{0:T})}-\\log{q(x_{1:T} \\mid x_0)}] $$ which also can be driven by Jensen’s inequality as in Lil’log. And we can further rewrite this object as: where $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs. Using Bayes’ rule, we can deduce the fact that $q(x_{t-1}\\mid x_t, x_0)$ is a gaussian distribution. Therefore, if forward variances is all fixed, we can simplify this objective as follows with parameterization tricks (fit distribution $\\to$ fit mean $\\to$ predict noise) and empirical results. $$ L_t=E_{x_0\\sim q, \\epsilon\\sim N(0,1)}\\left[|| \\epsilon_\\theta(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon, t)-\\epsilon {||}2^2 \\right] $$ For last $L_0$, DDPMs treat it as an independent a discrete decoder derived from $N(x_0;\\mu{\\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.\nAt the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\\theta(x_{t-1}\\mid x_t)$ using predicted $\\epsilon_\\theta(\\cdot)$ (Alg. 2). If all mathematical derivations are funny like this …\nDDPM+ DDIM One More Thing ","wordCount":"395","inLanguage":"en","datePublished":"2022-12-09T00:00:00Z","dateModified":"2022-12-09T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/collections/2212ddpm/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)"><img src=https://yuhaoo00.github.io/apple-touch-icon.png alt aria-label=logo height=35>Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>DDPMs and Early Variants</h1><div class=post-meta><span title='2022-12-09 00:00:00 +0000 UTC'>December 9, 2022</span>&nbsp;·&nbsp;395 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#generative--diffusion>Generative & Diffusion</a></li><li><a href=#training-objective>Training Objective</a></li><li><a href=#ddpm>DDPM+</a></li><li><a href=#ddim>DDIM</a></li><li><a href=#one-more-thing>One More Thing</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Although Diffusion Model is a new generative framework, it still has many shades of other methods.</p></blockquote><h2 id=generative--diffusion>Generative & Diffusion<a hidden class=anchor aria-hidden=true href=#generative--diffusion>#</a></h2><p>Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as follows:
$$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$
$$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta^{(t)}\left(x_{t-1} \mid x_t\right)$$
<img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png alt=image.png></p><p>Similar to VAE, we can use the posterior $q(x_{1:t} \mid x_0)$ to do the estimation for $\theta$. The difference is that $x_1,\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \mid x_0)$ is fixed to a <strong>Markov chain</strong>, which can be defined as Gaussian transitions parameterized by a decreasing sequence $\alpha_{1:T}\in [0,1]^T$ :
$$
q(x_{1:T} \mid x_0) := \prod_{t=1}^T q\left(x_{t} \mid x_{t-1}\right)
$$
$$
q(x_t \mid x_{t-1}):=N(\frac{\sqrt{\alpha_t}}{\sqrt{\alpha_{t-1}}}x_{t-1}, (1-\frac{\alpha_t}{\alpha_{t-1}})I)
$$</p><blockquote><p>Markov chain: What happens next depends only on the state of affairs now. So we have $q(x_t\mid x_{0:t-1})=q(x_t\mid x_{t-1})$</p></blockquote><p>A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$:
$$
q(x_t\mid x_0)=N(x_t;\sqrt{\alpha_t}x_0, (1-\alpha_t)I)
$$</p><h2 id=training-objective>Training Objective<a hidden class=anchor aria-hidden=true href=#training-objective>#</a></h2><p>We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood:
$$
\max_{\theta}E_{q}[\log{p_\theta(x_0)}]\leq \max_{\theta}E_{q}[\log{p_{\theta} (x_{0:T})}-\log{q(x_{1:T} \mid x_0)}]
$$
which also can be driven by Jensen’s inequality as in <a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ title="Lil'log">Lil&rsquo;log</a>. And we can further rewrite this object as:
<img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png alt=image.png>
where $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs. Using Bayes&rsquo; rule, we can deduce the fact that $q(x_{t-1}\mid x_t, x_0)$ is a gaussian distribution. Therefore, <strong>if forward variances is all fixed</strong>, we can simplify this objective as follows with parameterization tricks (fit distribution $\to$ fit mean $\to$ predict noise) and empirical results.
$$
L_t=E_{x_0\sim q, \epsilon\sim N(0,1)}\left[|| \epsilon_\theta(\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\epsilon, t)-\epsilon {||}<em>2^2 \right]
$$
For last $L_0$, DDPMs treat it as an independent a discrete decoder derived from $N(x_0;\mu</em>{\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.</p><p>At the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\theta(x_{t-1}\mid x_t)$ using predicted $\epsilon_\theta(\cdot)$ (Alg. 2).
<img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png alt=image.png></p><p><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png alt=image.png>
If all mathematical derivations are funny like this &mldr;</p><h2 id=ddpm>DDPM+<a hidden class=anchor aria-hidden=true href=#ddpm>#</a></h2><h2 id=ddim>DDIM<a hidden class=anchor aria-hidden=true href=#ddim>#</a></h2><p><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013152121.png alt=image.png></p><h2 id=one-more-thing>One More Thing<a hidden class=anchor aria-hidden=true href=#one-more-thing>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://yuhaoo00.github.io/tags/imagegeneration/>ImageGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/classicpaper/>ClassicPaper</a></li></ul><nav class=paginav><a class=prev href=https://yuhaoo00.github.io/posts/zero-shot-image-restoration-using-denoising-diffusion-null-space-model/><span class=title>« Prev</span><br><span>Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model</span></a>
<a class=next href=https://yuhaoo00.github.io/posts/snapshots/2212ncsn/><span class=title>Next »</span><br><span>[Paper Snapshot] Generative Modeling by Estimating Gradients of the Data Distribution</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>