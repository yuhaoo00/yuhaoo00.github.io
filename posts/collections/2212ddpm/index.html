<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DDPMs and Early Variants | Mem.Capsule</title><meta name=keywords content="Diffusion,ImageGeneration,ClassicPaper"><meta name=description content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
{{figure src=&ldquo;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png&#34; width=80% align=&ldquo;center&rdquo; caption=&ldquo;If all mathematical derivations are funny like this &mldr; :zap:&rdquo;}}
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \mid x_t\right)$$ $$p_\theta\left(x_{t-1} \mid x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))$$ {{figure src=&ldquo;https://image-1300968464."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/collections/2212ddpm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8386e9a51327ebe0742181d4e05b9214ae1eca0a767c79f534b2349a31d68296.css integrity="sha256-g4bppRMn6+B0IYHU4FuSFK4eygp2fHn1NLI0mjHWgpY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="DDPMs and Early Variants"><meta property="og:description" content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
{{figure src=&ldquo;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png&#34; width=80% align=&ldquo;center&rdquo; caption=&ldquo;If all mathematical derivations are funny like this &mldr; :zap:&rdquo;}}
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \mid x_t\right)$$ $$p_\theta\left(x_{t-1} \mid x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))$$ {{figure src=&ldquo;https://image-1300968464."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/collections/2212ddpm/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-09T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="DDPMs and Early Variants"><meta name=twitter:description content="Although Diffusion Model is a new generative framework, it still has many shades of other methods.
{{figure src=&ldquo;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png&#34; width=80% align=&ldquo;center&rdquo; caption=&ldquo;If all mathematical derivations are funny like this &mldr; :zap:&rdquo;}}
Generative & Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$ $$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \mid x_t\right)$$ $$p_\theta\left(x_{t-1} \mid x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))$$ {{figure src=&ldquo;https://image-1300968464."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"DDPMs and Early Variants","item":"https://yuhaoo00.github.io/posts/collections/2212ddpm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DDPMs and Early Variants","name":"DDPMs and Early Variants","description":"Although Diffusion Model is a new generative framework, it still has many shades of other methods.\n{{figure src=\u0026ldquo;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png\u0026quot; width=80% align=\u0026ldquo;center\u0026rdquo; caption=\u0026ldquo;If all mathematical derivations are funny like this \u0026hellip; :zap:\u0026rdquo;}}\nGenerative \u0026amp; Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta\\left(x_{t-1} \\mid x_t\\right)$$ $$p_\\theta\\left(x_{t-1} \\mid x_t\\right):=N(x_{t-1};\\mu_{\\theta}(x_t,t),\\Sigma_\\theta(x_t,t))$$ {{figure src=\u0026ldquo;https://image-1300968464.","keywords":["Diffusion","ImageGeneration","ClassicPaper"],"articleBody":" Although Diffusion Model is a new generative framework, it still has many shades of other methods.\n{{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png\" width=80% align=“center” caption=“If all mathematical derivations are funny like this … :zap:”}}\nGenerative \u0026 Diffusion Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following Markov chain with learnable Gaussian transitions: $$p_\\theta\\left(x_0\\right)=\\int p_\\theta\\left(x_{0: T}\\right) \\mathrm{d} x_{1: T}$$ $$p_\\theta\\left(x_{0: T}\\right):=p_\\theta\\left(x_T\\right) \\prod_{t=1}^T p_\\theta\\left(x_{t-1} \\mid x_t\\right)$$ $$p_\\theta\\left(x_{t-1} \\mid x_t\\right):=N(x_{t-1};\\mu_{\\theta}(x_t,t),\\Sigma_\\theta(x_t,t))$$ {{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png\" width=100% align=“center” }}\nMarkov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\\mid x_{t:T})=p(x_{t-1}\\mid x_{t})$\nSimilar to VAE, we can use the posterior $q(x_{1:t} \\mid x_0)$ to do the estimation for $\\theta$. The difference is that $x_1,\\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \\mid x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\\alpha_{1:T}\\in [0,1]^T$: $$ q(x_{1:T} \\mid x_0) := \\prod_{t=1}^T q\\left(x_{t} \\mid x_{t-1}\\right) $$ $$ q(x_t \\mid x_{t-1}):=N(\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}}x_{t-1}, (1-\\frac{\\alpha_t}{\\alpha_{t-1}})I) $$ A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$: $$ q(x_t\\mid x_0)=N(x_t;\\sqrt{\\alpha_t}x_0, (1-\\alpha_t)I) $$\nTraining Objective We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood: $$ \\max_{\\theta}E_{q}[\\log{p_\\theta(x_0)}]\\leq \\max_{\\theta}E_{q}[\\log{p_{\\theta} (x_{0:T})}-\\log{q(x_{1:T} \\mid x_0)}] $$ which also can be driven by Jensen’s inequality as in Lil’log. And we can further rewrite this object as:\n{{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png\" width=100% align=“center” }}\nUsing Bayes’ rule, we can deduce the fact that $q(x_{t-1}\\mid x_t, x_0)$ is also a gaussian distribution.\nwhere $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs. If generative variances is all fixed $\\Sigma_t = \\sigma^2$, using parameterization (fit distribution $\\to$ fit mean $\\to$ predict noise) and reweighting based on empirical results, we can simplify this objective as follows: $$ L_t=E_{x_0\\sim q, \\epsilon\\sim N(0,1)}\\left[|| \\epsilon_\\theta(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon, t)-\\epsilon {||}_2^2 \\right] $$\nNCSNs vs DDPMs, different ways lead to almost the same objective!\nFor last $L_0$, DDPMs treat it as an independent a discrete decoder derived from $N(x_0;\\mu_{\\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.\nAt the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\\theta(x_{t-1}\\mid x_t)$ using predicted $\\epsilon_\\theta(\\cdot)$ (Alg. 2).\n{{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png\" width=100% align=“center” }}\nDDPM+ Finding 1: Why fixing $\\sigma^2$ to $\\beta$ or$\\tilde{\\beta}$ achieve similar sample quality? {{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png\" width=60% align=“center” }}\nAs $\\beta \\approx \\tilde{\\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\\frac{\\beta_t}{\\beta_{t-1}} \\to 0$.\nFinding 2: The diffusion samples from linear schedule lose information very quickly. {{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png\" width=100% align=“center” }}\n{{figure src=“https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png\" width=60% align=“center” }}\nIn the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggest that so much full noisy samples does not contribute to generation.\nImprovements Learnable variances with an interpolation between $\\beta$ and $\\tilde{\\beta}$, driven by loss $+ \\lambda L_{vlb}$. Cosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end. Resampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting) DDIM One More Thing ","wordCount":"613","inLanguage":"en","datePublished":"2022-12-09T00:00:00Z","dateModified":"2022-12-09T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/collections/2212ddpm/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)"><img src=https://yuhaoo00.github.io/apple-touch-icon.png alt aria-label=logo height=35>Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>DDPMs and Early Variants</h1><div class=post-meta><span title='2022-12-09 00:00:00 +0000 UTC'>December 9, 2022</span>&nbsp;·&nbsp;613 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#generative--diffusion>Generative & Diffusion</a></li><li><a href=#training-objective>Training Objective</a></li><li><a href=#ddpm>DDPM+</a><ul><li><a href=#finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality>Finding 1: Why fixing $\sigma^2$ to $\beta$ or$\tilde{\beta}$ achieve similar sample quality?</a></li><li><a href=#finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly>Finding 2: The diffusion samples from linear schedule lose information very quickly.</a></li></ul></li><li><a href=#improvements>Improvements</a></li><li><a href=#ddim>DDIM</a></li><li><a href=#one-more-thing>One More Thing</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Although Diffusion Model is a new generative framework, it still has many shades of other methods.</p></blockquote><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png"</a> width=80% align=&ldquo;center&rdquo; caption=&ldquo;If all mathematical derivations are funny like this &mldr; :zap:&rdquo;}}</p><h2 id=generative--diffusion>Generative & Diffusion<a hidden class=anchor aria-hidden=true href=#generative--diffusion>#</a></h2><p>Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following <strong>Markov chain</strong> with learnable Gaussian transitions:
$$p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}$$
$$p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \mid x_t\right)$$
$$p_\theta\left(x_{t-1} \mid x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))$$
{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png"</a> width=100% align=&ldquo;center&rdquo; }}</p><blockquote><p>Markov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\mid x_{t:T})=p(x_{t-1}\mid x_{t})$</p></blockquote><p>Similar to VAE, we can use the posterior $q(x_{1:t} \mid x_0)$ to do the estimation for $\theta$. The difference is that $x_1,\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \mid x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\alpha_{1:T}\in [0,1]^T$:
$$
q(x_{1:T} \mid x_0) := \prod_{t=1}^T q\left(x_{t} \mid x_{t-1}\right)
$$
$$
q(x_t \mid x_{t-1}):=N(\frac{\sqrt{\alpha_t}}{\sqrt{\alpha_{t-1}}}x_{t-1}, (1-\frac{\alpha_t}{\alpha_{t-1}})I)
$$
A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$:
$$
q(x_t\mid x_0)=N(x_t;\sqrt{\alpha_t}x_0, (1-\alpha_t)I)
$$</p><h2 id=training-objective>Training Objective<a hidden class=anchor aria-hidden=true href=#training-objective>#</a></h2><p>We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood:
$$
\max_{\theta}E_{q}[\log{p_\theta(x_0)}]\leq \max_{\theta}E_{q}[\log{p_{\theta} (x_{0:T})}-\log{q(x_{1:T} \mid x_0)}]
$$
which also can be driven by Jensen’s inequality as in <a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ title="Lil'log">Lil&rsquo;log</a>. And we can further rewrite this object as:</p><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png"</a> width=100% align=&ldquo;center&rdquo; }}</p><blockquote><p>Using Bayes&rsquo; rule, we can deduce the fact that $q(x_{t-1}\mid x_t, x_0)$ is also a gaussian distribution.</p></blockquote><p>where $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs. <strong>If generative variances is all fixed $\Sigma_t = \sigma^2$</strong>, using parameterization (fit distribution $\to$ fit mean $\to$ predict noise) and reweighting based on empirical results, we can simplify this objective as follows:
$$
L_t=E_{x_0\sim q, \epsilon\sim N(0,1)}\left[|| \epsilon_\theta(\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\epsilon, t)-\epsilon {||}_2^2 \right]
$$</p><blockquote><p>NCSNs vs DDPMs, different ways lead to almost the same objective!</p></blockquote><p>For last $L_0$, DDPMs treat it as an independent a discrete decoder derived from $N(x_0;\mu_{\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.</p><p>At the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\theta(x_{t-1}\mid x_t)$ using predicted $\epsilon_\theta(\cdot)$ (Alg. 2).</p><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png"</a> width=100% align=&ldquo;center&rdquo; }}</p><h2 id=ddpm>DDPM+<a hidden class=anchor aria-hidden=true href=#ddpm>#</a></h2><h3 id=finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality>Finding 1: Why fixing $\sigma^2$ to $\beta$ or$\tilde{\beta}$ achieve similar sample quality?<a hidden class=anchor aria-hidden=true href=#finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality>#</a></h3><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png"</a> width=60% align=&ldquo;center&rdquo; }}</p><p>As $\beta \approx \tilde{\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\frac{\beta_t}{\beta_{t-1}} \to 0$.</p><h3 id=finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly>Finding 2: The diffusion samples from linear schedule lose information very quickly.<a hidden class=anchor aria-hidden=true href=#finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly>#</a></h3><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png"</a> width=100% align=&ldquo;center&rdquo; }}</p><p>{{figure src=&ldquo;<a href=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png%22>https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png"</a> width=60% align=&ldquo;center&rdquo; }}</p><p>In the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggest that so much full noisy samples does not contribute to generation.</p><h2 id=improvements>Improvements<a hidden class=anchor aria-hidden=true href=#improvements>#</a></h2><ul><li>Learnable variances with an interpolation between $\beta$ and $\tilde{\beta}$, driven by loss $+ \lambda L_{vlb}$.</li><li>Cosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end.</li><li>Resampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting)</li></ul><h2 id=ddim>DDIM<a hidden class=anchor aria-hidden=true href=#ddim>#</a></h2><p><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013152121.png alt=image.png></p><h2 id=one-more-thing>One More Thing<a hidden class=anchor aria-hidden=true href=#one-more-thing>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://yuhaoo00.github.io/tags/imagegeneration/>ImageGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/classicpaper/>ClassicPaper</a></li></ul><nav class=paginav><a class=prev href=https://yuhaoo00.github.io/posts/zero-shot-image-restoration-using-denoising-diffusion-null-space-model/><span class=title>« Prev</span><br><span>Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model</span></a>
<a class=next href=https://yuhaoo00.github.io/posts/snapshots/2212ncsn/><span class=title>Next »</span><br><span>[Paper Snapshot] Generative Modeling by Estimating Gradients of the Data Distribution</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>