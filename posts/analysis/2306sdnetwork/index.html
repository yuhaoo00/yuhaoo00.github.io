<!DOCTYPE html>
<html class="nojs" lang="en-GB" dir="ltr">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Network Design in Stable Diffusion – Yu&#39;s MemoCapsule</title>
<meta name="view-transition" content="same-origin">
<meta name="description" content="StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent …">
<meta name="created" content="2023-02-01T00:00:00+0000">
<meta name="modified" content="2023-02-01T00:00:00+0000">

<meta name="contact" content="TimberH2000@outlook.com">
<meta property="og:site_name" content="Yu&#39;s MemoCapsule">
<meta property="og:title" content="Network Design in Stable Diffusion">
<meta property="og:url" content="https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/">
<meta property="og:type" content="article">
<meta property="og:image" content="https://yuhaoo00.github.io/favicon-192.png">
<meta name="generator" content="Hugo 0.118.2">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff">


<link rel="canonical" href="https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link rel="stylesheet" href="/css/mobile.928a90a2e5ceccad718844ea67aa6bba151ff555ab514fb103071cc7b60c2ff2.css" media="screen">
<link rel="stylesheet" href="/css/styles.4b0725243fe74b2ef00d4a9effe4e55d8555c705d333dd673a4232e7c4693eac.css">
<link rel="stylesheet" href="/css/print.31e2819287afc91406f2fd43d21a8ba4a0cdfc272e439c90db0c6e47efc7c346.css" media="print">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "WebPage",
    "headline": "Network Design in Stable Diffusion",
    "datePublished": "2023-02-01T00:00:00Z",
    "dateModified": "2023-02-01T00:00:00Z",
    "url" : "https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/",
    "description": "StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent …",
    "keywords": ["Diffusion","ImageGeneration","Text2Image"],
    "image" : "https://yuhaoo00.github.io/favicon-192.png",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://yuhaoo00.github.io"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Yu's MemoCapsule",
      "logo" : {
        "@type": "ImageObject",
        "url": "https://yuhaoo00.github.io/favicon-192.png"
      },
      "url": "https://yuhaoo00.github.io"
    }
  }
</script>

<script src="/js/script-early.214cc4b0248ef64425811a906c0c6a3702edffba5dbc127128e9ff989c74c137.js"></script>

<script defer src="/js/mobile.2ee7f57bec29e942a94fc13cd4cf128b0b8daf996652b9725d7e9bf960496721.js"></script>
<script defer src="/js/script.1688e04483adf683d47143ac70e770c7b5e4b75213641f33b46d9b84cb3e91ca.js"></script>

<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/contrib/auto-render.min.js"></script>
<script defer src="/js/math.f08366f847be855a2a8edced48e6cbaca080c199dfc2efd172b4788a2f58d85e.js"></script>



</head>

<body class="single-page">
<div class="page layout__page">
<header class="header layout__header">
<a href="/" title="Home" rel="home" class="header__logo">
<img src="/images/logo.png" width="40" height="40" alt="Home" class="header__logo-image">
</a>
<h1 class="header__site-name">
<a href="/" title="Home" class="header__site-link" rel="home"><span>Yu&#39;s MemoCapsule</span></a>
</h1>
<div class="region header__region">
</div>
<div class="mobile-nav" dir="ltr">
  <div class="mobile-nav__cover"></div>
  <button class="mobile-nav__toggle button--outline" aria-expanded="false" aria-controls="sheet">
    Menu
    <svg class="mobile-nav__hamburger" viewBox="0 0 100 100" focusable="false" aria-hidden="true">
      <rect width="80" height="12" x="10" y="20" rx="5"></rect>
      <rect width="80" height="12" x="10" y="45" rx="5"></rect>
      <rect width="80" height="12" x="10" y="70" rx="5"></rect>
    </svg>
  </button>
  <div class="mobile-nav__sheet link-inverted link-nav" id="sheet" aria-hidden="true">
    <div class="mobile-nav__region"></div>
    <nav class="mobile-nav__main-menu" aria-label="Main menu">
    <ul class="mobile-nav__navbar">
    <li><a href="/">Home</a></li>
    <li><a href="/contact/">Contact</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/posts/" aria-current="page">Posts</a></li>
    </ul>
    </nav>
  </div>
</div>
</header>

<nav class="main-menu layout__navigation" aria-label="Main menu">
<ul class="navbar">
<li><a href="/">Home</a></li>
<li><a href="/contact/">Contact</a></li>
<li><a href="/search/">Search</a></li>
<li><a href="/posts/" aria-current="page">Posts</a></li>
</ul>
</nav>
<main class="main layout__main">
<article class="single-view single-view--posts">
<header>
<h1 class="title mb--xxs">Network Design in Stable Diffusion</h1>
<div class="submitted meta">
<time class="created-date" datetime="2023-02-01T00:00:00Z">1 February, 2023</time>

</div>
<div class="tags meta">
Tags:
<ul>
<li><a href="https://yuhaoo00.github.io/tags/diffusion/">Diffusion</a></li>
<li><a href="https://yuhaoo00.github.io/tags/imagegeneration/">ImageGeneration</a></li>
<li><a href="https://yuhaoo00.github.io/tags/text2image/">Text2Image</a></li>
</ul>
</div>
</header>

<p>StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">latent diffusion</a>, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.</p>
<h2 id="unet" class="icon-inline" id="unet">Unet<a class="icon-link" href="#unet" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/4320ED37-C913-487C-AF89-3563E40F5828_1_201_a.jpeg" loading="lazy">
<figcaption>
Fig. 1: Overall of the Unet in Stable Diffusion 1.x &amp; 2.x</figcaption>
</figure>

<p>As in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x &amp; SD2.x, $l_1=l_2=l_3=l_4=2$.</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/B431465F-81A1-404F-9AA0-1474625D7B44_1_201_a.jpeg" loading="lazy" width="600">
<figcaption>
Fig. 2: Skip-connection in the 2nd scale.</figcaption>
</figure>

<p>The skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer&rsquo;s input as in Fig. 2.</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/A012E8AE-8131-4160-8158-03EEC2A01A61_1_201_a.jpeg" loading="lazy" width="500">
<figcaption>
Fig 3: Transformer&rsquo;s Design</figcaption>
</figure>

<p>In the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follows the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BA68D415-0DF9-4CB5-B0F4-59EC7C8033E5_1_201_a.jpeg" loading="lazy" width="500">
<figcaption>
Fig. 4: Resnet block&rsquo;s design.</figcaption>
</figure>

<p>The SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling.</p>
<p>$$
\text{SiLU}(z)=z*\text{sigmoid}(z)
$$</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BFE0937A-36CA-4CC4-859F-BFC0F9F3EC51_1_201_a.jpeg" loading="lazy" width="500">
<figcaption>
Fig. 5: Timestep Embedding (based on positional encoding)</figcaption>
</figure>

<h2 id="vae" class="icon-inline" id="vae">VAE<a class="icon-link" href="#vae" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>The KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don&rsquo;t have the same weight (SD2&rsquo;s VAE might be fine-tuned for higher resolution $768\times 768$).<br>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/CB9453C4-4906-4188-B99B-1186CDEDA771_1_201_a.jpeg" loading="lazy">
<figcaption>
Fig.6: VAE Encoder</figcaption>
</figure>
</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/774E90D6-50FD-4AD7-800D-D0B903C6549A_1_201_a.jpeg" loading="lazy">
<figcaption>
Fig.6: VAE Decoder</figcaption>
</figure>

<h3 id="tiled-processing" class="icon-inline" id="tiled-processing">Tiled Processing<a class="icon-link" href="#tiled-processing" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<blockquote>
<p>In inference, VAE decoding often occupies a lot of memory.</p>
</blockquote>
<p>In the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.</p>
<p>This is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.</p>
<h1 id="references" class="icon-inline" id="references">References<a class="icon-link" href="#references" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h1>
<ul>
<li><a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a></li>
<li><a href="https://github.com/Stability-AI/stablediffusion">https://github.com/Stability-AI/stablediffusion</a></li>
<li><a href="https://github.com/Stability-AI/generative-models">https://github.com/Stability-AI/generative-models</a></li>
</ul>


</article>
</main>


<footer class="footer layout__footer mt--l">
<p><!--Creative Commons License-->This site is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA 4.0</a> licence.<!--/Creative Commons License--></p>


</footer>

</div>
</body>
</html>
