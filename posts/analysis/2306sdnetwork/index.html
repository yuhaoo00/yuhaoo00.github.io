<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Network Design in Stable Diffusion | Mem.Capsule</title><meta name=keywords content="Diffusion,ImageGeneration,Text2Image"><meta name=description content="StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.
Unet Fig. 1: Overall of the Unet in Stable Diffusion 1."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6c0e4385959ef5b32dbacc9d5c583ce22e75d9b61896b9fa8c1d6e4c98fbde8.css integrity="sha256-1sDkOFlZ71sy26zJ1cWDziLnXZthiWufqMHW5MmPveg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],output:"htmlAndMathml",strict:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Network Design in Stable Diffusion"><meta property="og:description" content="StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.
Unet Fig. 1: Overall of the Unet in Stable Diffusion 1."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-01T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Network Design in Stable Diffusion"><meta name=twitter:description content="StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.
Unet Fig. 1: Overall of the Unet in Stable Diffusion 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Network Design in Stable Diffusion","item":"https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Network Design in Stable Diffusion","name":"Network Design in Stable Diffusion","description":"StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.\nUnet Fig. 1: Overall of the Unet in Stable Diffusion 1.","keywords":["Diffusion","ImageGeneration","Text2Image"],"articleBody":"StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on latent diffusion, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.\nUnet Fig. 1: Overall of the Unet in Stable Diffusion 1.x \u0026 2.x\nAs in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x \u0026 SD2.x, $l_1=l_2=l_3=l_4=2$.\nFig. 2: Skip-connection in the 2nd scale.\nThe skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer’s input as in Fig. 2.\nFig 3: Transformer’s Design\nIn the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follow\u001ds the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.\nFig. 4: Resnet block’s design.\nThe SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling. $$ \\text{SiLU}(z)=z*\\text{sigmoid}(z) $$\nFig. 5: Timestep Embedding (based on positional encoding)\nVAE The KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don’t have the same weight (SD2’s VAE might be fine-tuned for higher resolution $768\\times 768$).\nFig.6: VAE Encoder\nFig.6: VAE Decoder\nTiled Processing In inference, VAE decoding often occupies a lot of memory.\nIn the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.\nThis is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.\nReferences https://github.com/huggingface/diffusers https://github.com/Stability-AI/stablediffusion https://github.com/Stability-AI/generative-models ","wordCount":"400","inLanguage":"en","datePublished":"2023-02-01T00:00:00Z","dateModified":"2023-02-01T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)"><img src=https://yuhaoo00.github.io/apple-touch-icon.png alt aria-label=logo height=35>Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Network Design in Stable Diffusion</h1><div class=post-meta><span title='2023-02-01 00:00:00 +0000 UTC'>February 1, 2023</span>&nbsp;·&nbsp;400 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#unet>Unet</a></li><li><a href=#vae>VAE</a><ul><li><a href=#tiled-processing>Tiled Processing</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on <a href=https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html>latent diffusion</a>, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.</p><h2 id=unet>Unet<a hidden class=anchor aria-hidden=true href=#unet>#</a></h2><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/4320ED37-C913-487C-AF89-3563E40F5828_1_201_a.jpeg#center alt="Fig. 1: Overall of the Unet in Stable Diffusion 1.x &amp;amp; 2.x"><figcaption><p>Fig. 1: Overall of the Unet in Stable Diffusion 1.x & 2.x</p></figcaption></figure><p>As in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x & SD2.x, $l_1=l_2=l_3=l_4=2$.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/B431465F-81A1-404F-9AA0-1474625D7B44_1_201_a.jpeg#center alt="Fig. 2: Skip-connection in the 2nd scale." width=70%><figcaption><p>Fig. 2: Skip-connection in the 2nd scale.</p></figcaption></figure><p>The skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer&rsquo;s input as in Fig. 2.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/A012E8AE-8131-4160-8158-03EEC2A01A61_1_201_a.jpeg#center alt="Fig 3: Transformer&amp;rsquo;s Design" width=50%><figcaption><p>Fig 3: Transformer&rsquo;s Design</p></figcaption></figure><p>In the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follows the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BA68D415-0DF9-4CB5-B0F4-59EC7C8033E5_1_201_a.jpeg#center alt="Fig. 4: Resnet block&amp;rsquo;s design." width=50%><figcaption><p>Fig. 4: Resnet block&rsquo;s design.</p></figcaption></figure><p>The SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling.
$$
\text{SiLU}(z)=z*\text{sigmoid}(z)
$$</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BFE0937A-36CA-4CC4-859F-BFC0F9F3EC51_1_201_a.jpeg#center alt="Fig. 5: Timestep Embedding (based on positional encoding)" width=50%><figcaption><p>Fig. 5: Timestep Embedding (based on positional encoding)</p></figcaption></figure><h2 id=vae>VAE<a hidden class=anchor aria-hidden=true href=#vae>#</a></h2><p>The KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don&rsquo;t have the same weight (SD2&rsquo;s VAE might be fine-tuned for higher resolution $768\times 768$).<br><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/CB9453C4-4906-4188-B99B-1186CDEDA771_1_201_a.jpeg#center alt="Fig.6: VAE Encoder"><figcaption><p>Fig.6: VAE Encoder</p></figcaption></figure></p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/774E90D6-50FD-4AD7-800D-D0B903C6549A_1_201_a.jpeg#center alt="Fig.6: VAE Decoder"><figcaption><p>Fig.6: VAE Decoder</p></figcaption></figure><h3 id=tiled-processing>Tiled Processing<a hidden class=anchor aria-hidden=true href=#tiled-processing>#</a></h3><blockquote><p>In inference, VAE decoding often occupies a lot of memory.</p></blockquote><p>In the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.</p><p>This is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ul><li><a href=https://github.com/huggingface/diffusers>https://github.com/huggingface/diffusers</a></li><li><a href=https://github.com/Stability-AI/stablediffusion>https://github.com/Stability-AI/stablediffusion</a></li><li><a href=https://github.com/Stability-AI/generative-models>https://github.com/Stability-AI/generative-models</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://yuhaoo00.github.io/tags/imagegeneration/>ImageGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/text2image/>Text2Image</a></li></ul><nav class=paginav><a class=prev href=https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/><span class=title>« Prev</span><br><span>Two Inpainting Pipelines in Diffusers</span></a>
<a class=next href=https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/><span class=title>Next »</span><br><span>Inverse Problem × Diffusion -- Part: B</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>