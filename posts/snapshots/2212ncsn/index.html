<!DOCTYPE html>
<html class="nojs" lang="en-GB" dir="ltr">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Image Generation based on Score Model – Yu&#39;s MemoCapsule</title>
<meta name="view-transition" content="same-origin">
<meta name="description" content=" Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function …">
<meta name="created" content="2022-12-08T00:00:00+0000">
<meta name="modified" content="2022-12-08T00:00:00+0000">

<meta name="contact" content="TimberH2000@outlook.com">
<meta property="og:site_name" content="Yu&#39;s MemoCapsule">
<meta property="og:title" content="Image Generation based on Score Model">
<meta property="og:url" content="https://yuhaoo00.github.io/posts/snapshots/2212ncsn/">
<meta property="og:type" content="article">
<meta property="og:image" content="https://yuhaoo00.github.io/favicon-192.png">
<meta name="generator" content="Hugo 0.118.2">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff">


<link rel="canonical" href="https://yuhaoo00.github.io/posts/snapshots/2212ncsn/">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link rel="stylesheet" href="/css/mobile.928a90a2e5ceccad718844ea67aa6bba151ff555ab514fb103071cc7b60c2ff2.css" media="screen">
<link rel="stylesheet" href="/css/styles.4b0725243fe74b2ef00d4a9effe4e55d8555c705d333dd673a4232e7c4693eac.css">
<link rel="stylesheet" href="/css/print.31e2819287afc91406f2fd43d21a8ba4a0cdfc272e439c90db0c6e47efc7c346.css" media="print">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "WebPage",
    "headline": "Image Generation based on Score Model",
    "datePublished": "2022-12-08T00:00:00Z",
    "dateModified": "2022-12-08T00:00:00Z",
    "url" : "https://yuhaoo00.github.io/posts/snapshots/2212ncsn/",
    "description": " Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function …",
    "keywords": ["Diffusion","ScoreModel","ImageGeneration","ClassicPaper"],
    "image" : "https://yuhaoo00.github.io/favicon-192.png",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://yuhaoo00.github.io"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Yu's MemoCapsule",
      "logo" : {
        "@type": "ImageObject",
        "url": "https://yuhaoo00.github.io/favicon-192.png"
      },
      "url": "https://yuhaoo00.github.io"
    }
  }
</script>

<script src="/js/script-early.214cc4b0248ef64425811a906c0c6a3702edffba5dbc127128e9ff989c74c137.js"></script>

<script defer src="/js/mobile.2ee7f57bec29e942a94fc13cd4cf128b0b8daf996652b9725d7e9bf960496721.js"></script>
<script defer src="/js/script.1688e04483adf683d47143ac70e770c7b5e4b75213641f33b46d9b84cb3e91ca.js"></script>

<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/contrib/auto-render.min.js"></script>
<script defer src="/js/math.f08366f847be855a2a8edced48e6cbaca080c199dfc2efd172b4788a2f58d85e.js"></script>



</head>

<body class="single-page">
<div class="page layout__page">
<header class="header layout__header">
<a href="/" title="Home" rel="home" class="header__logo">
<img src="/images/logo.png" width="40" height="40" alt="Home" class="header__logo-image">
</a>
<h1 class="header__site-name">
<a href="/" title="Home" class="header__site-link" rel="home"><span>Yu&#39;s MemoCapsule</span></a>
</h1>
<div class="region header__region">
</div>
<div class="mobile-nav" dir="ltr">
  <div class="mobile-nav__cover"></div>
  <button class="mobile-nav__toggle button--outline" aria-expanded="false" aria-controls="sheet">
    Menu
    <svg class="mobile-nav__hamburger" viewBox="0 0 100 100" focusable="false" aria-hidden="true">
      <rect width="80" height="12" x="10" y="20" rx="5"></rect>
      <rect width="80" height="12" x="10" y="45" rx="5"></rect>
      <rect width="80" height="12" x="10" y="70" rx="5"></rect>
    </svg>
  </button>
  <div class="mobile-nav__sheet link-inverted link-nav" id="sheet" aria-hidden="true">
    <div class="mobile-nav__region"></div>
    <nav class="mobile-nav__main-menu" aria-label="Main menu">
    <ul class="mobile-nav__navbar">
    <li><a href="/">Home</a></li>
    <li><a href="/contact/">Contact</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/posts/" aria-current="page">Posts</a></li>
    </ul>
    </nav>
  </div>
</div>
</header>

<nav class="main-menu layout__navigation" aria-label="Main menu">
<ul class="navbar">
<li><a href="/">Home</a></li>
<li><a href="/contact/">Contact</a></li>
<li><a href="/search/">Search</a></li>
<li><a href="/posts/" aria-current="page">Posts</a></li>
</ul>
</nav>
<main class="main layout__main">
<article class="single-view single-view--posts">
<header>
<h1 class="title mb--xxs">Image Generation based on Score Model</h1>
<div class="submitted meta">
<time class="created-date" datetime="2022-12-08T00:00:00Z">8 December, 2022</time>

</div>
<div class="tags meta">
Tags:
<ul>
<li><a href="https://yuhaoo00.github.io/tags/diffusion/">Diffusion</a></li>
<li><a href="https://yuhaoo00.github.io/tags/scoremodel/">ScoreModel</a></li>
<li><a href="https://yuhaoo00.github.io/tags/imagegeneration/">ImageGeneration</a></li>
<li><a href="https://yuhaoo00.github.io/tags/classicpaper/">ClassicPaper</a></li>
</ul>
</div>
</header>

<blockquote>
<p>Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function $\nabla_{ x} \log p_{\text {data }}( x)$) may be a better choice than learning the data density directly.</p>
</blockquote>
<h2 id="score-estimation-for-training" class="icon-inline" id="score-estimation-for-training">Score Estimation (for training)<a class="icon-link" href="#score-estimation-for-training" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>We want to train a network $s_{\theta}(x)$ to estimate $\nabla_{ x} \log p_{\text {data }}( x)$, but how can we get the ground truth (the real score)? In this paper, the objective $\frac{1}{2} E_{p_{\text{data}}} \lbrack\lVert s_{\theta}( x)-\nabla_{ x} \log p_{\text{data}}( x)\rVert_2^2\rbrack$ is equivalent to the following by Score Matching:</p>
<p>$$
E_{p_{\text{data}}}\left[tr(\nabla_{ x}s_{\theta}( x))+\frac{1}{2}\left|s_{\theta}( x))\right|_2^2\right]
$$</p>
<p>Unfortunately, it is not easy to compute $tr(\cdot)$ for a large-scale problem. Both <strong>Denoising score matching</strong> and <strong>Sliced score matching</strong> are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q_\sigma$) as follows:</p>
<p>$$
\frac{1}{2} E_{q_\sigma(\tilde{ x} \mid  x) p_{\text {data }}( x)}\left[\left| s_{\theta}(\tilde{ x})-\nabla_{\tilde{ x}} \log q_\sigma(\tilde{ x} \mid  x)\right|_2^2\right]
$$</p>
<p>Minimizing this objective, we can get $s_{\theta}( x)=\nabla_{ x} \log q_{\sigma}( x)$. And if the noise is small enough $q_\sigma( x) \approx p_{\text {data }}( x)$, we will get $s_{\theta}( x)=\nabla_{ x} \log q_\sigma( x) \approx \nabla_{ x} \log p_{\text {data }}( x)$.
As $q_\sigma(\tilde{x}| x)$ can be defined as a known simple distribution, this minimization is easier than regular score matching. In this paper, they choose $q_\sigma(\tilde{x}| x) =N\left(\tilde{x} \mid x, \sigma^2 I\right)$, which leads to:</p>
<p>$$
\ell(\theta ; \sigma) \triangleq \frac{1}{2} E_{p_{\text {data }}(x)} E_{\tilde{x} \sim N\left(x, \sigma^2 I\right)}\left[\left|\mathbf{s}_{\theta}(\tilde{x}, \sigma)+\frac{\tilde{x}-x}{\sigma^2}\right|_2^2\right]
$$</p>
<h2 id="langevin-dynamics-for-inference" class="icon-inline" id="langevin-dynamics-for-inference">Langevin dynamics (for inference)<a class="icon-link" href="#langevin-dynamics-for-inference" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>How can we do sampling from $p_{\text {data }}( x)$ when we get a nice estimation of $\nabla_{ x} \log p_{\text {data }}( x)$? Just along the direction of this gradient? Yes, but plus more tricks.</p>
<p>$$
x_t = x_{t-1}+\frac{\epsilon}{2} \nabla_{x} \log p\left(x_{t-1}\right)+\sqrt{\epsilon} z_t
$$</p>
<p>The Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide <em>more stable distribution</em>! Briefly, the random noise term $\mathbf{z}_t \thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\epsilon \to 0$), the iterative $x_t$ will approach the distribution $p(x)$.</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012150556.png" loading="lazy" width="400">
</figure>

<h2 id="practical-challenges" class="icon-inline" id="practical-challenges">Practical Challenges<a class="icon-link" href="#practical-challenges" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<h3 id="the-manifold-hypothesis" class="icon-inline" id="the-manifold-hypothesis">The manifold hypothesis<a class="icon-link" href="#the-manifold-hypothesis" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<blockquote>
<p>The data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).</p>
</blockquote>
<ul>
<li>The score is <em>undefined</em>.</li>
<li>The estimation by Score Matching <em>isn&rsquo;t consistent</em>.</li>
</ul>
<p>If we <strong>perturb the data</strong> with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast &amp; faithful) will converge (Fig. 1).</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192631.png" loading="lazy">
<figcaption>
Figure 1: Left: Sliced score matching (SSM) loss w.r.t. iterations. No noise is added to data. Right: Same but data are perturbed with N(0, 0.0001).</figcaption>
</figure>

<h3 id="low-data-density-regions" class="icon-inline" id="low-data-density-regions">Low data density regions<a class="icon-link" href="#low-data-density-regions" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<blockquote>
<p>Our training is based on the data in high density ($\thicksim p_{\text {data }}( x)$).</p>
</blockquote>
<ul>
<li>The estimation in low density regions is <em>inaccurate.</em></li>
<li>Regular Langevin Dynamics <em>can&rsquo;t be able to correctly recover</em> the relative weights of the multi-modal distribution <em>in reasonable time.</em></li>
</ul>
<p>If we <strong>perturb the data by using multiple noise levels</strong> (anneal down), we can fill the low density regions.</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192901.png" loading="lazy">
</figure>

<p>So naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions.
(Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)</p>
<h2 id="one-more-thing" class="icon-inline" id="one-more-thing">One More Thing<a class="icon-link" href="#one-more-thing" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>In the score matching, the authors approximately have $\left| s_{\theta}( x, \sigma)\right|_2 \propto 1 / \sigma$, so they choose $\lambda (\sigma) = \sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\sigma$.</p>
<p>$$
\mathcal{L}\left(\theta ;\lbrace\sigma_i \rbrace_{i=1}^{L}\right) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda\left(\sigma_i\right) \ell\left(\theta ; \sigma_i\right)
$$</p>
<p>Correspondingly, in the langevin dynamics, they choose $\alpha_i \propto \sigma^2$ to make the order magnitude of &ldquo;signal-to-noise ratio&rdquo; independent of $\sigma$.</p>
<h1 id="references" class="icon-inline" id="references">References<a class="icon-link" href="#references" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h1>
<ul>
<li>Yang Song and Stefano Ermon, ‘Generative Modeling by Estimating Gradients of the Data Distribution’, in <em>Advances in Neural Information Processing Systems</em>, 2019.</li>
<li>Yang Song and others, ‘Score-Based Generative Modeling through Stochastic Differential Equations’, in <em>International Conference on Learning Representations</em>, 2021.</li>
</ul>


</article>
</main>


<footer class="footer layout__footer mt--l">
<p><!--Creative Commons License-->This site is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA 4.0</a> licence.<!--/Creative Commons License--></p>


</footer>

</div>
</body>
</html>
