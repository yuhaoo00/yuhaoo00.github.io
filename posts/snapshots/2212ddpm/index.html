<!DOCTYPE html>
<html class="nojs" lang="en-GB" dir="ltr">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>DDPM and Early Variants – Yu&#39;s MemoCapsule</title>
<meta name="view-transition" content="same-origin">
<meta name="description" content=" Although Diffusion Model is a new generative framework, it still has many shades of other methods. Bayes&amp;amp;rsquo; rule is all you need Generation &amp;amp;amp; Diffusion Just like …">
<meta name="created" content="2022-12-13T00:00:00+0000">
<meta name="modified" content="2022-12-13T00:00:00+0000">

<meta name="contact" content="TimberH2000@outlook.com">
<meta property="og:site_name" content="Yu&#39;s MemoCapsule">
<meta property="og:title" content="DDPM and Early Variants">
<meta property="og:url" content="https://yuhaoo00.github.io/posts/snapshots/2212ddpm/">
<meta property="og:type" content="article">
<meta property="og:image" content="https://yuhaoo00.github.io/favicon-192.png">
<meta name="generator" content="Hugo 0.118.2">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#ffffff">


<link rel="canonical" href="https://yuhaoo00.github.io/posts/snapshots/2212ddpm/">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link rel="stylesheet" href="/css/mobile.928a90a2e5ceccad718844ea67aa6bba151ff555ab514fb103071cc7b60c2ff2.css" media="screen">
<link rel="stylesheet" href="/css/styles.4b0725243fe74b2ef00d4a9effe4e55d8555c705d333dd673a4232e7c4693eac.css">
<link rel="stylesheet" href="/css/print.31e2819287afc91406f2fd43d21a8ba4a0cdfc272e439c90db0c6e47efc7c346.css" media="print">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "WebPage",
    "headline": "DDPM and Early Variants",
    "datePublished": "2022-12-13T00:00:00Z",
    "dateModified": "2022-12-13T00:00:00Z",
    "url" : "https://yuhaoo00.github.io/posts/snapshots/2212ddpm/",
    "description": " Although Diffusion Model is a new generative framework, it still has many shades of other methods. Bayes\u0026amp;rsquo; rule is all you need Generation \u0026amp;amp; Diffusion Just like …",
    "keywords": ["Diffusion","ImageGeneration","ClassicPaper"],
    "image" : "https://yuhaoo00.github.io/favicon-192.png",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://yuhaoo00.github.io"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Yu's MemoCapsule",
      "logo" : {
        "@type": "ImageObject",
        "url": "https://yuhaoo00.github.io/favicon-192.png"
      },
      "url": "https://yuhaoo00.github.io"
    }
  }
</script>

<script src="/js/script-early.214cc4b0248ef64425811a906c0c6a3702edffba5dbc127128e9ff989c74c137.js"></script>

<script defer src="/js/mobile.2ee7f57bec29e942a94fc13cd4cf128b0b8daf996652b9725d7e9bf960496721.js"></script>
<script defer src="/js/script.1688e04483adf683d47143ac70e770c7b5e4b75213641f33b46d9b84cb3e91ca.js"></script>

<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/contrib/auto-render.min.js"></script>
<script defer src="/js/math.f08366f847be855a2a8edced48e6cbaca080c199dfc2efd172b4788a2f58d85e.js"></script>



</head>

<body class="single-page">
<div class="page layout__page">
<header class="header layout__header">
<a href="/" title="Home" rel="home" class="header__logo">
<img src="/images/logo.png" width="40" height="40" alt="Home" class="header__logo-image">
</a>
<h1 class="header__site-name">
<a href="/" title="Home" class="header__site-link" rel="home"><span>Yu&#39;s MemoCapsule</span></a>
</h1>
<div class="region header__region">
</div>
<div class="mobile-nav" dir="ltr">
  <div class="mobile-nav__cover"></div>
  <button class="mobile-nav__toggle button--outline" aria-expanded="false" aria-controls="sheet">
    Menu
    <svg class="mobile-nav__hamburger" viewBox="0 0 100 100" focusable="false" aria-hidden="true">
      <rect width="80" height="12" x="10" y="20" rx="5"></rect>
      <rect width="80" height="12" x="10" y="45" rx="5"></rect>
      <rect width="80" height="12" x="10" y="70" rx="5"></rect>
    </svg>
  </button>
  <div class="mobile-nav__sheet link-inverted link-nav" id="sheet" aria-hidden="true">
    <div class="mobile-nav__region"></div>
    <nav class="mobile-nav__main-menu" aria-label="Main menu">
    <ul class="mobile-nav__navbar">
    <li><a href="/">Home</a></li>
    <li><a href="/contact/">Contact</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/posts/" aria-current="page">Posts</a></li>
    </ul>
    </nav>
  </div>
</div>
</header>

<nav class="main-menu layout__navigation" aria-label="Main menu">
<ul class="navbar">
<li><a href="/">Home</a></li>
<li><a href="/contact/">Contact</a></li>
<li><a href="/search/">Search</a></li>
<li><a href="/posts/" aria-current="page">Posts</a></li>
</ul>
</nav>
<main class="main layout__main">
<article class="single-view single-view--posts">
<header>
<h1 class="title mb--xxs">DDPM and Early Variants</h1>
<div class="submitted meta">
<time class="created-date" datetime="2022-12-13T00:00:00Z">13 December, 2022</time>

</div>
<div class="tags meta">
Tags:
<ul>
<li><a href="https://yuhaoo00.github.io/tags/diffusion/">Diffusion</a></li>
<li><a href="https://yuhaoo00.github.io/tags/imagegeneration/">ImageGeneration</a></li>
<li><a href="https://yuhaoo00.github.io/tags/classicpaper/">ClassicPaper</a></li>
</ul>
</div>
</header>

<blockquote>
<p>Although Diffusion Model is a new generative framework, it still has many shades of other methods.</p>
</blockquote>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png" loading="lazy">
<figcaption>
Bayes&rsquo; rule is all you need</figcaption>
</figure>

<h2 id="generation--diffusion" class="icon-inline" id="generation--diffusion">Generation &amp; Diffusion<a class="icon-link" href="#generation--diffusion" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following <strong>Markov chain</strong> with learnable Gaussian transitions:</p>
<p>$$
\begin{align}
p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}\\
p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \vert x_t\right)\\
p_\theta\left(x_{t-1} \vert x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))
\end{align}
$$</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png" loading="lazy">
</figure>

<blockquote>
<p>Markov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\vert x_{t:T})=p(x_{t-1}\vert x_{t})$</p>
</blockquote>
<p>Similar to VAE, we can use the posterior $q(x_{1:t} \vert x_0)$ to do the estimation for $\theta$. The difference is that $x_1,\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \vert x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\alpha_{1:T}\in [0,1]^T$:</p>
<p>$$
\begin{align}
q(x_{1:T} \vert x_0) := \prod_{t=1}^T q\left(x_{t} \vert x_{t-1}\right) \\
q(x_t \vert x_{t-1}):=N(\frac{\sqrt{\alpha_t}}{\sqrt{\alpha_{t-1}}}x_{t-1}, (1-\frac{\alpha_t}{\alpha_{t-1}})I)
\end{align}
$$</p>
<p>A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$:</p>
<p>$$
q(x_t\vert x_0)=N(x_t;\sqrt{\alpha_t}x_0, (1-\alpha_t)I)
$$</p>
<h2 id="training-objective" class="icon-inline" id="training-objective">Training Objective<a class="icon-link" href="#training-objective" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<p>We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood:</p>
<p>$$
\max_{\theta}E_{q}[\log{p_\theta(x_0)}]\leq \max_{\theta}E_{q}[\log{p_{\theta} (x_{0:T})}-\log{q(x_{1:T} \vert x_0)}]
$$</p>
<p>which also can be driven by Jensen’s inequality as in <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" title="Lil'log">Lil&rsquo;log</a>. And we can further rewrite this object as:</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png" loading="lazy">
</figure>

<blockquote>
<p>Using Bayes&rsquo; rule, we can deduce the fact that $q(x_{t-1}\vert x_t, x_0)$ is also a gaussian distribution.</p>
</blockquote>
<p>where $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs.  <strong>If generative variances is all fixed  $\Sigma_t = \sigma^2_t$</strong>, using parameterization (fit distribution $\to$ fit mean $\to$ predict noise) and reweighting based on the empirical results, we can simplify this objective as follows:</p>
<p>$$
L_t=E_{x_0\sim q, \epsilon\sim N(0,1)}\left[|| \epsilon_\theta(\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\epsilon, t)-\epsilon {||}_2^2 \right]
$$</p>
<blockquote>
<p>NCSN vs DDPM, different ways lead to almost the same objective!</p>
</blockquote>
<p>For last $L_0$, DDPMs treat it as an independent discrete decoder derived from $N(x_0;\mu_{\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.</p>
<p>At the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\theta(x_{t-1}\vert x_t)$ using predicted $\epsilon_\theta(\cdot)$ (Alg. 2).</p>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png" loading="lazy">
</figure>

<h2 id="ddpm" class="icon-inline" id="ddpm">DDPM+<a class="icon-link" href="#ddpm" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<h3 id="finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality" class="icon-inline" id="finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality">Finding 1: Why fixing $\sigma^2$ to $\beta$ or$\tilde{\beta}$ achieve similar sample quality?<a class="icon-link" href="#finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png" loading="lazy" width="500">
</figure>

<p>As $\beta \approx \tilde{\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\frac{\beta_t}{\beta_{t-1}} \to 0$.</p>
<h3 id="finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly" class="icon-inline" id="finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly">Finding 2: The diffusion samples from linear schedule lose information very quickly.<a class="icon-link" href="#finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png" loading="lazy">
</figure>

<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png" loading="lazy" width="500">
</figure>

<p>In the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggests that so much full noisy samples might do not contribute to generation.</p>
<h3 id="improvements" class="icon-inline" id="improvements">Improvements<a class="icon-link" href="#improvements" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<ul>
<li>Learnable variances with an interpolation between $\beta$ and $\tilde{\beta}$, driven by loss $+ \lambda L_{vlb}$.</li>
<li>Cosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end.</li>
<li>Resampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting)</li>
</ul>
<h2 id="ddim" class="icon-inline" id="ddim">DDIM<a class="icon-link" href="#ddim" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h2>
<h3 id="more-free-diffusion-chain" class="icon-inline" id="more-free-diffusion-chain">More free diffusion chain<a class="icon-link" href="#more-free-diffusion-chain" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<p>In DDIM, the authors introduce a extended version $q(x_{t-1}\vert x_t, x_0)$, which has the same marginal noise distribution $q(x_{t}\vert x_0)$：</p>
<p>$$
q_{\sigma}(x_{t-1} \vert x_t, x_0) = N(x_{t-1}; \sqrt{\alpha_{t-1}} x_0 + \sqrt{1 - \alpha_{t-1} - \sigma_t^2} \frac{x_t - \sqrt{\alpha_t} x_0}{\sqrt{1 - \alpha_t}}, \sigma_t^2 I)
$$</p>
<p>Therefore, the corresponding generative process can be exported as:
<figure class="image">
<img src="https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013230807.png" loading="lazy">
</figure>

If we set $\sigma_t^2=\tilde{\beta}_t$,  the diffusion process becomes Markovian, and the generative process becomes DDPM. And if $\sigma_t=0$, there is a deterministic generation, called DDIM.</p>
<h3 id="acceleration-via-subsampling" class="icon-inline" id="acceleration-via-subsampling">Acceleration via subsampling<a class="icon-link" href="#acceleration-via-subsampling" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h3>
<p>In the generation, we sample a subset of S steps ${\tau_1,\dots,\tau_S}$ to form a new chain as:</p>
<p>$$
q_{\sigma, \tau}(x_{\tau_{i-1}} \vert x_{\tau_i}, x_0) = N(x_{\tau_{i-1}}; \sqrt{\alpha_{\tau_{i-1}}} x_0 + \sqrt{1 - \alpha_{\tau_{i-1}} - \sigma_t^2} \frac{x_{\tau_i} - \sqrt{\alpha_{\tau_i}} x_0}{\sqrt{1 - \alpha_{\tau_i}}}, \sigma_{\tau_i}^2 I)
$$</p>
<p>which can still provide high-quality samples using a much fewer number of steps.</p>
<h1 id="references" class="icon-inline" id="references">References<a class="icon-link" href="#references" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none" />
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg></a></h1>
<ul>
<li>Jonathan Ho, Ajay Jain, and Pieter Abbeel, ‘Denoising Diffusion Probabilistic Models’, in <em>Advances in Neural Information Processing Systems</em>, 2020.</li>
<li>Alexander Quinn Nichol and Prafulla Dhariwal, ‘Improved Denoising Diffusion Probabilistic Models’, in <em>Proceedings of the 38th International Conference on Machine Learning</em>, 2021.</li>
<li>Jiaming Song, Chenlin Meng, and Stefano Ermon, ‘Denoising Diffusion Implicit Models’, in <em>International Conference on Learning Representations</em>, 2021.</li>
</ul>


</article>
</main>


<footer class="footer layout__footer mt--l">
<p><!--Creative Commons License-->This site is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA 4.0</a> licence.<!--/Creative Commons License--></p>


</footer>

</div>
</body>
</html>
