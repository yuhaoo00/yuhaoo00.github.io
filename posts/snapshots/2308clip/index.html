<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Learning the Multi-modal Feature Space | Mem.Capsule</title><meta name=keywords content="ClassicPaper,MultiModal,ContrastiveLearning,Pre-training"><meta name=description content="In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.
CLIP -> Alec Radford, et al."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/snapshots/2308clip/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6c0e4385959ef5b32dbacc9d5c583ce22e75d9b61896b9fa8c1d6e4c98fbde8.css integrity="sha256-1sDkOFlZ71sy26zJ1cWDziLnXZthiWufqMHW5MmPveg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],output:"htmlAndMathml",strict:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Learning the Multi-modal Feature Space"><meta property="og:description" content="In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.
CLIP -> Alec Radford, et al."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/snapshots/2308clip/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-11T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-11T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Learning the Multi-modal Feature Space"><meta name=twitter:description content="In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.
CLIP -> Alec Radford, et al."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Learning the Multi-modal Feature Space","item":"https://yuhaoo00.github.io/posts/snapshots/2308clip/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning the Multi-modal Feature Space","name":"Learning the Multi-modal Feature Space","description":"In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.\nCLIP -\u0026gt; Alec Radford, et al.","keywords":["ClassicPaper","MultiModal","ContrastiveLearning","Pre-training"],"articleBody":"In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as how to learn a better multi-modal feature space.\nCLIP -\u003e Alec Radford, et al. NeuraIPS 2021\nThe model architecture of CLIP (source from paper)\nCLIP has a quite simple design which consists of a joint-training image encoder and text encoder, but there are still some interesting details worth noting.\n1.1 Bag-of-words encoding In the text encoder, the output at last [EOS] token are used as the feature representation of the text (i.e. $[N,L]\\to[N,L,D]\\to[N,D]$). The authors refer to this process as “Bag-of-words Encoding” where the text of arbitrary length can be encoded into a D-dimensional vector.\nIn Stable Diffusion, CLIP’s text encoder is used to provide crucial text feature. People often take full-sequence features (skip last 1 or 2 layers) from CLIP’s text encoder into the cross-attention to realize a “word-by-word instruction”. But indeed, if you take only the feature at last [EOS] token as the instruction, the results will be fine due to its globality.\n1.2 Contrastive learning The contrastive learning is implemented by optimizing a symmetric cross entropy loss over the cosine similarity score. We say that an image-text pair is matched if the cosine similarity between their embeddings is high enough.\nIn this paper, they set a learnable temperature parameter $e^t$ to scale the similarities, but didn’t explain much. I speculate such exponentiation can make learning more sensitive (imagine that steep curve) than a pure scalar.\nBTW, the authors separate out l2_normalize() and refer to np.dot() as the cosine similarity, which is of course equivalent but might be confusing.\nBLIP -\u003e Junnan Li, et al. ICML 2022 The model architecture of BLIP (source from paper)\nIn addition to the contrastive learning, BLIP introduces two more vision-language objectives: image-text matching and image-conditioned language modeling. Such multimodal mixture of encoder-decoder model is jointly trained with three objectives, enabling a wider range of downstream tasks.\n2.1 Coarse-grained alignment Similar to CLIP, BLIP includes the image-text contrastive learning by two unimodal encoders. We can think of such alignment is coarse-grained, since the interaction between the features from two modalities occurs only in the last shallow linear network.\nA few differences from CLIP are listed below:\nBLIP’s text encoder is BERT-like architecture which the global feature of the whole text is allocated at the first [CLS] token instead of the [EOS]. BLIP’s text encoder and image encoder is initialized from BERT-base and ViT (pre-trained on ImageNet), instead of training from scratch. (CLIP’s networks are heavily modified and scaled, so there are certainly no suitable pre-trained baselines for initialization.) 2.2 Fine-grained alignment The image-grounded (conditioned) text encoder aims to learn image-text multimodal representation with dense cross-attention layer, which can capture the fine-grained alignment. The objective is a binary classification task to predict whether an image-text pair is matched or not.\nNoting that here they adopt the hard negative mining strategy, where those negatives pairs with higher contrastive similarity in a batch will be more likely to be chosen to compute the 0-1 loss. It makes sense because some unmatched pairs may have quite high cosine scores. Such filtering strategy is obviously another sense of fine-grained.\n2.3 Language modeling Altering to the masked self-attention can preserve the ability of language modeling with a new auxiliary objective, and is the future work as mentioned in CLIP. Now BLIP realizes this thing with the image-grounded text decoder.\nThis decoder is inherited from the image-grounded text encoder, where bi-directional SAs are replaced with causal SAs (i.e. triangle) just like in the regular text decoder. Correspondingly, it optimizes a cross entropy loss which trains the model to maximize the likelihood of the text in an autoregressive manner. And the text decoder share all parameters with text encoder except for the SA layers, since the remaining layers serve a similar function.\nPreserving the ability of LM enforce the image feature to capture all the information about the text, therefore make it easy to transfer to those vision-language generation tasks, such as Image Captioning, Visual Question Answering, and etc. But this LM objective also makes the learned feature more “text-like”, whether from the text encoder, text decoder, or even image encoder.\n2.4 Dataset bootstrapping They introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (source from paper)\nBLIP-2 -\u003e Junnan Li, et al. arXiv 2023 The model architecture of BLIP-2 (source from paper)\nBLIP-2 consists of (1) an image transformer that interacts with the frozen image encoder for visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder. These two transformers share the same SA layers.\n3.1 Learnable queries A set number of learnable queries is created as input to the image transformer. Instead of taking image patches as input, allocating learnable queries can output the smaller-length $L=32$ features independent of image resolution (since the encoder/decoder-only model outputs the same length sequence as input). The output embeddings from these queries will be forced to extract visual features that capture all the information about the text, by three objectives. Due to such query’s importance, BLIP-2’s network is also known as the Q-Former.\n3.2 Shared self-attention We have observed that BLIP-1’s sub-networks have many common points in architectures and even in weights. So why not merge them as possible? and how?\nThe brilliance of BLIP-2 is that they employ different SA masking strategies for different objective to restrict the interaction between the vision token and text token, so two branches in the Q-Former can share the same SA layers. The self-attention masking strategy (source from paper)\nAlthough the masking strategies and the joint-training for three objectives sounds very straightforward and reasonable, there are a lot of unclear details waiting to be revealed in the source code.\nFor the contrastive learning, the query and text will be passed into Q-Former, respectively. Noting that the key\u0026value of SA layers in the image transformer will be cached for later use.\nquery_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, return_dict=True, ) For the image-text matching, the query will be concatenated with the text and passed into Q-Former. All queries and texts can attend to each other.\noutput_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) For the language modeling, there are only the text (first token is replaced by [DEC]) as input to the Q-Former. As the visual instruction, the previous key\u0026value caches (with pure visual information) will be concatenated with the current key\u0026value (from the texts), and then passed to the SA layers with a multimodal causal mask.\nlm_output = self.Qformer( decoder_input_ids, attention_mask=attention_mask, past_key_values=query_output.past_key_values, return_dict=True, labels=labels, ) Compared to BLIP-1, the shared SAs of BLIP2 make the output query embeddings more “text-like” inevitably, so the image encoder should be frozen to counteract such imbalance.\n3.3 Bootstrap LLM Benefit from the LM objective, the output query embedding contains rich image and text features. So them can function as soft visual prompts to LLM by a simple linear projection.\n(Top) Bootstrapping a decoder-based LLM. (Bottom) Bootstrapping an encoder-decoder-based LLM (source from paper)\n","wordCount":"1229","inLanguage":"en","datePublished":"2023-09-11T00:00:00Z","dateModified":"2023-09-11T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/snapshots/2308clip/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)"><img src=https://yuhaoo00.github.io/apple-touch-icon.png alt aria-label=logo height=35>Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Learning the Multi-modal Feature Space</h1><div class=post-meta><span title='2023-09-11 00:00:00 +0000 UTC'>September 11, 2023</span>&nbsp;·&nbsp;1229 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#clip>CLIP</a><ul><li><a href=#11-bag-of-words-encoding>1.1 Bag-of-words encoding</a></li><li><a href=#12-contrastive-learning>1.2 Contrastive learning</a></li></ul></li><li><a href=#blip>BLIP</a><ul><li><a href=#21-coarse-grained-alignment>2.1 Coarse-grained alignment</a></li><li><a href=#22-fine-grained-alignment>2.2 Fine-grained alignment</a></li><li><a href=#23-language-modeling>2.3 Language modeling</a></li><li><a href=#24-dataset-bootstrapping>2.4 Dataset bootstrapping</a></li></ul></li><li><a href=#blip-2>BLIP-2</a><ul><li><a href=#31-learnable-queries>3.1 Learnable queries</a></li><li><a href=#32-shared-self-attention>3.2 Shared self-attention</a></li><li><a href=#33-bootstrap-llm>3.3 Bootstrap LLM</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>In multi-modal tasks, one of the key challenges is the alignment between feature spaces of different modals. CLIP is representative of this type of work. Although its motivation is to learn a transferable visual model (like BERT) for downstream vision tasks, CLIP has brought a lot of inspirations for multi-modal tasks. Therefore, I prefer to describe CLIP and variants as <strong>how to learn a better multi-modal feature space</strong>.</p><h2 id=clip>CLIP<a hidden class=anchor aria-hidden=true href=#clip>#</a></h2><p><a href=https://proceedings.mlr.press/v139/radford21a.html>-> Alec Radford, et al. NeuraIPS 2021</a></p><p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023154456.png#center alt="The model architecture of CLIP (source from paper)" width=80%><figcaption><p>The model architecture of CLIP (source from paper)</p></figcaption></figure>CLIP has a quite simple design which consists of a <strong>joint-training</strong> image encoder and text encoder, but there are still some interesting details worth noting.</p><h3 id=11-bag-of-words-encoding>1.1 Bag-of-words encoding<a hidden class=anchor aria-hidden=true href=#11-bag-of-words-encoding>#</a></h3><p>In the text encoder, the output at last [EOS] token are used as the feature representation of the text (i.e. $[N,L]\to[N,L,D]\to[N,D]$). The authors refer to this process as &ldquo;Bag-of-words Encoding&rdquo; where the text of arbitrary length can be encoded into a D-dimensional vector.</p><blockquote><p>In Stable Diffusion, CLIP&rsquo;s text encoder is used to provide crucial text feature. People often take full-sequence features (skip last 1 or 2 layers) from CLIP&rsquo;s text encoder into the cross-attention to realize a &ldquo;word-by-word instruction&rdquo;. But indeed, if you take only the feature at last [EOS] token as the instruction, the results will be fine due to its globality.</p></blockquote><h3 id=12-contrastive-learning>1.2 Contrastive learning<a hidden class=anchor aria-hidden=true href=#12-contrastive-learning>#</a></h3><p>The contrastive learning is implemented by optimizing a symmetric cross entropy loss over the cosine similarity score. We say that an image-text pair is matched if the cosine similarity between their embeddings is high enough.</p><p>In this paper, they set a learnable temperature parameter $e^t$ to scale the similarities, but didn&rsquo;t explain much. I speculate such exponentiation can make learning more sensitive (imagine that steep curve) than a pure scalar.</p><blockquote><p>BTW, the authors separate out l2_normalize() and refer to np.dot() as the cosine similarity, which is of course equivalent but might be confusing.</p></blockquote><h2 id=blip>BLIP<a hidden class=anchor aria-hidden=true href=#blip>#</a></h2><p><a href=https://proceedings.mlr.press/v162/li22n.html>-> Junnan Li, et al. ICML 2022</a><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023171329.png#center alt="The model architecture of BLIP (source from paper)"><figcaption><p>The model architecture of BLIP (source from paper)</p></figcaption></figure></p><p>In addition to the contrastive learning, BLIP introduces two more vision-language objectives: image-text matching and image-conditioned language modeling. Such multimodal mixture of encoder-decoder model is jointly trained with three objectives, enabling a wider range of downstream tasks.</p><h3 id=21-coarse-grained-alignment>2.1 Coarse-grained alignment<a hidden class=anchor aria-hidden=true href=#21-coarse-grained-alignment>#</a></h3><p>Similar to CLIP, BLIP includes the image-text contrastive learning by two unimodal encoders. We can think of such alignment is coarse-grained, since the interaction between the features from two modalities occurs only in the last shallow linear network.</p><p>A few differences from CLIP are listed below:</p><ul><li>BLIP&rsquo;s text encoder is BERT-like architecture which the global feature of the whole text is allocated at the first [CLS] token instead of the [EOS].</li><li>BLIP&rsquo;s text encoder and image encoder is initialized from BERT-base and ViT (pre-trained on ImageNet), instead of training from scratch. (CLIP&rsquo;s networks are heavily modified and scaled, so there are certainly no suitable pre-trained baselines for initialization.)</li></ul><h3 id=22-fine-grained-alignment>2.2 Fine-grained alignment<a hidden class=anchor aria-hidden=true href=#22-fine-grained-alignment>#</a></h3><p>The image-grounded (conditioned) text encoder aims to learn image-text multimodal representation with dense cross-attention layer, which can capture the fine-grained alignment. The objective is a binary classification task to predict whether an image-text pair is matched or not.</p><p>Noting that here they adopt the <strong>hard negative mining strategy</strong>, where those negatives pairs with higher contrastive similarity in a batch will <strong>be more likely to be chosen</strong> to compute the 0-1 loss. It makes sense because some unmatched pairs may have quite high cosine scores. Such filtering strategy is obviously another sense of fine-grained.</p><h3 id=23-language-modeling>2.3 Language modeling<a hidden class=anchor aria-hidden=true href=#23-language-modeling>#</a></h3><p>Altering to the masked self-attention can preserve the ability of language modeling with a new auxiliary objective, and is the future work as mentioned in CLIP. Now BLIP realizes this thing with the image-grounded text decoder.</p><p>This decoder is inherited from the image-grounded text encoder, where bi-directional SAs are replaced with causal SAs (i.e. triangle) just like in the regular text decoder. Correspondingly, it optimizes a cross entropy loss which trains the model to maximize the likelihood of the text in an <strong>autoregressive manner</strong>. And the text decoder share all parameters with text encoder except for the SA layers, since the remaining layers serve a similar function.</p><p>Preserving the ability of LM enforce the image feature to capture all the information about the text, therefore make it easy to transfer to those vision-language generation tasks, such as Image Captioning, Visual Question Answering, and etc. But this LM objective also makes the learned feature more &ldquo;text-like&rdquo;, whether from the text encoder, text decoder, or even image encoder.</p><h3 id=24-dataset-bootstrapping>2.4 Dataset bootstrapping<a hidden class=anchor aria-hidden=true href=#24-dataset-bootstrapping>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231023173348.png#center alt="They introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (source from paper)"><figcaption><p>They introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (source from paper)</p></figcaption></figure><h2 id=blip-2>BLIP-2<a hidden class=anchor aria-hidden=true href=#blip-2>#</a></h2><p><a href=http://arxiv.org/abs/2301.12597>-> Junnan Li, et al. arXiv 2023</a><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025153821.png#center alt="The model architecture of BLIP-2 (source from paper)"><figcaption><p>The model architecture of BLIP-2 (source from paper)</p></figcaption></figure></p><p>BLIP-2 consists of (1) an image transformer that interacts with the frozen image encoder for visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder. These two transformers share the same SA layers.</p><h3 id=31-learnable-queries>3.1 Learnable queries<a hidden class=anchor aria-hidden=true href=#31-learnable-queries>#</a></h3><p>A set number of learnable queries is created as input to the image transformer. Instead of taking image patches as input, allocating learnable queries can output the smaller-length $L=32$ features independent of image resolution (since the encoder/decoder-only model outputs the same length sequence as input). The output embeddings from these queries will be forced to extract visual features that capture all the information about the text, by three objectives. Due to such query&rsquo;s importance, BLIP-2&rsquo;s network is also known as the <strong>Q-Former</strong>.</p><h3 id=32-shared-self-attention>3.2 Shared self-attention<a hidden class=anchor aria-hidden=true href=#32-shared-self-attention>#</a></h3><p>We have observed that BLIP-1&rsquo;s sub-networks have many common points in architectures and even in weights. So why not merge them as possible? and how?</p><p>The brilliance of BLIP-2 is that they employ different SA masking strategies for different objective to restrict the interaction between the vision token and text token, so two branches in the Q-Former can share the same SA layers.<figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025162242.png#center alt="The self-attention masking strategy (source from paper)"><figcaption><p>The self-attention masking strategy (source from paper)</p></figcaption></figure></p><p>Although the masking strategies and the joint-training for three objectives sounds very straightforward and reasonable, there are a lot of unclear details waiting to be revealed in the source code.</p><p><strong>For the contrastive learning</strong>, the query and text will be passed into Q-Former, respectively. Noting that the key&amp;value of SA layers in the image transformer will be cached for later use.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Qformer</span><span class=o>.</span><span class=n>bert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query_embeds</span><span class=o>=</span><span class=n>query_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>image_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_attention_mask</span><span class=o>=</span><span class=n>image_atts</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>use_cache</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Qformer</span><span class=o>.</span><span class=n>bert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>text_tokens</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=o>=</span><span class=n>text_tokens</span><span class=o>.</span><span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><p><strong>For the image-text matching</strong>, the query will be concatenated with the text and passed into Q-Former. All queries and texts can attend to each other.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output_itm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Qformer</span><span class=o>.</span><span class=n>bert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>text_ids_all</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>query_embeds</span><span class=o>=</span><span class=n>query_tokens_itm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask_all</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>image_embeds_all</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_attention_mask</span><span class=o>=</span><span class=n>image_atts_all</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><p><strong>For the language modeling</strong>, there are only the text (first token is replaced by [DEC]) as input to the Q-Former. As the visual instruction, the previous key&amp;value caches (with pure visual information) will be concatenated with the current key&amp;value (from the texts), and then passed to the SA layers with a multimodal causal mask.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lm_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Qformer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>decoder_input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>past_key_values</span><span class=o>=</span><span class=n>query_output</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><blockquote><p>Compared to BLIP-1, the shared SAs of BLIP2 make the output query embeddings more &ldquo;text-like&rdquo; inevitably, so the image encoder should be frozen to counteract such imbalance.</p></blockquote><h3 id=33-bootstrap-llm>3.3 Bootstrap LLM<a hidden class=anchor aria-hidden=true href=#33-bootstrap-llm>#</a></h3><p>Benefit from the LM objective, the output query embedding contains rich image and text features. So them can function as soft visual prompts to LLM by a simple linear projection.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231025213420.png#center alt="(Top) Bootstrapping a decoder-based LLM. (Bottom) Bootstrapping an encoder-decoder-based LLM (source from paper)"><figcaption><p>(Top) Bootstrapping a decoder-based LLM. (Bottom) Bootstrapping an encoder-decoder-based LLM (source from paper)</p></figcaption></figure></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/classicpaper/>ClassicPaper</a></li><li><a href=https://yuhaoo00.github.io/tags/multimodal/>MultiModal</a></li><li><a href=https://yuhaoo00.github.io/tags/contrastivelearning/>ContrastiveLearning</a></li><li><a href=https://yuhaoo00.github.io/tags/pre-training/>Pre-training</a></li></ul><nav class=paginav><a class=next href=https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/><span class=title>Next »</span><br><span>Two Inpainting Pipelines in Diffusers</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>