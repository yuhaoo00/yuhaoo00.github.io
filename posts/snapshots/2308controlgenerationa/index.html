<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Controllable Text-To-Image Diffusion Models —— Explicit Control | Mem.Capsule</title><meta name=keywords content="ImageGeneration,Diffusion,ControlledGeneration,ScoreModel"><meta name=description content="Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.
At present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6c0e4385959ef5b32dbacc9d5c583ce22e75d9b61896b9fa8c1d6e4c98fbde8.css integrity="sha256-1sDkOFlZ71sy26zJ1cWDziLnXZthiWufqMHW5MmPveg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/favicon-16.png><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/favicon-32.png><link rel=apple-touch-icon href=https://yuhaoo00.github.io/favicon-192.png><link rel=mask-icon href=https://yuhaoo00.github.io/favicon-full.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],output:"htmlAndMathml",strict:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Controllable Text-To-Image Diffusion Models —— Explicit Control"><meta property="og:description" content="Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.
At present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-17T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-17T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Controllable Text-To-Image Diffusion Models —— Explicit Control"><meta name=twitter:description content="Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.
At present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Controllable Text-To-Image Diffusion Models —— Explicit Control","item":"https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Controllable Text-To-Image Diffusion Models —— Explicit Control","name":"Controllable Text-To-Image Diffusion Models —— Explicit Control","description":"Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.\nAt present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control.","keywords":["ImageGeneration","Diffusion","ControlledGeneration","ScoreModel"],"articleBody":"Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.\nAt present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control.\nThis blog, as the first part of this series, will summarize representative researches related to Explicit Control, which can be further divided into two technical routes: forward guidance and backward guidance. The discussion of implicit control will be updated in another blog post.\nPS: This explicit or implicit statement comes from my personal preference and is not yet widely accepted in the academic community.\nEarly Explorations before the Stable Diffusion Image-to-Image \u0026 Inpainting -\u003e Chenlin Meng, et al. arXiv 2021 -\u003e Andreas Lugmayr, et al. CVPR 2022 The idea of explicit control can date back to before the Stable Diffusion, with two classic examples being SDEdit (Image-to-Image) and RePaint (Inpainting). Due to their relatively simple principles, here we just briefly summarize them:\nSDEdit uses a noisy version of a reference image as the starting point for the denoising process, ensuring that the generated image maintains a similar color layout to the reference image. Repaint, during the denoising process, combines the masked background image with the predicted foreground image to realize inpainting new content in the masked regions.\nBoth of these classic methods achieve controllable generation through modify denoising process (without any training), demonstrating the strong robustness of the diffusion model and the significant potential for modifying the denoising process manually!\nIn the Diffusers library, the StableDiffusionInpaintPipeline has two versions. The unfinetuned version is based on the idea of Repaint without training, while the finetuned version involves concatenating the Mask and Masked_image_latents as additional 5 channels to the input of Unet and fine-tuning all layers. The latter version achieves better results.\nBlended Diffusion -\u003e Omri Avrahami, et al. CVPR 2022 A simple diagram of the Blended Diffusion\nWith the introduction of CLIP technology, Blended Diffusion achieves text-driven inpainting. The basic approach can be summarized as above figure. In each iteration, $\\hat{x_0}$ is predicted based on the inverse equation of the known $p(x_t|x_0)$, and is masked accordingly. Then, the masked image and the text prompt are jointly input into CLIP to obtain a cosine similarity score $\\mathcal{L}$ (with the trick of data augmentation to improve sensitivity). The gradient of this score is used to update the latent $\\epsilon(x_t)+\\nabla_{\\hat{x_0}}\\mathcal{L}$. Moreover, the foreground and background images are fused based on the given mask to update the latent $x_{t-1}$ for next iteration.\nThis approach is essentially a combination of CLIP-based T2I and Repaint method. In fact, the CLIP-based generation is a kind of classifier-based guidance, where the iteration values are updated based on the gradient of a certain criterion. Although this gradient-based approach may sound a little old-school in the days of Classifier-Free Guidance (CFG), we can still see its effectiveness in serval cutting-edge studies later on.\nForward Guidance Prompt-to-Prompt Image Editing with Cross Attention -\u003e Amir Hertz, et al. arXiv 2022\nPrompt-to-Prompt (P2P) focuses on editing already generated images, which may not be relevant to controllable generation, but it shows great potential of manipulating Cross Attention (CA) maps and brings quite a lot of inspiration to controllable generation techniques!\nCross-Attention visualization in Unet (from the paper)\nAs shown above, P2P first found that CA corresponding to text tokens had strong semantic information (even in the early stages of generation). This finding seems obvious when you realize that CA is the similarity between image features and text features. But no one had visualized them before, and such strong semantic connections were not well known!\nAt the same time, this observation also reveals how the CA mechanism drives T2I generation. Let’s imagine the data flow: the sequence of text features from CLIP is first mapped to base values, then fused at different locations based on attention weights (the similarity between images and texts), and finally makes up the image we see (magical multimodal learning)!\nImage editing effect achieved by P2P (from the paper)\nMore excitingly, the authors successfully controlled generation by manipulating CA values:\nReplacing the CA corresponding to a text token for content replacement. Deleting the CA corresponding to a text token for content erasure. Adding the CA corresponding to a text token for content addition. Weighting the CA corresponding to a text token to effectively enhance or weaken content. Now we can enhance or weaken the content by weighting the text features from CLIP directly. This “weighted prompt” trick is widely integrated into various T2I tools (such as compel, A1111-webui, Midjourney, etc.).\nCompositional Visual Generation with Composable Diffusion Models -\u003e Nan Liu, et al. ECCV 2022\nA simple diagram of the Composed Diffusion\nComposed Diffusion supports the generation of more content or objects. Although this paper is inspired by composing multiple EBMs, this idea is equivalent to the advanced CFG formally. In short, multiple text prompts separated by the “AND” symbol are fed into the diffusion model separately and then weighted to produce the results: $$ \\hat{\\epsilon}(x_t,t)=\\epsilon(x_t,t)+\\sum_{i=1}^{n}w_i(\\epsilon(x_t,t|c_i)-\\epsilon(x_t,t)) $$ This synthesis is a bit crude and often less than expected, because different content will merge globally without explicit location constraints. Therefore, Composed Diffusion is more suitable for those multi-content with large spatial differences to avoid unexpected conflicts.\nComposing effect achieved by Composed Diffusion (from the paper))\nMultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation -\u003e Omer Bar-Tal, et al. arXiv 2023\nA simple diagram of the MultiDiffusion\nIf we want more control over spatial position, the most direct way is to draw something in a certain region by specifying a target mask or box. While this sounds like a multi-region inpainting task, the question is how to ensure that they are independent and harmonious with each other?\nMultiDiffusion gives the answer. The authors first build a mathematical optimization model for harmonious multi-region generation and derive a closed-form solution: $$ \\Psi\\left(J_t \\mid z\\right)=\\arg\\min_{J\\in\\mathcal{J}}\\sum_{i=1}^n\\left|W_i \\otimes\\left[F_i(J)-\\Phi\\left(I_t^i \\mid y_i\\right)\\right]\\right|^2 $$ $$ \\Psi\\left(J_t \\mid z\\right)=\\sum_{i=1}^n \\frac{F_i^{-1}\\left(W_i\\right)}{\\sum_{j=1}^n F_j^{-1}\\left(W_j\\right)} \\otimes F_i^{-1}\\left(\\Phi\\left(I_t^i \\mid y_i\\right)\\right) $$ In fact, MultiDiffusion can be understood as a multi-region extension of Repaint. It fuses multiple images generated from different regions (depending on the degree of overlap) in each step, and then eliminates dissonance at the boundaries solely by the robustness of the denoising process. Obviously, due to the lack of interaction between the central features of different regions, the final generated image has obvious style variations across regions.\nThe derived fusion equation is intuitive and can be obtained even without establishing the optimization problem. But it does make the whole paper more academic… -_-\nTraining-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis -\u003e Weixi Feng, et al. ICLR 2023 A simple diagram of the Structured Diffusion\nSimilar to the goal of Composed Diffusion, Structured Diffusion seeks to synthesize more content through text input alone. The approach derives from two key observations: first, the Cross-Attention of the input text token is very semantic (also observed by P2P); Second, CLIP’s text encoder (based on Transformer) will inevitably incorporate contextual information for each token.\nTherefore, Structured Diffusion attempts to ensure that specific text is not affected by context so that the corresponding content is properly synthesized. They first strip all the noun phrases from a sentence using a parser like Constituency Tree or Scene Graph; Then input these noun phrases into the CLIP text encoder separately to avoid mutual influence; And fill in the rest with the features of the whole sentence $W_p$; Finally, in the Unet, these new text features $W_i$ will be mapped into new values $V_i$, and fused according to the old CA $M^t$: $$ O^t=\\frac{1}{n}\\sum_{i=1}^{n}(M^tV_i) $$\nNote that attention is still computed from the original whole-sentence query, which contribute to the spatial layout allocation.\nThus, we enhance the features of all noun phrases and reduce their interference with each other. Unfortunately, the actual improvement from this approach is not obvious.\nBackward Guidance Training-Free Layout Control with Cross-Attention Guidance -\u003e Minghao Chen, et al. arXiv 2023\nThis paper attempts to implement control generation for multiple regions with a single diffusion model, called layout control. Interestingly, the authors explore two different methods of guidance: 1. a region constraint imposed in the generation iteration, also known as forward guidance; 2. a gradient-based update idea similar to Blended Diffusion, also known as backward guidance.\nA simple diagram of the Layout Control\nIn particular, the forward guidance will modify the Cross-Attention of the specified text tokens (in each layer) to cluster them into the specified box: $$ \\hat{A_{p,i}}=(1-\\lambda)A_{p,i}+\\lambda g_p\\sum_{j}A_{j,i} $$ Where $p$ is the position coordinate, $i$ is the index of the text token, $g_p$ is the gaussian weight in a specified box (sum of 1), and $\\sum_{j}A_{j,i}$ is the sum of the entire CA Map. It means that the latter term in the equation will redistribute the attention weights according to the gaussian box.\nBackward guidance defines a function to measure the CA concentration of a given token over a given box $B$: $$ E(A,B,i)=\\left(1-\\frac{\\sum_{p\\in B}A_{p,i}}{\\sum_{p}A_{p,i}}\\right)^2 $$ We update the latent $x_t$ at the current time step with the gradient of this metric as shown in the figure.\nAccording to the experimental results, the backward guidance is more effective (and of course consumes more running time). In addition, the author also integrates this method with Dreambooth and Text Inversion, which still achieve a good control effect.\nAttend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models -\u003e Hila Chefer et al. ACM Trans. Graph. 2023\nSimilar to the observations in P2P, this paper assumes that if something is not generated as expected, the corresponding Cross-Attention should be increased. The difference, however, is that Attend-and-Excite establishes a certain metric as a loss function and backpropagates the gradient to excite the update of latents, rather than forcing up the corresponding value by direct re-weighting.\nAttend-and-Excite\nSpecifically, we first need to select a few text tokens as the target to be enhanced (for example, the 2nd and 5th), calculate their corresponding average cross-attention ($A_t^2,A_t^5$), and then take the smallest peak of them as the loss: $$ Loss=max\\left(1-max(A_t^2),1-max(A_t^5)\\right) $$ Finally, the latent is updated according to the gradient as $x_t^{\\prime}=x_t-\\alpha \\nabla_{x_t} Loss$, which encourages the peak cross-attention of all selected text tokens to be as high as possible. Of course, there are several limitations of Attend-and-Excite as follows:\nManually selecting the index of the enhanced token is not a convenient and perfect solution. If and only the excitation in the middle scale of Unet is effective. Because not all scales of cross-attention have semantic information and contribute semantically to the final generation. This involves too many hyperparameter settings, such as artificial thresholds that determine the number of backward updates during early iterations. DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing -\u003e Yujun Shi, et al. arXiv 2023\nDragDiffusion is a direct extension of the famous DragGAN. Similarly, DragDiffusion can achieve image editing by dragging multiple points, and the principle behind it is the backward guidance for the generated path by gradient-based update.\nAttend-and-Excite\nAs shown above, in addition to the regular diffusion denoising, the generated path in DragDiffusion contains two other mappings: Motion Supervision and Point Tracking. The loss function of Motion Supervision is shown as follows: $$ \\mathcal{L}\\left(z_t^k\\right)= \\sum_{i=1}^n \\sum_{q \\in \\Omega\\left(h_i^k, r_1\\right)}\\lVert F_{q+d_i}\\left(z_t^k\\right)-\\operatorname{sg}\\left(F_q\\left(z_t^k\\right)\\right) \\rVert_1 +\\lambda \\lVert \\left(z_{t-1}^k-\\operatorname{sg}\\left(z_{t-1}^0\\right)\\right) \\odot(1-M) \\rVert_1 $$ In simple terms, the former of this loss encourages feature movement at the patch level $F_{\\Omega+d_i}(z_t^k) \\gets F_{\\Omega}(z_t^k)$, while the latter keeps the pixel values outside the mask nearly constant $(1-M)z_t^k \\approx (1-M)z_t^{k+1}$.\nHere $sg(\\cdot)$ is the stop gradient operator, which makes the pixel values $F_{\\Omega}(z_t^k)$ in the original patch are not affected by gradient descent directly. We can calculate the gradient and update the latent as follows: $$ z_t^{k+1}=z_t^k-\\eta \\cdot \\nabla_{z_t^k} \\mathcal{L} $$ Point Tracking is used to rematch the location of the handle points, which is implemented directly using a nearest neighbor search: $$ h_i^{k+1}=\\underset{q \\in \\Omega\\left(h_i^k, r_2\\right)}{\\arg \\min }\\left|F_q\\left(z_t^{k+1}\\right)-F_{h_i^k}\\left(z_t\\right)\\right|_1 $$ Although this method directly follows DragGAN’s idea, the non-stationary iterative process of the Diffusion Models makes many hyperparameter different, and the author also abandons CFG to avoid large numerical errors.\n","wordCount":"2065","inLanguage":"en","datePublished":"2023-08-17T00:00:00Z","dateModified":"2023-08-17T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)">Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Controllable Text-To-Image Diffusion Models —— Explicit Control</h1><div class=post-meta><span title='2023-08-17 00:00:00 +0000 UTC'>August 17, 2023</span>&nbsp;·&nbsp;2065 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#early-explorations-before-the-stable-diffusion>Early Explorations before the Stable Diffusion</a><ul><li><a href=#image-to-image--inpainting>Image-to-Image & Inpainting</a></li><li><a href=#blended-diffusion>Blended Diffusion</a></li></ul></li><li><a href=#forward-guidance>Forward Guidance</a><ul><li><a href=#prompt-to-prompt-image-editing-with-cross-attention>Prompt-to-Prompt Image Editing with Cross Attention</a></li><li><a href=#compositional-visual-generation-with-composable-diffusion-models>Compositional Visual Generation with Composable Diffusion Models</a></li><li><a href=#multidiffusion-fusing-diffusion-paths-for-controlled-image-generation>MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</a></li><li><a href=#training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</a></li></ul></li><li><a href=#backward-guidance>Backward Guidance</a><ul><li><a href=#training-free-layout-control-with-cross-attention-guidance>Training-Free Layout Control with Cross-Attention Guidance</a></li><li><a href=#attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</a></li><li><a href=#dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.</p><p>At present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control.</p><p>This blog, as the first part of this series, will summarize representative researches related to Explicit Control, which can be further divided into two technical routes: forward guidance and backward guidance. The discussion of implicit control will be updated in another blog post.</p><blockquote><p>PS: This explicit or implicit statement comes from my personal preference and is not yet widely accepted in the academic community.</p></blockquote><hr><h2 id=early-explorations-before-the-stable-diffusion>Early Explorations before the Stable Diffusion<a hidden class=anchor aria-hidden=true href=#early-explorations-before-the-stable-diffusion>#</a></h2><h3 id=image-to-image--inpainting>Image-to-Image & Inpainting<a hidden class=anchor aria-hidden=true href=#image-to-image--inpainting>#</a></h3><p><a href=http://arxiv.org/abs/2108.01073>-> Chenlin Meng, et al. arXiv 2021</a>
<a href=https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html>-> Andreas Lugmayr, et al. CVPR 2022</a>
The idea of explicit control can date back to before the Stable Diffusion, with two classic examples being SDEdit (Image-to-Image) and RePaint (Inpainting). Due to their relatively simple principles, here we just briefly summarize them:</p><p>SDEdit uses a noisy version of a reference image as the starting point for the denoising process, ensuring that the generated image maintains a similar color layout to the reference image. Repaint, during the denoising process, combines the masked background image with the predicted foreground image to realize inpainting new content in the masked regions.</p><p>Both of these classic methods achieve controllable generation through modify denoising process (without any training), demonstrating the strong robustness of the diffusion model and the significant potential for modifying the denoising process manually!</p><blockquote><p>In the <strong>Diffusers</strong> library, the <em>StableDiffusionInpaintPipeline</em> has two versions. The unfinetuned version is based on the idea of Repaint without training, while the finetuned version involves concatenating the <em>Mask</em> and <em>Masked_image_latents</em> as additional 5 channels to the input of Unet and fine-tuning all layers. The latter version achieves better results.</p></blockquote><h3 id=blended-diffusion>Blended Diffusion<a hidden class=anchor aria-hidden=true href=#blended-diffusion>#</a></h3><p><a href=https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html>-> Omri Avrahami, et al. CVPR 2022</a><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/DFD12FE9-4E0D-436D-8767-86E51992E1B6_1_201_a.jpeg#center alt="A simple diagram of the Blended Diffusion"><figcaption><p>A simple diagram of the Blended Diffusion</p></figcaption></figure></p><p>With the introduction of CLIP technology, Blended Diffusion achieves text-driven inpainting. The basic approach can be summarized as above figure. In each iteration, $\hat{x_0}$ is predicted based on the inverse equation of the known $p(x_t|x_0)$, and is masked accordingly. Then, the masked image and the text prompt are jointly input into CLIP to obtain a cosine similarity score $\mathcal{L}$ (with the trick of data augmentation to improve sensitivity). The gradient of this score is used to update the latent $\epsilon(x_t)+\nabla_{\hat{x_0}}\mathcal{L}$. Moreover, the foreground and background images are fused based on the given mask to update the latent $x_{t-1}$ for next iteration.</p><p>This approach is essentially a combination of CLIP-based T2I and Repaint method. In fact, the CLIP-based generation is a kind of classifier-based guidance, where the iteration values are updated based on the gradient of a certain criterion. Although this gradient-based approach may sound a little old-school in the days of Classifier-Free Guidance (CFG), we can still see its effectiveness in serval cutting-edge studies later on.</p><hr><h2 id=forward-guidance>Forward Guidance<a hidden class=anchor aria-hidden=true href=#forward-guidance>#</a></h2><h3 id=prompt-to-prompt-image-editing-with-cross-attention>Prompt-to-Prompt Image Editing with Cross Attention<a hidden class=anchor aria-hidden=true href=#prompt-to-prompt-image-editing-with-cross-attention>#</a></h3><p><a href=http://arxiv.org/abs/2208.01626>-> Amir Hertz, et al. arXiv 2022</a></p><p>Prompt-to-Prompt (P2P) focuses on editing already generated images, which may not be relevant to controllable generation, but it shows great potential of manipulating Cross Attention (CA) maps and brings quite a lot of inspiration to controllable generation techniques!</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106181510.png#center alt="Cross-Attention visualization in Unet (from the paper)"><figcaption><p>Cross-Attention visualization in Unet (from the paper)</p></figcaption></figure><p>As shown above, P2P first found that CA corresponding to text tokens had strong semantic information (even in the early stages of generation). This finding seems obvious when you realize that CA is the similarity between image features and text features. But no one had visualized them before, and such strong semantic connections were not well known!</p><p>At the same time, this observation also reveals how the CA mechanism drives T2I generation. Let&rsquo;s imagine the data flow: the sequence of text features from CLIP is first mapped to base values, then fused at different locations based on attention weights (the similarity between images and texts), and finally makes up the image we see (magical multimodal learning)!</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231107163401.png#center alt="Image editing effect achieved by P2P (from the paper)"><figcaption><p>Image editing effect achieved by P2P (from the paper)</p></figcaption></figure><p>More excitingly, the authors successfully controlled generation by manipulating CA values:</p><ul><li>Replacing the CA corresponding to a text token for content replacement.</li><li>Deleting the CA corresponding to a text token for content erasure.</li><li>Adding the CA corresponding to a text token for content addition.</li><li>Weighting the CA corresponding to a text token to effectively enhance or weaken content.</li></ul><blockquote><p>Now we can enhance or weaken the content by weighting the text features from CLIP directly. This <strong>&ldquo;weighted prompt&rdquo;</strong> trick is widely integrated into various T2I tools (such as compel, A1111-webui, Midjourney, etc.).</p></blockquote><h3 id=compositional-visual-generation-with-composable-diffusion-models>Compositional Visual Generation with Composable Diffusion Models<a hidden class=anchor aria-hidden=true href=#compositional-visual-generation-with-composable-diffusion-models>#</a></h3><p><a href=http://arxiv.org/abs/2206.01714>-> Nan Liu, et al. ECCV 2022</a></p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/9F007937-5B9C-4D33-A13E-DEC5BC7F7683_1_201_a.jpeg#center alt="A simple diagram of the Composed Diffusion"><figcaption><p>A simple diagram of the Composed Diffusion</p></figcaption></figure><p>Composed Diffusion supports the generation of more content or objects. Although this paper is inspired by composing multiple EBMs, this idea is equivalent to the advanced CFG formally. In short, multiple text prompts separated by the &ldquo;AND&rdquo; symbol are fed into the diffusion model separately and then weighted to produce the results:
$$
\hat{\epsilon}(x_t,t)=\epsilon(x_t,t)+\sum_{i=1}^{n}w_i(\epsilon(x_t,t|c_i)-\epsilon(x_t,t))
$$
This synthesis is a bit crude and often less than expected, because different content will merge globally without explicit location constraints. Therefore, Composed Diffusion is more suitable for those multi-content with large spatial differences to avoid unexpected conflicts.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106202519.png#center alt="Composing effect achieved by Composed Diffusion (from the paper))"><figcaption><p>Composing effect achieved by Composed Diffusion (from the paper))</p></figcaption></figure><h3 id=multidiffusion-fusing-diffusion-paths-for-controlled-image-generation>MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation<a hidden class=anchor aria-hidden=true href=#multidiffusion-fusing-diffusion-paths-for-controlled-image-generation>#</a></h3><p><a href=http://arxiv.org/abs/2302.08113>-> Omer Bar-Tal, et al. arXiv 2023</a></p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/0172B6F3-2F6E-49E8-82EF-07F6EB37DEF0_1_201_a.jpeg#center alt="A simple diagram of the MultiDiffusion"><figcaption><p>A simple diagram of the MultiDiffusion</p></figcaption></figure><p>If we want more control over spatial position, the most direct way is to draw something in a certain region by specifying a target mask or box. While this sounds like a multi-region inpainting task, the question is how to ensure that they are independent and harmonious with each other?</p><p>MultiDiffusion gives the answer. The authors first build a mathematical optimization model for harmonious multi-region generation and derive a closed-form solution:
$$
\Psi\left(J_t \mid z\right)=\arg\min_{J\in\mathcal{J}}\sum_{i=1}^n\left|W_i \otimes\left[F_i(J)-\Phi\left(I_t^i \mid y_i\right)\right]\right|^2
$$
$$
\Psi\left(J_t \mid z\right)=\sum_{i=1}^n \frac{F_i^{-1}\left(W_i\right)}{\sum_{j=1}^n F_j^{-1}\left(W_j\right)} \otimes F_i^{-1}\left(\Phi\left(I_t^i \mid y_i\right)\right)
$$
In fact, MultiDiffusion can be understood as a multi-region extension of Repaint. It fuses multiple images generated from different regions (depending on the degree of overlap) in each step, and then eliminates dissonance at the boundaries solely by the robustness of the denoising process. Obviously, due to the lack of interaction between the central features of different regions, the final generated image has obvious style variations across regions.</p><blockquote><p>The derived fusion equation is intuitive and can be obtained even without establishing the optimization problem. But it does make the whole paper more academic&mldr; -_-</p></blockquote><h3 id=training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis<a hidden class=anchor aria-hidden=true href=#training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis>#</a></h3><p><a href=http://arxiv.org/abs/2212.05032>-> Weixi Feng, et al. ICLR 2023</a><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/C5A36A3D-3958-4026-B90B-791D289DE33C_1_201_a.jpeg#center alt="A simple diagram of the Structured Diffusion"><figcaption><p>A simple diagram of the Structured Diffusion</p></figcaption></figure></p><p>Similar to the goal of Composed Diffusion, Structured Diffusion seeks to synthesize more content through text input alone. The approach derives from two key observations: first, the Cross-Attention of the input text token is very semantic (also observed by P2P); Second, CLIP&rsquo;s text encoder (based on Transformer) will inevitably incorporate contextual information for each token.</p><p>Therefore, Structured Diffusion attempts to ensure that specific text is not affected by context so that the corresponding content is properly synthesized. They first strip all the noun phrases from a sentence using a parser like Constituency Tree or Scene Graph; Then input these noun phrases into the CLIP text encoder separately to avoid mutual influence; And fill in the rest with the features of the whole sentence $W_p$; Finally, in the Unet, these new text features $W_i$ will be mapped into new values $V_i$, and fused according to the old CA $M^t$:
$$
O^t=\frac{1}{n}\sum_{i=1}^{n}(M^tV_i)
$$</p><blockquote><p>Note that attention is still computed from the original whole-sentence query, which contribute to the spatial layout allocation.</p></blockquote><p>Thus, we enhance the features of all noun phrases and reduce their interference with each other. Unfortunately, the actual improvement from this approach is not obvious.</p><hr><h2 id=backward-guidance>Backward Guidance<a hidden class=anchor aria-hidden=true href=#backward-guidance>#</a></h2><h3 id=training-free-layout-control-with-cross-attention-guidance>Training-Free Layout Control with Cross-Attention Guidance<a hidden class=anchor aria-hidden=true href=#training-free-layout-control-with-cross-attention-guidance>#</a></h3><p><a href=http://arxiv.org/abs/2304.03373>-> Minghao Chen, et al. arXiv 2023</a></p><p>This paper attempts to implement control generation for multiple regions with a single diffusion model, called layout control. Interestingly, the authors explore two different methods of guidance: 1. a region constraint imposed in the generation iteration, also known as forward guidance; 2. a gradient-based update idea similar to Blended Diffusion, also known as backward guidance.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/98AB3A71-A285-471A-A6AD-ADF1A157F1AF_1_201_a.jpeg#center alt="A simple diagram of the Layout Control"><figcaption><p>A simple diagram of the Layout Control</p></figcaption></figure><p>In particular, the forward guidance will modify the Cross-Attention of the specified text tokens (in each layer) to cluster them into the specified box:
$$
\hat{A_{p,i}}=(1-\lambda)A_{p,i}+\lambda g_p\sum_{j}A_{j,i}
$$
Where $p$ is the position coordinate, $i$ is the index of the text token, $g_p$ is the gaussian weight in a specified box (sum of 1), and $\sum_{j}A_{j,i}$ is the sum of the entire CA Map. It means that the latter term in the equation will redistribute the attention weights according to the gaussian box.</p><p>Backward guidance defines a function to measure the CA concentration of a given token over a given box $B$:
$$
E(A,B,i)=\left(1-\frac{\sum_{p\in B}A_{p,i}}{\sum_{p}A_{p,i}}\right)^2
$$
We update the latent $x_t$ at the current time step with the gradient of this metric as shown in the figure.</p><p>According to the experimental results, the backward guidance is more effective (and of course consumes more running time). In addition, the author also integrates this method with Dreambooth and Text Inversion, which still achieve a good control effect.</p><h3 id=attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models<a hidden class=anchor aria-hidden=true href=#attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models>#</a></h3><p><a href=https://doi.org/10.1145/3592116>-> Hila Chefer et al. ACM Trans. Graph. 2023</a></p><p>Similar to the observations in P2P, this paper assumes that if something is not generated as expected, the corresponding Cross-Attention should be increased. The difference, however, is that Attend-and-Excite establishes a certain metric as a loss function and backpropagates the gradient to excite the update of latents, rather than forcing up the corresponding value by direct re-weighting.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/90C7799C-B173-4CF9-B8FA-07BB9958EEA6_1_201_a.jpeg#center alt=Attend-and-Excite><figcaption><p>Attend-and-Excite</p></figcaption></figure><p>Specifically, we first need to select a few text tokens as the target to be enhanced (for example, the 2nd and 5th), calculate their corresponding average cross-attention ($A_t^2,A_t^5$), and then take the smallest peak of them as the loss:
$$
Loss=max\left(1-max(A_t^2),1-max(A_t^5)\right)
$$
Finally, the latent is updated according to the gradient as $x_t^{\prime}=x_t-\alpha \nabla_{x_t} Loss$, which encourages the peak cross-attention of all selected text tokens to be as high as possible. Of course, there are several limitations of Attend-and-Excite as follows:</p><ul><li><strong>Manually selecting</strong> the index of the enhanced token is not a convenient and perfect solution.</li><li>If and only the excitation in <strong>the middle scale of Unet</strong> is effective. Because not all scales of cross-attention have semantic information and contribute semantically to the final generation.</li><li>This involves <strong>too many hyperparameter settings</strong>, such as artificial thresholds that determine the number of backward updates during early iterations.</li></ul><h3 id=dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing<a hidden class=anchor aria-hidden=true href=#dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing>#</a></h3><p><a href=http://arxiv.org/abs/2306.14435>-> Yujun Shi, et al. arXiv 2023</a></p><p>DragDiffusion is a direct extension of the famous DragGAN. Similarly, DragDiffusion can achieve image editing by dragging multiple points, and the principle behind it is the backward guidance for the generated path by gradient-based update.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/42F9A785-50FF-4340-8262-2EE338776B96_1_201_a.jpeg#center alt=Attend-and-Excite><figcaption><p>Attend-and-Excite</p></figcaption></figure><p>As shown above, in addition to the regular diffusion denoising, the generated path in DragDiffusion contains two other mappings: Motion Supervision and Point Tracking. The loss function of Motion Supervision is shown as follows:
$$
\mathcal{L}\left(z_t^k\right)= \sum_{i=1}^n \sum_{q \in \Omega\left(h_i^k, r_1\right)}\lVert F_{q+d_i}\left(z_t^k\right)-\operatorname{sg}\left(F_q\left(z_t^k\right)\right) \rVert_1 +\lambda \lVert \left(z_{t-1}^k-\operatorname{sg}\left(z_{t-1}^0\right)\right) \odot(1-M) \rVert_1
$$
In simple terms, the former of this loss encourages feature movement at the patch level $F_{\Omega+d_i}(z_t^k) \gets F_{\Omega}(z_t^k)$, while the latter keeps the pixel values outside the mask nearly constant $(1-M)z_t^k \approx (1-M)z_t^{k+1}$.</p><p>Here $sg(\cdot)$ is the stop gradient operator, which makes the pixel values $F_{\Omega}(z_t^k)$ in the original patch are not affected by gradient descent directly. We can calculate the gradient and update the latent as follows:
$$
z_t^{k+1}=z_t^k-\eta \cdot \nabla_{z_t^k} \mathcal{L}
$$
Point Tracking is used to rematch the location of the handle points, which is implemented directly using a nearest neighbor search:
$$
h_i^{k+1}=\underset{q \in \Omega\left(h_i^k, r_2\right)}{\arg \min }\left|F_q\left(z_t^{k+1}\right)-F_{h_i^k}\left(z_t\right)\right|_1
$$
Although this method directly follows DragGAN&rsquo;s idea, the non-stationary iterative process of the Diffusion Models makes many hyperparameter different, and the author also abandons CFG to avoid large numerical errors.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/imagegeneration/>ImageGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://yuhaoo00.github.io/tags/controlledgeneration/>ControlledGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/scoremodel/>ScoreModel</a></li></ul><nav class=paginav><a class=prev href=https://yuhaoo00.github.io/posts/snapshots/2309clip/><span class=title>« Prev</span><br><span>Learning the Multi-modal Feature Space</span></a>
<a class=next href=https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/><span class=title>Next »</span><br><span>Two Inpainting Pipelines in Diffusers</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>