<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generating Images Like Texts | Mem.Capsule</title><meta name=keywords content="ImageGeneration,ClassicPaper,ControlledGeneration,Text2Image,Autoregression,MaskedTransformer"><meta name=description content="Can we generate images in the same way as autoregressive language model?
Although this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don&rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.
Taming Transformer -> Patrick Esser, et al. CVPR 2021
The key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts."><meta name=author content="Yuhao"><link rel=canonical href=https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6c0e4385959ef5b32dbacc9d5c583ce22e75d9b61896b9fa8c1d6e4c98fbde8.css integrity="sha256-1sDkOFlZ71sy26zJ1cWDziLnXZthiWufqMHW5MmPveg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yuhaoo00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yuhaoo00.github.io/favicon-16.png><link rel=icon type=image/png sizes=32x32 href=https://yuhaoo00.github.io/favicon-32.png><link rel=apple-touch-icon href=https://yuhaoo00.github.io/favicon-192.png><link rel=mask-icon href=https://yuhaoo00.github.io/favicon-full.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],output:"htmlAndMathml",strict:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Generating Images Like Texts"><meta property="og:description" content="Can we generate images in the same way as autoregressive language model?
Although this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don&rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.
Taming Transformer -> Patrick Esser, et al. CVPR 2021
The key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts."><meta property="og:type" content="article"><meta property="og:url" content="https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/"><meta property="og:image" content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-26T00:00:00+00:00"><meta property="og:site_name" content="Mem.Capsule"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuhaoo00.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Generating Images Like Texts"><meta name=twitter:description content="Can we generate images in the same way as autoregressive language model?
Although this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don&rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.
Taming Transformer -> Patrick Esser, et al. CVPR 2021
The key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yuhaoo00.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Generating Images Like Texts","item":"https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generating Images Like Texts","name":"Generating Images Like Texts","description":"Can we generate images in the same way as autoregressive language model?\nAlthough this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don\u0026rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.\nTaming Transformer -\u0026gt; Patrick Esser, et al. CVPR 2021\nThe key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts.","keywords":["ImageGeneration","ClassicPaper","ControlledGeneration","Text2Image","Autoregression","MaskedTransformer"],"articleBody":"Can we generate images in the same way as autoregressive language model?\nAlthough this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don’t worry too much, there are serval brilliant methods to try to make this idea more competitive.\nTaming Transformer -\u003e Patrick Esser, et al. CVPR 2021\nThe key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts. For this sake, the Taming Transformer is designed as a two-stage approach including a VQGAN and an Autoregressive Transformer.\n1.1 VQ-GAN Summary of taming transformer\nVector quantization is a brilliant idea to provide the compression and discrete representation for images (like a image tokenizer). Inspired by that, VQ-GAN realizes more effective representation with patchGAN, compressing an image into a learnable space (codebook). More precisely, any image $x\\in R^{H\\times W\\times 3}$ can be represented by a discrete vector $s \\in R^{h×w}$ (an index set of the closest codebook entries).\nThe training objective for finding the optimal compression model can be expressed as: $$ L_{VQ}=L_{perceptual}+||sg(E(x))-z_q||^2_2+\\beta||sg(z_q)-E(x)||^2_2 $$ $$ L_{GAN}=\\log{D(x)}+\\log{(1-D(\\hat{x}))} $$ $$ \\arg{\\min_{E,G,Z}\\max_{D}}\\ \\mathbb{E}\\lbrack L_{VQ}(E,G,Z)+\\lambda L_{GAN}(E,G,Z,D)\\rbrack $$ where the adaptive weight $\\lambda=\\nabla_{G_L}[L_{rec}]/(\\nabla_{G_L}[L_{GAN}]+10^{−6})$ tends to focus on the smaller one of $L_{rec}$ and $L_{GAN}$.\n1.2 Autoregression For the autoregressive transformer, it can be implemented by the “decoder-only” structure and the casual self-attention mask. And we can directly maximize the log-likelihood of the data representations as: $$ L_{Transformer} = \\mathbb{E}[-\\log(\\Pi_i{p(s_i|s_{","wordCount":"1031","inLanguage":"en","datePublished":"2023-09-26T00:00:00Z","dateModified":"2023-09-26T00:00:00Z","author":{"@type":"Person","name":"Yuhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/"},"publisher":{"@type":"Organization","name":"Mem.Capsule","logo":{"@type":"ImageObject","url":"https://yuhaoo00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yuhaoo00.github.io accesskey=h title="Mem.Capsule (Alt + H)">Mem.Capsule</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yuhaoo00.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yuhaoo00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yuhaoo00.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yuhaoo00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Generating Images Like Texts</h1><div class=post-meta><span title='2023-09-26 00:00:00 +0000 UTC'>September 26, 2023</span>&nbsp;·&nbsp;1031 words&nbsp;·&nbsp;Yuhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#taming-transformer>Taming Transformer</a><ul><li><a href=#11-vq-gan>1.1 VQ-GAN</a></li><li><a href=#12-autoregression>1.2 Autoregression</a></li></ul></li><li><a href=#parti>Parti</a><ul><li><a href=#21-vit-vqgan>2.1 ViT-VQGAN</a></li><li><a href=#22-text-to-image>2.2 Text-to-Image</a></li><li><a href=#23-fine-tune--super-resolution>2.3 Fine-tune & Super-Resolution</a></li></ul></li><li><a href=#muse>Muse</a><ul><li><a href=#31-frozen-llm-as-text-encoder>3.1 Frozen LLM as Text Encoder</a></li><li><a href=#32-base-model>3.2 Base Model</a></li><li><a href=#33-super-resolution-model>3.3 Super-Resolution Model</a></li><li><a href=#34-fine-tune>3.4 Fine-tune</a></li><li><a href=#35-iterative-parallel-decoding>3.5 Iterative Parallel Decoding</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Can we generate images in the same way as autoregressive language model?</p><p>Although this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don&rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.</p><h2 id=taming-transformer>Taming Transformer<a hidden class=anchor aria-hidden=true href=#taming-transformer>#</a></h2><p><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com">-> Patrick Esser, et al. CVPR 2021</a></p><p>The key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts. For this sake, the Taming Transformer is designed as a two-stage approach including a VQGAN and an Autoregressive Transformer.</p><h3 id=11-vq-gan>1.1 VQ-GAN<a hidden class=anchor aria-hidden=true href=#11-vq-gan>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026163628.png#center alt="Summary of taming transformer"><figcaption><p>Summary of taming transformer</p></figcaption></figure><p><a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf>Vector quantization</a> is a brilliant idea to provide the compression and discrete representation for images (like a image tokenizer). Inspired by that, VQ-GAN realizes more effective representation with <a href=https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html>patchGAN</a>, compressing an image into a learnable space (codebook). More precisely, any image $x\in R^{H\times W\times 3}$ can be represented by <strong>a discrete vector</strong> $s \in R^{h×w}$ (an index set of the closest codebook entries).</p><p>The training objective for finding the optimal compression model can be expressed as:
$$
L_{VQ}=L_{perceptual}+||sg(E(x))-z_q||^2_2+\beta||sg(z_q)-E(x)||^2_2
$$
$$
L_{GAN}=\log{D(x)}+\log{(1-D(\hat{x}))}
$$
$$
\arg{\min_{E,G,Z}\max_{D}}\ \mathbb{E}\lbrack L_{VQ}(E,G,Z)+\lambda L_{GAN}(E,G,Z,D)\rbrack
$$
where the adaptive weight $\lambda=\nabla_{G_L}[L_{rec}]/(\nabla_{G_L}[L_{GAN}]+10^{−6})$ tends to focus on the smaller one of $L_{rec}$ and $L_{GAN}$.</p><h3 id=12-autoregression>1.2 Autoregression<a hidden class=anchor aria-hidden=true href=#12-autoregression>#</a></h3><p>For the autoregressive transformer, it can be implemented by the &ldquo;decoder-only&rdquo; structure and the casual self-attention mask. And we can directly maximize the log-likelihood of the data representations as:
$$
L_{Transformer} = \mathbb{E}[-\log(\Pi_i{p(s_i|s_{&lt;i})})]
$$</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026184452.png#center alt="Sliding attention window"><figcaption><p>Sliding attention window</p></figcaption></figure><p>When generating images in the megapixel regime (>256^2), that transformer will adopt the local attention in a sliding-window manner for efficiency. Since transformer is a kind of network without inductive bias, we need the spatial conditioning information or the training dataset with spatial invariance to ensure such local strategy work well.</p><h2 id=parti>Parti<a hidden class=anchor aria-hidden=true href=#parti>#</a></h2><p><a href=http://arxiv.org/abs/2206.10789>-> Jiahui Yu, et al. arXiv 2022</a></p><p>Parti (Pathways Autoregressive Text-to-Image) shows that above two-stage autoregressive method is able to realize high-fidelity generation for text-to-image with additional fine-tuning and upsampling.</p><h3 id=21-vit-vqgan>2.1 ViT-VQGAN<a hidden class=anchor aria-hidden=true href=#21-vit-vqgan>#</a></h3><p>They first train a stronger <a href=https://arxiv.org/abs/2110.04627>ViT-VQGAN-Small</a> encoder (30M) as a image tokenizer on their training data, which achieves 4x downsampling (i.e., $256\to 32$) and learns 8192 image token classes for the codebook.</p><h3 id=22-text-to-image>2.2 Text-to-Image<a hidden class=anchor aria-hidden=true href=#22-text-to-image>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113357.png#center alt="Summary of Parti"><figcaption><p>Summary of Parti</p></figcaption></figure><p>The text-to-image model is based on a classical encoder-decoder architecture in multi-modal tasks.
<img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027131920.png alt=image.png></p><p>They build a <a href=https://arxiv.org/abs/1808.06226>SentencePiece model</a> as the text encoder which provides text tokens of length 128. This text encoder is first <strong>pretrain on two datasets</strong>: the C4 datasets with BERT loss, and their image-text datasets with contrastive loss.</p><p>After pretraining, they continue <strong>training both encoder and decoder</strong> for text-to-image generation with softmax cross-entropy loss. Nothing that the decoder uses conv-shaped masked sparse attention (like the sliding window in Taming Transformer).</p><blockquote><p>The ability of text encoder after pretraining performs comparably to BERT, but degrades after the full encoder-decoder training, which indicates the difference between language representation and image-grounded language representation.</p></blockquote><p><strong>Classifier-free guidance</strong> has been adopted to great effect for Parti. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt.</p><h3 id=23-fine-tune--super-resolution>2.3 Fine-tune & Super-Resolution<a hidden class=anchor aria-hidden=true href=#23-fine-tune--super-resolution>#</a></h3><p>After two-stage training, they freeze the image tokenizer and codebook, and fine-tune a larger-size image detokenizer (600M) to further improve visual acuity.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113410.png#center alt="A learned super-resolution module to upsample images"><figcaption><p>A learned super-resolution module to upsample images</p></figcaption></figure><p>Moreover, they employ a simple super-resolution network (15M~30M) on top of the image detokenizer. This SR-network is based on <a href=https://arxiv.org/abs/1808.08718>WDSR</a> and trained with the same losses of <a href=https://arxiv.org/abs/2110.04627>ViT-VQGAN</a> (perceptual loss, StyleGAN loss and l2 loss). It has about 15M parameters for 2x upsampling and 30M parameters for 4x upsampling.</p><h2 id=muse>Muse<a hidden class=anchor aria-hidden=true href=#muse>#</a></h2><p><a href=https://arxiv.org/abs/2301.00704>-> Huiwen Chang, et al. arXiv 2023</a></p><p>Although above solutions alleviate high training cost to some extent, the autoregressive paradigm still slows down inference significantly. Can we adopt parallel iteration instead of one by one? Muse give that answer which employs a <strong>random masking strategy</strong> (like MLM in NLP) to facilitate predicting multiple tokens at once.</p><h3 id=31-frozen-llm-as-text-encoder>3.1 Frozen LLM as Text Encoder<a hidden class=anchor aria-hidden=true href=#31-frozen-llm-as-text-encoder>#</a></h3><p>Recent works show that the conceptual representations learned by LLMs are roughly linearly mappable to those learned by models trained on vision tasks. Fueled by these observations, Muse adopts frozen <a href=https://dl.acm.org/doi/abs/10.5555/3455716.3455856>T5-XXL</a> as the text encoder and tries to map those rich visual and semantic concepts in the LLM embeddings to the generated images. These embedding vectors are linearly projected to the hidden size of later Transformer models (base and super-res).</p><h3 id=32-base-model>3.2 Base Model<a hidden class=anchor aria-hidden=true href=#32-base-model>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142154.png#center alt="The base model of Muse"><figcaption><p>The base model of Muse</p></figcaption></figure><p>Muse&rsquo;s base model has the same encoder-decoder architecture as Parti, but it employs a random masking strategy to ensure learning more expressive and robust.</p><p>They leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens and replace them with a special [MASK] token. The masking rate is a variable based on a cosine scheduling $r\sim p(r)=\frac{2}{\pi} (1 − r^2)^{-1/2}$, where the bias towards higher masking rates makes the prediction problem harder.</p><blockquote><p>Noting that this masking functions on input rather than attention layers.</p></blockquote><p>In this way, the base model is trained to predict all masked tokens at once.</p><h3 id=33-super-resolution-model>3.3 Super-Resolution Model<a hidden class=anchor aria-hidden=true href=#33-super-resolution-model>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142223.png#center alt="The super-resolution model of Muse"><figcaption><p>The super-resolution model of Muse</p></figcaption></figure><p>For the high-resolution generation, the authors found that directly predicting $512\times 512$ resolution leads the model to focus on low-level details over large-scale semantics. To this end, they trained another decoder to predict masked tokens in higher resolution with the help of low-res conditioning and text conditioning.</p><h3 id=34-fine-tune>3.4 Fine-tune<a hidden class=anchor aria-hidden=true href=#34-fine-tune>#</a></h3><p>Following the Parti model, Muse increases the capacity of the VQGAN decoder by the addition of more residual layers and channels, and then fine-tune the new decoder layers while other modules frozen.</p><h3 id=35-iterative-parallel-decoding>3.5 Iterative Parallel Decoding<a hidden class=anchor aria-hidden=true href=#35-iterative-parallel-decoding>#</a></h3><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027153559.png#center alt="Inference samples in Muse"><figcaption><p>Inference samples in Muse</p></figcaption></figure><p>The above masked learning support us to decode multiple tokens at each step, so this inference is called as iterative parallel decoding. Based on a cosine schedule, we predict all masked tokens at each step, and choose a fraction of the highest confidence tokens as unmasked, and continue next step to predict remaining masked tokens.</p><figure class=align-center><img loading=lazy src=https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027155432.png#center alt="Per-batch inference time for several models." width=50%><figcaption><p>Per-batch inference time for several models.</p></figcaption></figure><p>Using this procedure, Muse is able to perform high-fidelity inference using only 24 steps in base model and 8 steps in super-resolution model, and is significantly faster than competing diffusion or other autoregressive models.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuhaoo00.github.io/tags/imagegeneration/>ImageGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/classicpaper/>ClassicPaper</a></li><li><a href=https://yuhaoo00.github.io/tags/controlledgeneration/>ControlledGeneration</a></li><li><a href=https://yuhaoo00.github.io/tags/text2image/>Text2Image</a></li><li><a href=https://yuhaoo00.github.io/tags/autoregression/>Autoregression</a></li><li><a href=https://yuhaoo00.github.io/tags/maskedtransformer/>MaskedTransformer</a></li></ul><nav class=paginav><a class=next href=https://yuhaoo00.github.io/posts/snapshots/2309clip/><span class=title>Next »</span><br><span>Learning the Multi-modal Feature Space</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yuhaoo00.github.io>Mem.Capsule</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>