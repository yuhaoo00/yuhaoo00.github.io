<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Autoregression on Mem.Capsule ðŸ’Š</title>
    <link>https://yuhaoo00.github.io/tags/autoregression/</link>
    <description>Recent content in Autoregression on Mem.Capsule ðŸ’Š</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <lastBuildDate>Tue, 26 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yuhaoo00.github.io/tags/autoregression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Generating Images Like Texts</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2309arforvisiongeneration/</guid>
      <description>&lt;p&gt;Can we generate images in the same way as autoregressive language model?&lt;/p&gt;
&lt;p&gt;Although this sounds simpler than diffusion models, we still need to deal with many computational cost problems. But don&amp;rsquo;t worry too much, there are serval brilliant methods to try to make this idea more competitive.&lt;/p&gt;
&lt;h2 id=&#34;taming-transformer&#34; class=&#34;icon-inline&#34; id=&#34;taming-transformer&#34;&gt;Taming Transformer&lt;a class=&#34;icon-link&#34; href=&#34;#taming-transformer&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com&#34;&gt;-&amp;gt; Patrick Esser, et al. CVPR 2021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The key challenge of autoregressive generation is how to solve the quadratically increasing cost of image sequences that are much longer than texts. For this sake, the Taming Transformer is designed as a two-stage approach including a VQGAN and an Autoregressive Transformer.&lt;/p&gt;
&lt;h3 id=&#34;11-vq-gan&#34; class=&#34;icon-inline&#34; id=&#34;11-vq-gan&#34;&gt;1.1 VQ-GAN&lt;a class=&#34;icon-link&#34; href=&#34;#11-vq-gan&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026163628.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Summary of taming transformer&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf&#34;&gt;Vector quantization&lt;/a&gt; is a brilliant idea to provide the compression and discrete representation for images (like a image tokenizer). Inspired by that, VQ-GAN realizes more effective representation with &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html&#34;&gt;patchGAN&lt;/a&gt;, compressing an image into a learnable space (codebook). More precisely, any image $x\in R^{H\times W\times 3}$ can be represented by &lt;strong&gt;a discrete vector&lt;/strong&gt; $s \in R^{hÃ—w}$ (an index set of the closest codebook entries).&lt;/p&gt;
&lt;p&gt;The training objective for finding the optimal compression model can be expressed as:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp;L_{VQ}=L_{perceptual}+||sg(E(x))-z_q||^2_2+\beta||sg(z_q)-E(x)||^2_2 \\
&amp;amp;L_{GAN}=\log{D(x)}+\log{(1-D(\hat{x}))} \\
&amp;amp;\arg{\min_{E,G,Z}\max_{D}}\ \mathbb{E}\lbrack L_{VQ}(E,G,Z)+\lambda L_{GAN}(E,G,Z,D)\rbrack
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where the adaptive weight $\lambda=\nabla_{G_L}[L_{rec}]/(\nabla_{G_L}[L_{GAN}]+10^{âˆ’6})$ tends to focus on the smaller one of $L_{rec}$ and $L_{GAN}$.&lt;/p&gt;
&lt;h3 id=&#34;12-autoregression&#34; class=&#34;icon-inline&#34; id=&#34;12-autoregression&#34;&gt;1.2 Autoregression&lt;a class=&#34;icon-link&#34; href=&#34;#12-autoregression&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For the autoregressive transformer, it can be implemented by the &amp;ldquo;decoder-only&amp;rdquo; structure and the casual self-attention mask. And we can directly maximize the log-likelihood of the data representations as:&lt;/p&gt;
&lt;p&gt;$$
L_{Transformer} = \mathbb{E}[-\log(\Pi_i{p(s_i|s_{&amp;lt;i})})]
$$&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231026184452.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Sliding attention window&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;When generating images in the megapixel regime (&amp;gt;256^2), that transformer will adopt the local attention in a sliding-window manner for efficiency. Since transformer is a kind of network without inductive bias, we need the spatial conditioning information or the training dataset with spatial invariance to ensure such local strategy work well.&lt;/p&gt;
&lt;h2 id=&#34;parti&#34; class=&#34;icon-inline&#34; id=&#34;parti&#34;&gt;Parti&lt;a class=&#34;icon-link&#34; href=&#34;#parti&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2206.10789&#34;&gt;-&amp;gt; Jiahui Yu, et al. arXiv 2022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Parti (Pathways Autoregressive Text-to-Image) shows that above two-stage autoregressive method is able to realize high-fidelity generation for text-to-image with additional fine-tuning and upsampling.&lt;/p&gt;
&lt;h3 id=&#34;21-vit-vqgan&#34; class=&#34;icon-inline&#34; id=&#34;21-vit-vqgan&#34;&gt;2.1 ViT-VQGAN&lt;a class=&#34;icon-link&#34; href=&#34;#21-vit-vqgan&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;They first train a stronger &lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;ViT-VQGAN-Small&lt;/a&gt; encoder (30M) as a image tokenizer on their training data, which achieves 4x downsampling (i.e., $256\to 32$) and learns 8192 image token classes for the codebook.&lt;/p&gt;
&lt;h3 id=&#34;22-text-to-image&#34; class=&#34;icon-inline&#34; id=&#34;22-text-to-image&#34;&gt;2.2 Text-to-Image&lt;a class=&#34;icon-link&#34; href=&#34;#22-text-to-image&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113357.png&#34; loading=&#34;lazy&#34; width=&#34;600&#34;&gt;
&lt;figcaption&gt;
Summary of Parti&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The text-to-image model is based on a classical encoder-decoder architecture in multi-modal tasks.
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027131920.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;They build a &lt;a href=&#34;https://arxiv.org/abs/1808.06226&#34;&gt;SentencePiece model&lt;/a&gt; as the text encoder which provides text tokens of length 128. This text encoder is first &lt;strong&gt;pretrain on two datasets&lt;/strong&gt;: the C4 datasets with BERT loss, and their image-text datasets with contrastive loss.&lt;/p&gt;
&lt;p&gt;After pretraining, they continue &lt;strong&gt;training both encoder and decoder&lt;/strong&gt; for text-to-image generation with softmax cross-entropy loss. Nothing that the decoder uses conv-shaped masked sparse attention (like the sliding window in Taming Transformer).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ability of text encoder after pretraining performs comparably to BERT, but degrades after the full encoder-decoder training, which indicates the difference between language representation and image-grounded language representation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Classifier-free guidance&lt;/strong&gt; has been adopted to great effect for Parti. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt.&lt;/p&gt;
&lt;h3 id=&#34;23-fine-tune--super-resolution&#34; class=&#34;icon-inline&#34; id=&#34;23-fine-tune--super-resolution&#34;&gt;2.3 Fine-tune &amp;amp; Super-Resolution&lt;a class=&#34;icon-link&#34; href=&#34;#23-fine-tune--super-resolution&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;After two-stage training, they freeze the image tokenizer and codebook, and fine-tune a larger-size image detokenizer (600M) to further improve visual acuity.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027113410.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
A learned super-resolution module to upsample images&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Moreover, they employ a simple super-resolution network (15M~30M) on top of the image detokenizer. This SR-network is based on &lt;a href=&#34;https://arxiv.org/abs/1808.08718&#34;&gt;WDSR&lt;/a&gt; and trained with the same losses of &lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;ViT-VQGAN&lt;/a&gt; (perceptual loss, StyleGAN loss and l2 loss). It has about 15M parameters for 2x upsampling and 30M parameters for 4x upsampling.&lt;/p&gt;
&lt;h2 id=&#34;muse&#34; class=&#34;icon-inline&#34; id=&#34;muse&#34;&gt;Muse&lt;a class=&#34;icon-link&#34; href=&#34;#muse&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.00704&#34;&gt;-&amp;gt; Huiwen Chang, et al. arXiv 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although above solutions alleviate high training cost to some extent, the autoregressive paradigm still slows down inference significantly. Can we adopt parallel iteration instead of one by one? Muse give that answer which employs a &lt;strong&gt;random masking strategy&lt;/strong&gt; (like MLM in NLP) to facilitate predicting multiple tokens at once.&lt;/p&gt;
&lt;h3 id=&#34;31-frozen-llm-as-text-encoder&#34; class=&#34;icon-inline&#34; id=&#34;31-frozen-llm-as-text-encoder&#34;&gt;3.1 Frozen LLM as Text Encoder&lt;a class=&#34;icon-link&#34; href=&#34;#31-frozen-llm-as-text-encoder&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Recent works show that the conceptual representations learned by LLMs are roughly linearly mappable to those learned by models trained on vision tasks. Fueled by these observations, Muse adopts frozen &lt;a href=&#34;https://dl.acm.org/doi/abs/10.5555/3455716.3455856&#34;&gt;T5-XXL&lt;/a&gt; as the text encoder and tries to map those rich visual and semantic concepts in the LLM embeddings to the generated images. These embedding vectors are linearly projected to the hidden size of later Transformer models (base and super-res).&lt;/p&gt;
&lt;h3 id=&#34;32-base-model&#34; class=&#34;icon-inline&#34; id=&#34;32-base-model&#34;&gt;3.2 Base Model&lt;a class=&#34;icon-link&#34; href=&#34;#32-base-model&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142154.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The base model of Muse&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Muse&amp;rsquo;s base model has the same encoder-decoder architecture as Parti, but it employs a random masking strategy to ensure learning more expressive and robust.&lt;/p&gt;
&lt;p&gt;They leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens and replace them with a special [MASK] token. The masking rate is a variable based on a cosine scheduling $r\sim p(r)=\frac{2}{\pi} (1 âˆ’ r^2)^{-1/2}$, where the bias towards higher masking rates makes the prediction problem harder.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Noting that this masking functions on input rather than attention layers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this way, the base model is trained to predict all masked tokens at once.&lt;/p&gt;
&lt;h3 id=&#34;33-super-resolution-model&#34; class=&#34;icon-inline&#34; id=&#34;33-super-resolution-model&#34;&gt;3.3 Super-Resolution Model&lt;a class=&#34;icon-link&#34; href=&#34;#33-super-resolution-model&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027142223.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The super-resolution model of Muse&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the high-resolution generation, the authors found that directly predicting $512\times 512$ resolution leads the model to focus on low-level details over large-scale semantics. To this end, they trained another decoder to predict masked tokens in higher resolution with the help of low-res conditioning and text conditioning.&lt;/p&gt;
&lt;h3 id=&#34;34-fine-tune&#34; class=&#34;icon-inline&#34; id=&#34;34-fine-tune&#34;&gt;3.4 Fine-tune&lt;a class=&#34;icon-link&#34; href=&#34;#34-fine-tune&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Following the Parti model, Muse increases the capacity of the VQGAN decoder by the addition of more residual layers and channels, and then fine-tune the new decoder layers while other modules frozen.&lt;/p&gt;
&lt;h3 id=&#34;35-iterative-parallel-decoding&#34; class=&#34;icon-inline&#34; id=&#34;35-iterative-parallel-decoding&#34;&gt;3.5 Iterative Parallel Decoding&lt;a class=&#34;icon-link&#34; href=&#34;#35-iterative-parallel-decoding&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027153559.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Inference samples in Muse&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The above masked learning support us to decode multiple tokens at each step, so this inference is called as iterative parallel decoding. Based on a cosine schedule, we predict all masked tokens at each step, and choose a fraction of the highest confidence tokens as unmasked, and continue next step to predict remaining masked tokens.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231027155432.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Per-batch inference time for several models.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Using this procedure, Muse is able to perform high-fidelity inference using only 24 steps in base model and 8 steps in super-resolution model, and is significantly faster than competing diffusion or other autoregressive models.&lt;/p&gt;
</description>
    </item>
    </channel>
</rss>
