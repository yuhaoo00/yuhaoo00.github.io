<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ScoreModel on Mem.Capsule ðŸ’Š</title>
    <link>https://yuhaoo00.github.io/tags/scoremodel/</link>
    <description>Recent content in ScoreModel on Mem.Capsule ðŸ’Š</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <lastBuildDate>Tue, 20 Dec 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yuhaoo00.github.io/tags/scoremodel/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inverse Problem Ã— Diffusion -- Part: B</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/</guid>
      <description>&lt;h2 id=&#34;ddrm&#34; class=&#34;icon-inline&#34; id=&#34;ddrm&#34;&gt;DDRM&lt;a class=&#34;icon-link&#34; href=&#34;#ddrm&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2201.11793&#34;&gt;-&amp;gt; Bahjat Kawar, et al. NeurIPS, 2022.&lt;/a&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018160625.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Illustration of DDRM (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;transformation-via-svd&#34; class=&#34;icon-inline&#34; id=&#34;transformation-via-svd&#34;&gt;Transformation via SVD&lt;a class=&#34;icon-link&#34; href=&#34;#transformation-via-svd&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Similar to &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2021/hash/b5c01503041b70d41d80e3dbe31bbd8c-Abstract.html&#34;&gt;SNIPS&lt;/a&gt;, DDRM consider the singular value decomposition (SVD) of the sampling matrix $H$ as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
y&amp;amp;=Hx+z\
y&amp;amp;=U\Sigma V^\top x+z\
\Sigma^{â€ } U^{\top}y&amp;amp;=V^\top x+\Sigma^{â€ } U^{\top}z\
\bar{y}&amp;amp;=\bar{x}+\bar{z}\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Since $U$ is orthogonal matrix, we have $p(U^\top z) = p(z) = \mathcal{N}(0,\sigma^2_y I)$, resulting $\bar{z}^{(i)}=(\Sigma^{â€ } U^{\top}z)^{(i)} \sim \mathcal{N}(0, \frac{\sigma^2_y}{s_i^2}I)$. So after these, we transform $x$ and $y$ into the same field (&lt;strong&gt;spectral space&lt;/strong&gt;), and these two only differ by the noise $\bar{z}$, which can be drawn as follows:&lt;/p&gt;
&lt;p&gt;$$
q(\bar{y}^{(i)}|x_0)=\mathcal{N}(\bar{x}_0^{(i)},\sigma_y^2/s_i^2 )
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DDRM might be unable to cope with the wild scene, cause the variance $\sigma_y$ is often unknown.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;conditional-diffusion--generation&#34; class=&#34;icon-inline&#34; id=&#34;conditional-diffusion--generation&#34;&gt;Conditional Diffusion &amp;amp; Generation&lt;a class=&#34;icon-link&#34; href=&#34;#conditional-diffusion--generation&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The desired conditional diffusion process should (a) be a tractable Gaussian distribution, (b) employ the known $y$ to construct noisy samples as possible and (c) ensure the original marginal:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
q(x_t|x_0) = q(\bar{x}_t|x_0, y)\cdot q(\bar{y}|x_0)=\mathcal{N}(\bar{x}_0,\sigma_t^2I) \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We note that $q(\bar{y}^{(i)}|x_0)=\mathcal{N}(\bar{x}_0^{(i)},\sigma_y^2/s_i^2 )$, so these conditional processes can be defined as follows:
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018141254.png&#34; alt=&#34;image.png&#34;&gt;
From the perspective of DDIM, we can deduce the corresponding generative process via replacing $x_0$ with &amp;ldquo;the predicted version&amp;rdquo; as follows:
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018153006.png&#34; alt=&#34;image.png&#34;&gt;
Intuitively, this construction considers different cases for each index of the spectral space. (i) If the corresponding singular value $s_i=0$, then $y$ does not directly provide any information to that index, and the update is similar to regular unconditional generation. (ii) If $s_i &amp;gt;0$, then the updates consider the information provided by $y$, which further depends on whether the measurementsâ€™ noise level $\sigma_y/s_i$ in the spectral space is larger than the noise level in the diffusion model or not.&lt;/p&gt;
&lt;p&gt;In the resulting generation, the initial sample carries a few information from $\bar{y}$, then is updated eventually by the guidance of $p_\theta(\bar{x}_t|x_t+1,y)$,  and is recovered to $x_0$ exactly by left multiplying $V$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although this conditional process results in a more complex ELBO objective for training, the authors proof that an optimal solution to DDPM / DDIM can also be an optimal solution to a DDRM problem, under some similar assumptions as in DDIM. So &lt;strong&gt;DDRM can be training-free&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;one-more-thing&#34; class=&#34;icon-inline&#34; id=&#34;one-more-thing&#34;&gt;One more thing&lt;a class=&#34;icon-link&#34; href=&#34;#one-more-thing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;DDRM is applied in super-resolution, deblurring, inpainting, and colorization. There&amp;rsquo;re no much difference of the value space between original $x$ and degraded $y$ in these task. These data are almost all in the perceptible pixel space. So does SVD really work as claimed? This paper lacks relevant ablation study for that. Maybe we should introduce this into more tasks, such as compressive sensing, and see what will happen.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Kawar, Bahjat, Gregory Vaksman, and Michael Elad. &amp;ldquo;SNIPS: Solving noisy inverse problems stochastically.&amp;rdquo;Â &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;Â 34 (2021): 21757-21769.&lt;/li&gt;
&lt;li&gt;Kawar, Bahjat, et al. &amp;ldquo;Denoising diffusion restoration models.&amp;rdquo;Â &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;Â 35 (2022): 23593-23606.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Inverse Problem Ã— Diffusion -- Part: A</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/</link>
      <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;An inverse problem seeks to recover an unknown signal from a set of observed measurements. Specifically, suppose $x\in R^n$ is an unknown signal, and $y\in R^m = Ax+z$ is a noisy observation given by m linear measurements, where the measurement acquisition process is represented by a linear operator $A\in R^{m\times n}$, and $z\in R^n$ represents a noise vector. Solving a linear inverse problem amounts to recovering the signal $x$ from its measurement $y$. Without further assumptions, the problem is ill-defined when $m&amp;lt; n$, so we additionally assume that $x$ is sampled from a prior distribution $p(x)$&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;source from &lt;a href=&#34;http://arxiv.org/abs/2111.08005&#34;&gt;Dr. Yang Song&amp;rsquo;s paper&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As unconditional generative model, Diffusion models (or Score models) can play a prior term in optimization for various ill-posed image inverse problems. Such methods are often labeled as training-free, zero-shot, unsupervised, etc.&lt;/p&gt;
&lt;p&gt;On the other hand, Diffusion models also can be trained as conditional one $s_\theta(x_t,y,t)$, where $y$ is the condition from other mode. However, this will consume a lot of resources both in the computing power and the collection of paired data $\lbrace x_i,y_i\rbrace$.&lt;/p&gt;
&lt;h2 id=&#34;medical-score-sde&#34; class=&#34;icon-inline&#34; id=&#34;medical-score-sde&#34;&gt;Medical Score-SDE&lt;a class=&#34;icon-link&#34; href=&#34;#medical-score-sde&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2111.08005&#34;&gt;-&amp;gt; Yang Song, et al. in ICLR, 2022.&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017172654.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The medical imaging problem (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;step1-perturbing-measurements&#34; class=&#34;icon-inline&#34; id=&#34;step1-perturbing-measurements&#34;&gt;step1: Perturbing measurements&lt;a class=&#34;icon-link&#34; href=&#34;#step1-perturbing-measurements&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Since $x_t=\alpha(t)x_0+\beta(t)z$, and we set $y_t=Ax_t+\alpha(t)\epsilon$, so we have $y_t=\alpha(t)y+\beta(t)Az$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Smart and reasonable assumption for the coefficient $+\alpha(t)\epsilon$ to avoid the subsequent diffusion of random measurement noise $\epsilon$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;step2-generation-with-score-sde&#34; class=&#34;icon-inline&#34; id=&#34;step2-generation-with-score-sde&#34;&gt;step2: Generation with Score-SDE&lt;a class=&#34;icon-link&#34; href=&#34;#step2-generation-with-score-sde&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Using the Euler-Maruyama sampler, the generative process given by:&lt;/p&gt;
&lt;p&gt;$$
x_{t-\Delta t} = x_t-f(t)x_t\Delta t+g(t)^2s_\theta(x_t,t)\Delta t+g(t)\sqrt{\Delta t}z
$$&lt;/p&gt;
&lt;p&gt;where $s_\theta(\cdot)$ is the trained score model, $\Delta t = 1/N$ is the set discrete time, and $z\sim \mathcal{N}(0,1)$.&lt;/p&gt;
&lt;h3 id=&#34;step3-consistent-guidance&#34; class=&#34;icon-inline&#34; id=&#34;step3-consistent-guidance&#34;&gt;step3: Consistent Guidance&lt;a class=&#34;icon-link&#34; href=&#34;#step3-consistent-guidance&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017164345.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The pipeline of medical Score-SDE&lt;/figcaption&gt;
&lt;/figure&gt;

We set the guided intermediate result as $x_t^{\prime}$, For simultaneously minimizing the distance between $x_t$ and $x_t^{\prime}$, and the distance between $x_t^{\prime}$ and the hyperplane $\lbrace x\in R^n| Ax=y_t \rbrace$, we build an  optimization problem as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x_t^{\prime}&amp;amp;=\arg \min_{v\in R^n}\lbrace (1-\lambda)\Vert v-x_t \Vert_{T}^2 + \min_{u\in R^n} \lambda \Vert v-u \Vert_{T}^2 \rbrace \
s.t.\quad Au&amp;amp;=y_t
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which has the closed-form solution as:&lt;/p&gt;
&lt;p&gt;$$
x_t^{\prime}=T^{-1}[\lambda \Lambda \mathcal{P} ^{-1}(\Lambda)y_t+(1-\lambda)\Lambda Tx_t+(1-\Lambda)Tx_t]
$$&lt;/p&gt;
&lt;p&gt;When the measurement process is noisy, we can choose $0&amp;lt;\lambda&amp;lt;1$ to allow slackness in $Ax_t^{\prime}=y_t$ (more affected by $p(x)$). When the measurement process contains no noise, the authors chosse $\lambda=1$ at the last sampling step to guarantee $Ax_0^{\prime}=y$.&lt;/p&gt;
&lt;h2 id=&#34;score-mri&#34; class=&#34;icon-inline&#34; id=&#34;score-mri&#34;&gt;Score-MRI&lt;a class=&#34;icon-link&#34; href=&#34;#score-mri&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1361841522001268&#34;&gt;-&amp;gt; Hyungjin Chung, et al.  Medical Image Analysis, 2022.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Similar to the above, Score-MRI inserts a more simple consistency mapping after every &amp;ldquo;Predictor&amp;rdquo; and &amp;ldquo;Corrector&amp;rdquo; iteration.&lt;/p&gt;
&lt;p&gt;$$
x_i=x_i+\lambda A^\ast(y-Ax_i)=(I-\lambda A^\ast A)x_i+A^\ast y
$$&lt;/p&gt;
&lt;p&gt;where $A^\ast$ denotes the Hermitian adjoint, and $+\lambda A^\ast (y-Ax_i)$ can be viewed as a rectification for minimizing the distance between $y$ and $Ax_i$. In the hybridtype Algorithm 5 (complex domain and multi-coils for MRI), the authors start with $\lambda = 1.0$ in the first iteration, and linearly decrease the value to $\lambda=0.2$ at the last iteration.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1706.00051&#34;&gt;GANCS&lt;/a&gt; has an similar rectification $(I-\Phi^{â€ }\Phi)x+\Phi^{â€ }y$, where pseudo-inverse $\Phi^{â€ }$ satisfies $\Phi\Phi^{â€ }\Phi=\Phi$, but without the scaling factor $\lambda$.  In this paper, the authors view $(I-\Phi^{â€ }\Phi)x$ as a projection onto the &lt;a href=&#34;https://dx.doi.org/10.1088/1361-6420/aaf14a&#34;&gt;nullspace&lt;/a&gt; of $\Phi$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017182641.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;ddnm&#34; class=&#34;icon-inline&#34; id=&#34;ddnm&#34;&gt;DDNM&lt;a class=&#34;icon-link&#34; href=&#34;#ddnm&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2212.00490&#34;&gt;-&amp;gt; Yinhuai Wang, et al. in ICLR, 2023.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;noise-free&#34; class=&#34;icon-inline&#34; id=&#34;noise-free&#34;&gt;Noise-free&lt;a class=&#34;icon-link&#34; href=&#34;#noise-free&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Similar to &lt;a href=&#34;http://arxiv.org/abs/1706.00051&#34;&gt;GANCS&lt;/a&gt;, DDNM adopts a rectified estimation as:&lt;/p&gt;
&lt;p&gt;$$
x_{0|t}^{\prime}=A^{â€ }y+(I-A^{â€ }A)x_{0|t}
$$&lt;/p&gt;
&lt;p&gt;where $x_{0|t}$ is the predicted version at timestep $t$. Based on &lt;a href=&#34;http://arxiv.org/abs/2010.02502&#34;&gt;DDIM&lt;/a&gt;, the generative process can be driven by $x_{t-1}\sim \mathcal{p}(x_{t-1}|x_t,x_{0|t}^{\prime})$. So it might provide a more reliable rectification to human-perceptible $x_{0|t}$, rather than noisy $x_t$ like in &lt;a href=&#34;http://arxiv.org/abs/2010.02502&#34;&gt;Score-MRI&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although this paper mentioned that this consistency mapping was inspired by &lt;strong&gt;range-null space decomposition&lt;/strong&gt; $x=A^{â€ }Ax+(I-A^{â€ }A)x$, but it seems to contradict with the subsequent DDNM+ for the noisy inverse problem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;noisy&#34; class=&#34;icon-inline&#34; id=&#34;noisy&#34;&gt;Noisy&lt;a class=&#34;icon-link&#34; href=&#34;#noisy&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$$
x_{0|t}^{\prime}=A^â€ y+(Iâˆ’A^â€ A)x_{0|t} =x_{0|t}-A^â€ (Ax_{0|t}-y)
$$&lt;/p&gt;
&lt;p&gt;In short, the authors employ two scale factors $\Sigma_t$ &amp;amp; $\Phi_t$ into range-space correction &amp;amp; generative variance, respectively.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x_{0|t}^{\prime}&amp;amp;= x_{0|t}-\Sigma_tA^â€ (Ax_{0|t}-y)\\
p^{\prime}(x_{t-1}|x_t,x_{0|t}^{\prime})&amp;amp;=\mathcal{N}(x_{t-1};\mu_t(x_t, x_{0|t}^{\prime}),\Phi_t I)
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the performance gains from these factor are not clear, due to the absence of ablations experiments. It looks like the &amp;ldquo;Time-Travel&amp;rdquo; (a kind of redundant computing) helped a lot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017212649.png&#34; loading=&#34;lazy&#34; width=&#34;300&#34;&gt;
&lt;figcaption&gt;
Time-Travel (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yang Song and others, â€˜Solving Inverse Problems in Medical Imaging with Score-Based Generative Modelsâ€™  in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2022.&lt;/li&gt;
&lt;li&gt;Hyungjin Chung and Jong Chul Ye, â€˜Score-Based Diffusion Models for Accelerated MRIâ€™, &lt;em&gt;Medical Image Analysis&lt;/em&gt;, 80 (2022), 102479.&lt;/li&gt;
&lt;li&gt;Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, â€˜Deep Null Space Learning for Inverse Problems: Convergence Analysis and Ratesâ€™, &lt;em&gt;Inverse Problems&lt;/em&gt;, 35.2 (2019), 025008.&lt;/li&gt;
&lt;li&gt;Morteza Mardani and others, â€˜Deep Generative Adversarial Networks for Compressed Sensing Automates MRIâ€™ arXiv, 2017.&lt;/li&gt;
&lt;li&gt;Yinhuai Wang, Jiwen Yu, and Jian Zhang, â€˜Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Modelâ€™ in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2023.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Image Generation based on Score Model</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212ncsn/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212ncsn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function $\nabla_{ x} \log p_{\text {data }}( x)$) may be a better choice than learning the data density directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;score-estimation-for-training&#34; class=&#34;icon-inline&#34; id=&#34;score-estimation-for-training&#34;&gt;Score Estimation (for training)&lt;a class=&#34;icon-link&#34; href=&#34;#score-estimation-for-training&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We want to train a network $s_{\theta}(x)$ to estimate $\nabla_{ x} \log p_{\text {data }}( x)$, but how can we get the ground truth (the real score)? In this paper, the objective $\frac{1}{2} E_{p_{\text{data}}} \lbrack\lVert s_{\theta}( x)-\nabla_{ x} \log p_{\text{data}}( x)\rVert_2^2\rbrack$ is equivalent to the following by Score Matching:&lt;/p&gt;
&lt;p&gt;$$
E_{p_{\text{data}}}\left[tr(\nabla_{ x}s_{\theta}( x))+\frac{1}{2}\left|s_{\theta}( x))\right|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;Unfortunately, it is not easy to compute $tr(\cdot)$ for a large-scale problem. Both &lt;strong&gt;Denoising score matching&lt;/strong&gt; and &lt;strong&gt;Sliced score matching&lt;/strong&gt; are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q_\sigma$) as follows:&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{2} E_{q_\sigma(\tilde{ x} \mid  x) p_{\text {data }}( x)}\left[\left| s_{\theta}(\tilde{ x})-\nabla_{\tilde{ x}} \log q_\sigma(\tilde{ x} \mid  x)\right|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;Minimizing this objective, we can get $s_{\theta}( x)=\nabla_{ x} \log q_{\sigma}( x)$. And if the noise is small enough $q_\sigma( x) \approx p_{\text {data }}( x)$, we will get $s_{\theta}( x)=\nabla_{ x} \log q_\sigma( x) \approx \nabla_{ x} \log p_{\text {data }}( x)$.
As $q_\sigma(\tilde{x}| x)$ can be defined as a known simple distribution, this minimization is easier than regular score matching. In this paper, they choose $q_\sigma(\tilde{x}| x) =N\left(\tilde{x} \mid x, \sigma^2 I\right)$, which leads to:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta ; \sigma) \triangleq \frac{1}{2} E_{p_{\text {data }}(x)} E_{\tilde{x} \sim N\left(x, \sigma^2 I\right)}\left[\left|\mathbf{s}_{\theta}(\tilde{x}, \sigma)+\frac{\tilde{x}-x}{\sigma^2}\right|_2^2\right]
$$&lt;/p&gt;
&lt;h2 id=&#34;langevin-dynamics-for-inference&#34; class=&#34;icon-inline&#34; id=&#34;langevin-dynamics-for-inference&#34;&gt;Langevin dynamics (for inference)&lt;a class=&#34;icon-link&#34; href=&#34;#langevin-dynamics-for-inference&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;How can we do sampling from $p_{\text {data }}( x)$ when we get a nice estimation of $\nabla_{ x} \log p_{\text {data }}( x)$? Just along the direction of this gradient? Yes, but plus more tricks.&lt;/p&gt;
&lt;p&gt;$$
x_t = x_{t-1}+\frac{\epsilon}{2} \nabla_{x} \log p\left(x_{t-1}\right)+\sqrt{\epsilon} z_t
$$&lt;/p&gt;
&lt;p&gt;The Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide &lt;em&gt;more stable distribution&lt;/em&gt;! Briefly, the random noise term $\mathbf{z}_t \thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\epsilon \to 0$), the iterative $x_t$ will approach the distribution $p(x)$.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012150556.png&#34; loading=&#34;lazy&#34; width=&#34;400&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;practical-challenges&#34; class=&#34;icon-inline&#34; id=&#34;practical-challenges&#34;&gt;Practical Challenges&lt;a class=&#34;icon-link&#34; href=&#34;#practical-challenges&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;the-manifold-hypothesis&#34; class=&#34;icon-inline&#34; id=&#34;the-manifold-hypothesis&#34;&gt;The manifold hypothesis&lt;a class=&#34;icon-link&#34; href=&#34;#the-manifold-hypothesis&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The score is &lt;em&gt;undefined&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The estimation by Score Matching &lt;em&gt;isn&amp;rsquo;t consistent&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data&lt;/strong&gt; with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast &amp;amp; faithful) will converge (Fig. 1).&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192631.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Figure 1: Left: Sliced score matching (SSM) loss w.r.t. iterations. No noise is added to data. Right: Same but data are perturbed with N(0, 0.0001).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;low-data-density-regions&#34; class=&#34;icon-inline&#34; id=&#34;low-data-density-regions&#34;&gt;Low data density regions&lt;a class=&#34;icon-link&#34; href=&#34;#low-data-density-regions&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Our training is based on the data in high density ($\thicksim p_{\text {data }}( x)$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The estimation in low density regions is &lt;em&gt;inaccurate.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Regular Langevin Dynamics &lt;em&gt;can&amp;rsquo;t be able to correctly recover&lt;/em&gt; the relative weights of the multi-modal distribution &lt;em&gt;in reasonable time.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data by using multiple noise levels&lt;/strong&gt; (anneal down), we can fill the low density regions.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192901.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;So naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions.
(Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)&lt;/p&gt;
&lt;h2 id=&#34;one-more-thing&#34; class=&#34;icon-inline&#34; id=&#34;one-more-thing&#34;&gt;One More Thing&lt;a class=&#34;icon-link&#34; href=&#34;#one-more-thing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the score matching, the authors approximately have $\left| s_{\theta}( x, \sigma)\right|_2 \propto 1 / \sigma$, so they choose $\lambda (\sigma) = \sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\sigma$.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}\left(\theta ;\lbrace\sigma_i \rbrace_{i=1}^{L}\right) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda\left(\sigma_i\right) \ell\left(\theta ; \sigma_i\right)
$$&lt;/p&gt;
&lt;p&gt;Correspondingly, in the langevin dynamics, they choose $\alpha_i \propto \sigma^2$ to make the order magnitude of &amp;ldquo;signal-to-noise ratio&amp;rdquo; independent of $\sigma$.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yang Song and Stefano Ermon, â€˜Generative Modeling by Estimating Gradients of the Data Distributionâ€™, in &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2019.&lt;/li&gt;
&lt;li&gt;Yang Song and others, â€˜Score-Based Generative Modeling through Stochastic Differential Equationsâ€™, in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2021.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    </channel>
</rss>
