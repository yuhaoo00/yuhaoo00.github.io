<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Diffusion on Mem.Capsule</title>
        <link>https://yuhaoo00.github.io/tags/diffusion/</link>
        <description>Recent content in Diffusion on Mem.Capsule</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 18 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yuhaoo00.github.io/tags/diffusion/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>On Distillation of Guided Diffusion Models</title>
        <link>https://yuhaoo00.github.io/p/on-distillation-of-guided-diffusion-models/</link>
        <pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/on-distillation-of-guided-diffusion-models/</guid>
        <description></description>
        </item>
        <item>
        <title>Adding Conditional Control to Text-to-Image Diffusion Models</title>
        <link>https://yuhaoo00.github.io/p/adding-conditional-control-to-text-to-image-diffusion-models/</link>
        <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/adding-conditional-control-to-text-to-image-diffusion-models/</guid>
        <description></description>
        </item>
        <item>
        <title>DPM-Solver&#39; A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</title>
        <link>https://yuhaoo00.github.io/p/dpm-solver-a-fast-ode-solver-for-diffusion-probabilistic-model-sampling-in-around-10-steps/</link>
        <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/dpm-solver-a-fast-ode-solver-for-diffusion-probabilistic-model-sampling-in-around-10-steps/</guid>
        <description></description>
        </item>
        <item>
        <title>Score-based diffusion models for accelerated MRI</title>
        <link>https://yuhaoo00.github.io/p/score-based-diffusion-models-for-accelerated-mri/</link>
        <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/score-based-diffusion-models-for-accelerated-mri/</guid>
        <description></description>
        </item>
        <item>
        <title>Score-Based Generative Modeling through Stochastic Differential Equations</title>
        <link>https://yuhaoo00.github.io/p/score-based-generative-modeling-through-stochastic-differential-equations/</link>
        <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/score-based-generative-modeling-through-stochastic-differential-equations/</guid>
        <description></description>
        </item>
        <item>
        <title>Video Diffusion Models</title>
        <link>https://yuhaoo00.github.io/p/video-diffusion-models/</link>
        <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/video-diffusion-models/</guid>
        <description></description>
        </item>
        <item>
        <title>Improving Diffusion Models for Inverse Problems using Manifold Constraints</title>
        <link>https://yuhaoo00.github.io/p/improving-diffusion-models-for-inverse-problems-using-manifold-constraints/</link>
        <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/improving-diffusion-models-for-inverse-problems-using-manifold-constraints/</guid>
        <description></description>
        </item>
        <item>
        <title>Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model</title>
        <link>https://yuhaoo00.github.io/p/zero-shot-image-restoration-using-denoising-diffusion-null-space-model/</link>
        <pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/zero-shot-image-restoration-using-denoising-diffusion-null-space-model/</guid>
        <description></description>
        </item>
        <item>
        <title>Denoising Diffusion Probabilistic Models</title>
        <link>https://yuhaoo00.github.io/p/denoising-diffusion-probabilistic-models/</link>
        <pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/denoising-diffusion-probabilistic-models/</guid>
        <description>&lt;p&gt;Denoising Diffusion Probabilistic Models
Denoising Diffusion Implicit Models
Improved Denoising Diffusion Probabilistic Models
High-Resolution Image Synthesis With Latent Diffusion Models&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Come-Closer-Diffuse-Faster&#39; Accelerating Conditional Diffusion Models for Inverse Problems Through Stochastic Contraction</title>
        <link>https://yuhaoo00.github.io/p/come-closer-diffuse-faster-accelerating-conditional-diffusion-models-for-inverse-problems-through-stochastic-contraction/</link>
        <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/come-closer-diffuse-faster-accelerating-conditional-diffusion-models-for-inverse-problems-through-stochastic-contraction/</guid>
        <description></description>
        </item>
        <item>
        <title>Generative Modeling by Estimating Gradients of the Data Distribution</title>
        <link>https://yuhaoo00.github.io/p/generative-modeling-by-estimating-gradients-of-the-data-distribution/</link>
        <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://yuhaoo00.github.io/p/generative-modeling-by-estimating-gradients-of-the-data-distribution/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function) may be a better choice than learning the data density directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;score-estimation-for-training&#34;&gt;Score Estimation (for training)&lt;/h2&gt;
&lt;p&gt;We want to train a network $s_{\theta}(x)$ to estimate $\nabla_{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})$, but how can we get the ground truth (real score)? In this paper, the objective $\frac{1}{2} \mathbb{E}&lt;em&gt;{p&lt;/em&gt;{\text {data }}}\left[\left|\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\mathbf{x})-\nabla&lt;/em&gt;{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})\right|&lt;em&gt;2^2\right]$ is equivalent to the following by Score Matching:
$$
\mathbb{E}&lt;/em&gt;{p_{\text {data }}}\left[tr(\nabla_{\mathbf{x}}\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\mathbf{x}))+\frac{1}{2}\left|\mathbf{s}&lt;/em&gt;{\boldsymbol{\theta}}(\mathbf{x}))\right|&lt;em&gt;2^2\right]
$$
Unfortunately, it is not easy to compute $tr(\cdot)$ for a large-scale problem. Both &lt;strong&gt;Denoising score matching&lt;/strong&gt; and &lt;strong&gt;Sliced score matching&lt;/strong&gt; are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q&lt;/em&gt;\sigma$) as follows:
$$\frac{1}{2} \mathbb{E}&lt;em&gt;{q&lt;/em&gt;\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) p_{\text {data }}(\mathbf{x})}\left[\left|\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\tilde{\mathbf{x}})-\nabla&lt;/em&gt;{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x})\right|&lt;em&gt;2^2\right]$$
As $q&lt;/em&gt;\sigma(\tilde{\mathbf{x}}|\mathbf{x})$ can be defined as a known simple distribution (e.g. gaussian), this minimization is easier than estimating the original one. And we can get $\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\mathbf{x})=\nabla&lt;/em&gt;{\mathbf{x}} \log q_{\sigma}(\mathbf{x})$. So, when the noise is small enough ($q_\sigma(\mathbf{x}) \approx p_{\text {data }}(\mathbf{x})$), we will get $\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\mathbf{x})=\nabla&lt;/em&gt;{\mathbf{x}} \log q_\sigma(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})$.&lt;/p&gt;
&lt;h2 id=&#34;langevin-dynamics-for-inference&#34;&gt;Langevin dynamics (for inference)&lt;/h2&gt;
&lt;p&gt;How can we do sampling from $p_{\text {data }}(\mathbf{x})$ when we get a nice estimation of $\nabla_{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})$? Just along the direction of this gradient? Yes, but plus more tricks.
$$\tilde{\mathbf{x}}&lt;em&gt;t=\tilde{\mathbf{x}}&lt;/em&gt;{t-1}+\frac{\epsilon}{2} \nabla_{\mathbf{x}} \log p\left(\tilde{\mathbf{x}}_{t-1}\right)+\sqrt{\epsilon} \mathbf{z}_t$$
The Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide &lt;em&gt;more stable distribution&lt;/em&gt;! Briefly, the random noise term $\mathbf{z}_t \thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\epsilon \to 0$), the iterative $x_t$ will approach the distribution $p(x)$.&lt;/p&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012150556.png&#34; width=&#34;60%&#34; height=&#34;auto&#34;&gt;
&lt;h2 id=&#34;practical-challenges&#34;&gt;Practical Challenges&lt;/h2&gt;
&lt;h3 id=&#34;the-manifold-hypothesis&#34;&gt;The manifold hypothesis&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The score is &lt;em&gt;undefined&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The estimation by Score Matching &lt;em&gt;isn&amp;rsquo;t consistent&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data&lt;/strong&gt; with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast &amp;amp; faithful) will converge (Fig. 1).&lt;/p&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012145622.png&#34; width=&#34;60%&#34; height=&#34;auto&#34;&gt;
&lt;h3 id=&#34;low-data-density-regions&#34;&gt;Low data density regions&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Our training is based on the data in high density ($\thicksim p_{\text {data }}(\mathbf{x})$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The estimation in low density regions is &lt;em&gt;inaccurate.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Regular Langevin Dynamics &lt;em&gt;can&amp;rsquo;t be able to correctly recover&lt;/em&gt; the relative weights of the multi-modal distribution &lt;em&gt;in reasonable time.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data by using multiple noise levels&lt;/strong&gt; (anneal down), we can fill the low density regions.&lt;/p&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012145702.png&#34; width=&#34;60%&#34; height=&#34;auto&#34;&gt;
&lt;p&gt;So naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions.
(Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)&lt;/p&gt;
&lt;h2 id=&#34;one-more-thing&#34;&gt;One More Thing&lt;/h2&gt;
&lt;p&gt;In the score matching, the authors approximately have $\left|\mathbf{s}&lt;em&gt;{\boldsymbol{\theta}}(\mathbf{x}, \sigma)\right|&lt;em&gt;2 \propto 1 / \sigma$, so they choose $\lambda (\sigma) = \sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\sigma$.
$$\mathcal{L}\left(\boldsymbol{\theta} ;\left{\sigma_i\right}&lt;/em&gt;{i=1}^L\right) \triangleq \frac{1}{L} \sum&lt;/em&gt;{i=1}^L \lambda\left(\sigma_i\right) \ell\left(\boldsymbol{\theta} ; \sigma_i\right)$$
Correspondingly, in the langevin dynamics, they choose $\alpha_i \propto \sigma^2$ to make the order magnitude of &amp;ldquo;signal-to-noise ratio&amp;rdquo; independent of $\sigma$.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
