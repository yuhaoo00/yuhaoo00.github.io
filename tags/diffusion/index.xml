<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion on Yu&#39;s MemoCapsule</title>
    <link>https://yuhaoo00.github.io/tags/diffusion/</link>
    <description>Recent content in Diffusion on Yu&#39;s MemoCapsule</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <lastBuildDate>Thu, 17 Aug 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yuhaoo00.github.io/tags/diffusion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Controllable Text-To-Image Diffusion Models —— Explicit Control</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2308controlgenerationa/</guid>
      <description>&lt;p&gt;Controllable Text-To-Image (T2I) generation has always been a major challenge in diffusion models. On the one hand, people hope that the generated images can follow some predefined physical attributes, such as the number, position, size, and texture of objects. On the other hand, they also require the T2I models to retain a certain level of creativity.&lt;/p&gt;
&lt;p&gt;At present, there are quite a lot of researches related to controllable T2I generation. I prefer to divide them into two categories: one primarily focuses on correcting the generation path in inference, called Explicit Control; the other one strengthens the network through fine-tuning or adding new layers, called Implicit Control.&lt;/p&gt;
&lt;p&gt;This blog, as the first part of this series, will summarize representative researches related to Explicit Control, which can be further divided into two technical routes: forward guidance and backward guidance. The discussion of implicit control will be updated in another blog post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PS: This explicit or implicit statement comes from my personal preference and is not yet widely accepted in the academic community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;early-explorations-before-the-stable-diffusion&#34; class=&#34;icon-inline&#34; id=&#34;early-explorations-before-the-stable-diffusion&#34;&gt;Early Explorations before the Stable Diffusion&lt;a class=&#34;icon-link&#34; href=&#34;#early-explorations-before-the-stable-diffusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;image-to-image--inpainting&#34; class=&#34;icon-inline&#34; id=&#34;image-to-image--inpainting&#34;&gt;Image-to-Image &amp;amp; Inpainting&lt;a class=&#34;icon-link&#34; href=&#34;#image-to-image--inpainting&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2108.01073&#34;&gt;-&amp;gt; Chenlin Meng, et al. arXiv 2021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html&#34;&gt;-&amp;gt; Andreas Lugmayr, et al. CVPR 2022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The idea of explicit control can date back to before the Stable Diffusion, with two classic examples being SDEdit (Image-to-Image) and RePaint (Inpainting). Due to their relatively simple principles,  here we just briefly summarize them:&lt;/p&gt;
&lt;p&gt;SDEdit uses a noisy version of a reference image as the starting point for the denoising process, ensuring that the generated image maintains a similar color layout to the reference image. Repaint, during the denoising process, combines the masked background image with the predicted foreground image to realize inpainting new content in the masked regions.&lt;/p&gt;
&lt;p&gt;Both of these classic methods achieve controllable generation through modify denoising process (without any training), demonstrating the strong robustness of the diffusion model and the significant potential for modifying the denoising process manually!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the &lt;strong&gt;Diffusers&lt;/strong&gt; library, the &lt;em&gt;StableDiffusionInpaintPipeline&lt;/em&gt; has two versions. The unfinetuned version is based on the idea of Repaint without training, while the finetuned version involves concatenating the &lt;em&gt;Mask&lt;/em&gt; and &lt;em&gt;Masked_image_latents&lt;/em&gt; as additional 5 channels to the input of Unet and fine-tuning all layers. The latter version achieves better results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;blended-diffusion&#34; class=&#34;icon-inline&#34; id=&#34;blended-diffusion&#34;&gt;Blended Diffusion&lt;a class=&#34;icon-link&#34; href=&#34;#blended-diffusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html&#34;&gt;-&amp;gt; Omri Avrahami, et al. CVPR 2022&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/DFD12FE9-4E0D-436D-8767-86E51992E1B6_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
A simple diagram of the Blended Diffusion&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;With the introduction of CLIP technology, Blended Diffusion achieves text-driven inpainting. The basic approach can be summarized as above figure. In each iteration,  $\hat{x_0}$ is predicted based on the inverse equation of the known $p(x_t|x_0)$, and is masked accordingly. Then, the masked image and the text prompt are jointly input into CLIP to obtain a cosine similarity score $\mathcal{L}$ (with the trick of data augmentation to improve sensitivity). The gradient of this score is used to update the latent $\epsilon(x_t)+\nabla_{\hat{x_0}}\mathcal{L}$. Moreover, the foreground and background images are fused based on the given mask to update the latent $x_{t-1}$ for next iteration.&lt;/p&gt;
&lt;p&gt;This approach is essentially a combination of CLIP-based T2I and Repaint method. In fact, the CLIP-based generation is a kind of classifier-based guidance, where the iteration values are updated based on the gradient of a certain criterion. Although this gradient-based approach may sound a little old-school in the days of Classifier-Free Guidance (CFG), we can still see its effectiveness in serval cutting-edge studies later on.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;forward-guidance&#34; class=&#34;icon-inline&#34; id=&#34;forward-guidance&#34;&gt;Forward Guidance&lt;a class=&#34;icon-link&#34; href=&#34;#forward-guidance&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;prompt-to-prompt-image-editing-with-cross-attention&#34; class=&#34;icon-inline&#34; id=&#34;prompt-to-prompt-image-editing-with-cross-attention&#34;&gt;Prompt-to-Prompt Image Editing with Cross Attention&lt;a class=&#34;icon-link&#34; href=&#34;#prompt-to-prompt-image-editing-with-cross-attention&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2208.01626&#34;&gt;-&amp;gt; Amir Hertz, et al. arXiv 2022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Prompt-to-Prompt (P2P) focuses on editing already generated images, which may not be relevant to controllable generation, but it shows great potential of manipulating Cross Attention (CA) maps and brings quite a lot of inspiration to controllable generation techniques!&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106181510.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Cross-Attention visualization in Unet (from the paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As shown above, P2P first found that CA corresponding to text tokens had strong semantic information (even in the early stages of generation). This finding seems obvious when you realize that CA is the similarity between image features and text features. But no one had visualized them before, and such strong semantic connections were not well known!&lt;/p&gt;
&lt;p&gt;At the same time, this observation also reveals how the CA mechanism drives T2I generation. Let&amp;rsquo;s imagine the data flow: the sequence of text features from CLIP is first mapped to base values, then fused at different locations based on attention weights (the similarity between images and texts), and finally makes up the image we see (magical multimodal learning)!&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231107163401.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Image editing effect achieved by P2P (from the paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;More excitingly, the authors successfully controlled generation by manipulating CA values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replacing the CA corresponding to a text token for content replacement.&lt;/li&gt;
&lt;li&gt;Deleting the CA corresponding to a text token for content erasure.&lt;/li&gt;
&lt;li&gt;Adding the CA corresponding to a text token for content addition.&lt;/li&gt;
&lt;li&gt;Weighting the CA corresponding to a text token to effectively enhance or weaken content.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Now we can enhance or weaken the content by weighting the text features from CLIP directly. This &lt;strong&gt;&amp;ldquo;weighted prompt&amp;rdquo;&lt;/strong&gt; trick is widely integrated into various T2I tools (such as compel, A1111-webui, Midjourney, etc.).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;compositional-visual-generation-with-composable-diffusion-models&#34; class=&#34;icon-inline&#34; id=&#34;compositional-visual-generation-with-composable-diffusion-models&#34;&gt;Compositional Visual Generation with Composable Diffusion Models&lt;a class=&#34;icon-link&#34; href=&#34;#compositional-visual-generation-with-composable-diffusion-models&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2206.01714&#34;&gt;-&amp;gt; Nan Liu, et al. ECCV 2022&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/9F007937-5B9C-4D33-A13E-DEC5BC7F7683_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
A simple diagram of the Composed Diffusion&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Composed Diffusion supports the generation of more content or objects. Although this paper is inspired by composing multiple EBMs, this idea is equivalent to the advanced CFG formally. In short, multiple text prompts separated by the &amp;ldquo;AND&amp;rdquo; symbol are fed into the diffusion model separately and then weighted to produce the results:&lt;/p&gt;
&lt;p&gt;$$
\hat{\epsilon}(x_t,t)=\epsilon(x_t,t)+\sum_{i=1}^{n}w_i(\epsilon(x_t,t|c_i)-\epsilon(x_t,t))
$$&lt;/p&gt;
&lt;p&gt;This synthesis is a bit crude and often less than expected, because different content will merge globally without explicit location constraints. Therefore, Composed Diffusion is more suitable for those multi-content with large spatial differences to avoid unexpected conflicts.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231106202519.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Composing effect achieved by Composed Diffusion (from the paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;multidiffusion-fusing-diffusion-paths-for-controlled-image-generation&#34; class=&#34;icon-inline&#34; id=&#34;multidiffusion-fusing-diffusion-paths-for-controlled-image-generation&#34;&gt;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&lt;a class=&#34;icon-link&#34; href=&#34;#multidiffusion-fusing-diffusion-paths-for-controlled-image-generation&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2302.08113&#34;&gt;-&amp;gt; Omer Bar-Tal, et al. arXiv 2023&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/0172B6F3-2F6E-49E8-82EF-07F6EB37DEF0_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
A simple diagram of the MultiDiffusion&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If we want more control over spatial position, the most direct way is to draw something in a certain region by specifying a target mask or box. While this sounds like a multi-region inpainting task, the question is how to ensure that they are independent and harmonious with each other?&lt;/p&gt;
&lt;p&gt;MultiDiffusion gives the answer. The authors first build a mathematical optimization model for harmonious multi-region generation and derive a closed-form solution:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\Psi\left(J_t \mid z\right)=\arg\min_{J\in\mathcal{J}}\sum_{i=1}^n\left|W_i \otimes\left[F_i(J)-\Phi\left(I_t^i \mid y_i\right)\right]\right|^2 \\
\Psi\left(J_t \mid z\right)=\sum_{i=1}^n \frac{F_i^{-1}\left(W_i\right)}{\sum_{j=1}^n F_j^{-1}\left(W_j\right)} \otimes F_i^{-1}\left(\Phi\left(I_t^i \mid y_i\right)\right)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;In fact, MultiDiffusion can be understood as a multi-region extension of Repaint. It fuses multiple images generated from different regions (depending on the degree of overlap) in each step, and then eliminates dissonance at the boundaries solely by the robustness of the denoising process. Obviously, due to the lack of interaction between the central features of different regions, the final generated image has obvious style variations across regions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The derived fusion equation is intuitive and can be obtained even without establishing the optimization problem. But it does make the whole paper more academic&amp;hellip; -_-&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis&#34; class=&#34;icon-inline&#34; id=&#34;training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis&#34;&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;a class=&#34;icon-link&#34; href=&#34;#training-free-structured-diffusion-guidance-for-compositional-text-to-image-synthesis&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2212.05032&#34;&gt;-&amp;gt; Weixi Feng, et al. ICLR 2023&lt;/a&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/C5A36A3D-3958-4026-B90B-791D289DE33C_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
A simple diagram of the Structured Diffusion&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Similar to the goal of Composed Diffusion, Structured Diffusion seeks to synthesize more content through text input alone. The approach derives from two key observations: first, the Cross-Attention of the input text token is very semantic (also observed by P2P); Second, CLIP&amp;rsquo;s text encoder (based on Transformer) will inevitably incorporate contextual information for each token.&lt;/p&gt;
&lt;p&gt;Therefore, Structured Diffusion attempts to ensure that specific text is not affected by context so that the corresponding content is properly synthesized. They first strip all the noun phrases from a sentence using a parser like Constituency Tree or Scene Graph; Then input these noun phrases into the CLIP text encoder separately to avoid mutual influence; And fill in the rest with the features of the whole sentence $W_p$; Finally, in the Unet, these new text features $W_i$ will be mapped into new values $V_i$, and fused according to the old CA $M^t$:&lt;/p&gt;
&lt;p&gt;$$
O^t=\frac{1}{n}\sum_{i=1}^{n}(M^tV_i)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that attention is still computed from the original whole-sentence query, which contribute to the spatial layout allocation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus, we enhance the features of all noun phrases and reduce their interference with each other. Unfortunately, the actual improvement from this approach is not obvious.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;backward-guidance&#34; class=&#34;icon-inline&#34; id=&#34;backward-guidance&#34;&gt;Backward Guidance&lt;a class=&#34;icon-link&#34; href=&#34;#backward-guidance&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;training-free-layout-control-with-cross-attention-guidance&#34; class=&#34;icon-inline&#34; id=&#34;training-free-layout-control-with-cross-attention-guidance&#34;&gt;Training-Free Layout Control with Cross-Attention Guidance&lt;a class=&#34;icon-link&#34; href=&#34;#training-free-layout-control-with-cross-attention-guidance&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2304.03373&#34;&gt;-&amp;gt; Minghao Chen, et al. arXiv 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper attempts to implement control generation for multiple regions with a single diffusion model, called layout control. Interestingly, the authors explore two different methods of guidance: 1. a region constraint imposed in the generation iteration, also known as forward guidance; 2. a gradient-based update idea similar to Blended Diffusion, also known as backward guidance.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/98AB3A71-A285-471A-A6AD-ADF1A157F1AF_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
A simple diagram of the Layout Control&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In particular, the forward guidance will modify the Cross-Attention of the specified text tokens (in each layer) to cluster them into the specified box:&lt;/p&gt;
&lt;p&gt;$$
\hat{A_{p,i}}=(1-\lambda)A_{p,i}+\lambda g_p\sum_{j}A_{j,i}
$$&lt;/p&gt;
&lt;p&gt;Where $p$ is the position coordinate, $i$ is the index of the text token, $g_p$ is the gaussian weight in a specified box (sum of 1), and $\sum_{j}A_{j,i}$ is the sum of the entire CA Map. It means that the latter term in the equation will redistribute the attention weights according to the gaussian box.&lt;/p&gt;
&lt;p&gt;Backward guidance defines a function to measure the CA concentration of a given token over a given box $B$:&lt;/p&gt;
&lt;p&gt;$$
E(A,B,i)=\left(1-\frac{\sum_{p\in B}A_{p,i}}{\sum_{p}A_{p,i}}\right)^2
$$&lt;/p&gt;
&lt;p&gt;We update the latent $x_t$ at the current time step with the gradient of this metric as shown in the figure.&lt;/p&gt;
&lt;p&gt;According to the experimental results, the backward guidance is more effective (and of course consumes more running time). In addition, the author also integrates this method with Dreambooth and Text Inversion, which still achieve a good control effect.&lt;/p&gt;
&lt;h3 id=&#34;attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models&#34; class=&#34;icon-inline&#34; id=&#34;attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models&#34;&gt;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&lt;a class=&#34;icon-link&#34; href=&#34;#attend-and-excite-attention-based-semantic-guidance-for-text-to-image-diffusion-models&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3592116&#34;&gt;-&amp;gt; Hila Chefer et al. ACM Trans. Graph. 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Similar to the observations in P2P, this paper assumes that if something is not generated as expected, the corresponding Cross-Attention should be increased. The difference, however, is that Attend-and-Excite establishes a certain metric as a loss function and backpropagates the gradient to excite the update of latents, rather than forcing up the corresponding value by direct re-weighting.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/90C7799C-B173-4CF9-B8FA-07BB9958EEA6_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Attend-and-Excite&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Specifically, we first need to select a few text tokens as the target to be enhanced (for example, the 2nd and 5th), calculate their corresponding average cross-attention ($A_t^2,A_t^5$), and then take the smallest peak of them as the loss:&lt;/p&gt;
&lt;p&gt;$$
Loss=max\left(1-max(A_t^2),1-max(A_t^5)\right)
$$&lt;/p&gt;
&lt;p&gt;Finally, the latent is updated according to the gradient as $x_t^{\prime}=x_t-\alpha \nabla_{x_t} Loss$, which encourages the peak cross-attention of all selected text tokens to be as high as possible. Of course, there are several limitations of Attend-and-Excite as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manually selecting&lt;/strong&gt; the index of the enhanced token is not a convenient and perfect solution.&lt;/li&gt;
&lt;li&gt;If and only the excitation in &lt;strong&gt;the middle scale of Unet&lt;/strong&gt; is effective. Because not all scales of cross-attention have semantic information and contribute semantically to the final generation.&lt;/li&gt;
&lt;li&gt;This involves &lt;strong&gt;too many hyperparameter settings&lt;/strong&gt;, such as artificial thresholds that determine the number of backward updates during early iterations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing&#34; class=&#34;icon-inline&#34; id=&#34;dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing&#34;&gt;DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing&lt;a class=&#34;icon-link&#34; href=&#34;#dragdiffusion-harnessing-diffusion-models-for-interactive-point-based-image-editing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2306.14435&#34;&gt;-&amp;gt; Yujun Shi, et al. arXiv 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DragDiffusion is a direct extension of the famous DragGAN. Similarly, DragDiffusion can achieve image editing by dragging multiple points, and the principle behind it is the backward guidance for the generated path by gradient-based update.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/42F9A785-50FF-4340-8262-2EE338776B96_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Attend-and-Excite&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As shown above, in addition to the regular diffusion denoising, the generated path in DragDiffusion contains two other mappings: Motion Supervision and Point Tracking. The loss function of Motion Supervision is shown as follows:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}\left(z_t^k\right)= \sum_{i=1}^n \sum_{q \in \Omega\left(h_i^k, r_1\right)}\lVert F_{q+d_i}\left(z_t^k\right)-\operatorname{sg}\left(F_q\left(z_t^k\right)\right) \rVert_1  +\lambda \lVert \left(z_{t-1}^k-\operatorname{sg}\left(z_{t-1}^0\right)\right) \odot(1-M) \rVert_1
$$&lt;/p&gt;
&lt;p&gt;In simple terms, the former of this loss encourages feature movement at the patch level $F_{\Omega+d_i}(z_t^k) \gets F_{\Omega}(z_t^k)$, while the latter keeps the pixel values outside the mask nearly constant $(1-M)z_t^k \approx (1-M)z_t^{k+1}$.&lt;/p&gt;
&lt;p&gt;Here $sg(\cdot)$ is the stop gradient operator, which makes the pixel values $F_{\Omega}(z_t^k)$ in the original patch are not affected by gradient descent directly. We can calculate the gradient and update the latent as follows:&lt;/p&gt;
&lt;p&gt;$$
z_t^{k+1}=z_t^k-\eta \cdot \nabla_{z_t^k} \mathcal{L}
$$&lt;/p&gt;
&lt;p&gt;Point Tracking is used to rematch the location of the handle points, which is implemented directly using a nearest neighbor search:&lt;/p&gt;
&lt;p&gt;$$
h_i^{k+1}=\underset{q \in \Omega\left(h_i^k, r_2\right)}{\arg \min }\left|F_q\left(z_t^{k+1}\right)-F_{h_i^k}\left(z_t\right)\right|_1
$$&lt;/p&gt;
&lt;p&gt;Although this method directly follows DragGAN&amp;rsquo;s idea, the non-stationary iterative process of the Diffusion Models makes many hyperparameter different, and the author also abandons CFG to avoid large numerical errors.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Two Inpainting Pipelines in Diffusers</title>
      <link>https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/</link>
      <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/analysis/2307inpaintpipeline/</guid>
      <description>&lt;h2 id=&#34;guided-generation&#34; class=&#34;icon-inline&#34; id=&#34;guided-generation&#34;&gt;Guided Generation&lt;a class=&#34;icon-link&#34; href=&#34;#guided-generation&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h2 id=&#34;hybrid-condition-by-fine-tuning&#34; class=&#34;icon-inline&#34; id=&#34;hybrid-condition-by-fine-tuning&#34;&gt;Hybrid-condition by Fine-Tuning&lt;a class=&#34;icon-link&#34; href=&#34;#hybrid-condition-by-fine-tuning&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
</description>
    </item>
    <item>
      <title>Network Design in Stable Diffusion</title>
      <link>https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/analysis/2306sdnetwork/</guid>
      <description>&lt;p&gt;StabilityAI has recently open sourced a series of foundational models for image generation, called Stable Diffusion. Although we know these models are based on &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&#34;&gt;latent diffusion&lt;/a&gt;, there are few reports mention their detailed designs. To facilitate better understanding and potential future improvement, this blog provide some information about the designs of Unet and VAE, which are key components of the magic generation.&lt;/p&gt;
&lt;h2 id=&#34;unet&#34; class=&#34;icon-inline&#34; id=&#34;unet&#34;&gt;Unet&lt;a class=&#34;icon-link&#34; href=&#34;#unet&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/4320ED37-C913-487C-AF89-3563E40F5828_1_201_a.jpeg&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Fig. 1: Overall of the Unet in Stable Diffusion 1.x &amp;amp; 2.x&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As in Fig.1, this Unet is consist of alternating convolution layers and transformer layers, which is the modern design to provide stronger representation than pure-conv or pure-transformer in the vision field. In SD1.x &amp;amp; SD2.x, $l_1=l_2=l_3=l_4=2$.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/B431465F-81A1-404F-9AA0-1474625D7B44_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;600&#34;&gt;
&lt;figcaption&gt;
Fig. 2: Skip-connection in the 2nd scale.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The skip-connection of this Unet is very dense, where the output from each transformer in encoder (downsample side) will be transmitted and concatenated with the corresponding decoder layer&amp;rsquo;s input as in Fig. 2.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/A012E8AE-8131-4160-8158-03EEC2A01A61_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Fig 3: Transformer&amp;rsquo;s Design&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the cross-attention, it is worth noting that the feature from extra condition is regarded as key $K$ and value $V$, and the feature from latent is regarded as query $Q$. This setting follows the classical decoder design in the autoregressive language transformer. At the same time, it is reasonable to use rich information as the base to generate.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BA68D415-0DF9-4CB5-B0F4-59EC7C8033E5_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Fig. 4: Resnet block&amp;rsquo;s design.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The SiLU activation has been widely utilized in this unet, it can provide better capability for nonlinear modeling.&lt;/p&gt;
&lt;p&gt;$$
\text{SiLU}(z)=z*\text{sigmoid}(z)
$$&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/BFE0937A-36CA-4CC4-859F-BFC0F9F3EC51_1_201_a.jpeg&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;figcaption&gt;
Fig. 5: Timestep Embedding (based on positional encoding)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;vae&#34; class=&#34;icon-inline&#34; id=&#34;vae&#34;&gt;VAE&lt;a class=&#34;icon-link&#34; href=&#34;#vae&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The KL-regularized VAE is almost composed of convolutions, expect for one self-attention layer at the bottom. SD1.x and SD2.x have the same structure of VAE (the numbers of channel $c_1,c_2,c_3,c_4=128,256,512,512$), but they don&amp;rsquo;t have the same weight (SD2&amp;rsquo;s VAE might be fine-tuned for higher resolution $768\times 768$).&lt;br&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/CB9453C4-4906-4188-B99B-1186CDEDA771_1_201_a.jpeg&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Fig.6: VAE Encoder&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/774E90D6-50FD-4AD7-800D-D0B903C6549A_1_201_a.jpeg&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Fig.6: VAE Decoder&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;tiled-processing&#34; class=&#34;icon-inline&#34; id=&#34;tiled-processing&#34;&gt;Tiled Processing&lt;a class=&#34;icon-link&#34; href=&#34;#tiled-processing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In inference, VAE decoding often occupies a lot of memory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the tiled mode, the VAE will split the input tensor into tiles to compute encoding in several steps, feed the fully concatenated latent into U-net for denoising, spilt the result again, and finally decode these tiles by a tiled VAE decoder.&lt;/p&gt;
&lt;p&gt;This is useful to keep memory use constant regardless of image size, but the end result of tiled encoding is different from non-tiled encoding. To avoid tiling artifacts, the tiles overlap and are blended together to form a smooth output.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;https://github.com/huggingface/diffusers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;https://github.com/Stability-AI/stablediffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;https://github.com/Stability-AI/generative-models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Inverse Problem × Diffusion -- Part: B</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinverseb/</guid>
      <description>&lt;h2 id=&#34;ddrm&#34; class=&#34;icon-inline&#34; id=&#34;ddrm&#34;&gt;DDRM&lt;a class=&#34;icon-link&#34; href=&#34;#ddrm&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2201.11793&#34;&gt;-&amp;gt; Bahjat Kawar, et al. NeurIPS, 2022.&lt;/a&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018160625.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Illustration of DDRM (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;transformation-via-svd&#34; class=&#34;icon-inline&#34; id=&#34;transformation-via-svd&#34;&gt;Transformation via SVD&lt;a class=&#34;icon-link&#34; href=&#34;#transformation-via-svd&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Similar to &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2021/hash/b5c01503041b70d41d80e3dbe31bbd8c-Abstract.html&#34;&gt;SNIPS&lt;/a&gt;, DDRM consider the singular value decomposition (SVD) of the sampling matrix $H$ as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
y&amp;amp;=Hx+z\
y&amp;amp;=U\Sigma V^\top x+z\
\Sigma^{†} U^{\top}y&amp;amp;=V^\top x+\Sigma^{†} U^{\top}z\
\bar{y}&amp;amp;=\bar{x}+\bar{z}\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Since $U$ is orthogonal matrix, we have $p(U^\top z) = p(z) = \mathcal{N}(0,\sigma^2_y I)$, resulting $\bar{z}^{(i)}=(\Sigma^{†} U^{\top}z)^{(i)} \sim \mathcal{N}(0, \frac{\sigma^2_y}{s_i^2}I)$. So after these, we transform $x$ and $y$ into the same field (&lt;strong&gt;spectral space&lt;/strong&gt;), and these two only differ by the noise $\bar{z}$, which can be drawn as follows:&lt;/p&gt;
&lt;p&gt;$$
q(\bar{y}^{(i)}|x_0)=\mathcal{N}(\bar{x}_0^{(i)},\sigma_y^2/s_i^2 )
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DDRM might be unable to cope with the wild scene, cause the variance $\sigma_y$ is often unknown.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;conditional-diffusion--generation&#34; class=&#34;icon-inline&#34; id=&#34;conditional-diffusion--generation&#34;&gt;Conditional Diffusion &amp;amp; Generation&lt;a class=&#34;icon-link&#34; href=&#34;#conditional-diffusion--generation&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The desired conditional diffusion process should (a) be a tractable Gaussian distribution, (b) employ the known $y$ to construct noisy samples as possible and (c) ensure the original marginal:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
q(x_t|x_0) = q(\bar{x}_t|x_0, y)\cdot q(\bar{y}|x_0)=\mathcal{N}(\bar{x}_0,\sigma_t^2I) \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We note that $q(\bar{y}^{(i)}|x_0)=\mathcal{N}(\bar{x}_0^{(i)},\sigma_y^2/s_i^2 )$, so these conditional processes can be defined as follows:
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018141254.png&#34; alt=&#34;image.png&#34;&gt;
From the perspective of DDIM, we can deduce the corresponding generative process via replacing $x_0$ with &amp;ldquo;the predicted version&amp;rdquo; as follows:
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231018153006.png&#34; alt=&#34;image.png&#34;&gt;
Intuitively, this construction considers different cases for each index of the spectral space. (i) If the corresponding singular value $s_i=0$, then $y$ does not directly provide any information to that index, and the update is similar to regular unconditional generation. (ii) If $s_i &amp;gt;0$, then the updates consider the information provided by $y$, which further depends on whether the measurements’ noise level $\sigma_y/s_i$ in the spectral space is larger than the noise level in the diffusion model or not.&lt;/p&gt;
&lt;p&gt;In the resulting generation, the initial sample carries a few information from $\bar{y}$, then is updated eventually by the guidance of $p_\theta(\bar{x}_t|x_t+1,y)$,  and is recovered to $x_0$ exactly by left multiplying $V$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although this conditional process results in a more complex ELBO objective for training, the authors proof that an optimal solution to DDPM / DDIM can also be an optimal solution to a DDRM problem, under some similar assumptions as in DDIM. So &lt;strong&gt;DDRM can be training-free&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;one-more-thing&#34; class=&#34;icon-inline&#34; id=&#34;one-more-thing&#34;&gt;One more thing&lt;a class=&#34;icon-link&#34; href=&#34;#one-more-thing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;DDRM is applied in super-resolution, deblurring, inpainting, and colorization. There&amp;rsquo;re no much difference of the value space between original $x$ and degraded $y$ in these task. These data are almost all in the perceptible pixel space. So does SVD really work as claimed? This paper lacks relevant ablation study for that. Maybe we should introduce this into more tasks, such as compressive sensing, and see what will happen.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Kawar, Bahjat, Gregory Vaksman, and Michael Elad. &amp;ldquo;SNIPS: Solving noisy inverse problems stochastically.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 34 (2021): 21757-21769.&lt;/li&gt;
&lt;li&gt;Kawar, Bahjat, et al. &amp;ldquo;Denoising diffusion restoration models.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022): 23593-23606.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Inverse Problem × Diffusion -- Part: A</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/</link>
      <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212diffusionforinversea/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;An inverse problem seeks to recover an unknown signal from a set of observed measurements. Specifically, suppose $x\in R^n$ is an unknown signal, and $y\in R^m = Ax+z$ is a noisy observation given by m linear measurements, where the measurement acquisition process is represented by a linear operator $A\in R^{m\times n}$, and $z\in R^n$ represents a noise vector. Solving a linear inverse problem amounts to recovering the signal $x$ from its measurement $y$. Without further assumptions, the problem is ill-defined when $m&amp;lt; n$, so we additionally assume that $x$ is sampled from a prior distribution $p(x)$&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;source from &lt;a href=&#34;http://arxiv.org/abs/2111.08005&#34;&gt;Dr. Yang Song&amp;rsquo;s paper&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As unconditional generative model, Diffusion models (or Score models) can play a prior term in optimization for various ill-posed image inverse problems. Such methods are often labeled as training-free, zero-shot, unsupervised, etc.&lt;/p&gt;
&lt;p&gt;On the other hand, Diffusion models also can be trained as conditional one $s_\theta(x_t,y,t)$, where $y$ is the condition from other mode. However, this will consume a lot of resources both in the computing power and the collection of paired data $\lbrace x_i,y_i\rbrace$.&lt;/p&gt;
&lt;h2 id=&#34;medical-score-sde&#34; class=&#34;icon-inline&#34; id=&#34;medical-score-sde&#34;&gt;Medical Score-SDE&lt;a class=&#34;icon-link&#34; href=&#34;#medical-score-sde&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2111.08005&#34;&gt;-&amp;gt; Yang Song, et al. in ICLR, 2022.&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017172654.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The medical imaging problem (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;step1-perturbing-measurements&#34; class=&#34;icon-inline&#34; id=&#34;step1-perturbing-measurements&#34;&gt;step1: Perturbing measurements&lt;a class=&#34;icon-link&#34; href=&#34;#step1-perturbing-measurements&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Since $x_t=\alpha(t)x_0+\beta(t)z$, and we set $y_t=Ax_t+\alpha(t)\epsilon$, so we have $y_t=\alpha(t)y+\beta(t)Az$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Smart and reasonable assumption for the coefficient $+\alpha(t)\epsilon$ to avoid the subsequent diffusion of random measurement noise $\epsilon$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;step2-generation-with-score-sde&#34; class=&#34;icon-inline&#34; id=&#34;step2-generation-with-score-sde&#34;&gt;step2: Generation with Score-SDE&lt;a class=&#34;icon-link&#34; href=&#34;#step2-generation-with-score-sde&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Using the Euler-Maruyama sampler, the generative process given by:&lt;/p&gt;
&lt;p&gt;$$
x_{t-\Delta t} = x_t-f(t)x_t\Delta t+g(t)^2s_\theta(x_t,t)\Delta t+g(t)\sqrt{\Delta t}z
$$&lt;/p&gt;
&lt;p&gt;where $s_\theta(\cdot)$ is the trained score model, $\Delta t = 1/N$ is the set discrete time, and $z\sim \mathcal{N}(0,1)$.&lt;/p&gt;
&lt;h3 id=&#34;step3-consistent-guidance&#34; class=&#34;icon-inline&#34; id=&#34;step3-consistent-guidance&#34;&gt;step3: Consistent Guidance&lt;a class=&#34;icon-link&#34; href=&#34;#step3-consistent-guidance&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017164345.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
The pipeline of medical Score-SDE&lt;/figcaption&gt;
&lt;/figure&gt;

We set the guided intermediate result as $x_t^{\prime}$, For simultaneously minimizing the distance between $x_t$ and $x_t^{\prime}$, and the distance between $x_t^{\prime}$ and the hyperplane $\lbrace x\in R^n| Ax=y_t \rbrace$, we build an  optimization problem as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x_t^{\prime}&amp;amp;=\arg \min_{v\in R^n}\lbrace (1-\lambda)\Vert v-x_t \Vert_{T}^2 + \min_{u\in R^n} \lambda \Vert v-u \Vert_{T}^2 \rbrace \
s.t.\quad Au&amp;amp;=y_t
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which has the closed-form solution as:&lt;/p&gt;
&lt;p&gt;$$
x_t^{\prime}=T^{-1}[\lambda \Lambda \mathcal{P} ^{-1}(\Lambda)y_t+(1-\lambda)\Lambda Tx_t+(1-\Lambda)Tx_t]
$$&lt;/p&gt;
&lt;p&gt;When the measurement process is noisy, we can choose $0&amp;lt;\lambda&amp;lt;1$ to allow slackness in $Ax_t^{\prime}=y_t$ (more affected by $p(x)$). When the measurement process contains no noise, the authors chosse $\lambda=1$ at the last sampling step to guarantee $Ax_0^{\prime}=y$.&lt;/p&gt;
&lt;h2 id=&#34;score-mri&#34; class=&#34;icon-inline&#34; id=&#34;score-mri&#34;&gt;Score-MRI&lt;a class=&#34;icon-link&#34; href=&#34;#score-mri&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1361841522001268&#34;&gt;-&amp;gt; Hyungjin Chung, et al.  Medical Image Analysis, 2022.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Similar to the above, Score-MRI inserts a more simple consistency mapping after every &amp;ldquo;Predictor&amp;rdquo; and &amp;ldquo;Corrector&amp;rdquo; iteration.&lt;/p&gt;
&lt;p&gt;$$
x_i=x_i+\lambda A^\ast(y-Ax_i)=(I-\lambda A^\ast A)x_i+A^\ast y
$$&lt;/p&gt;
&lt;p&gt;where $A^\ast$ denotes the Hermitian adjoint, and $+\lambda A^\ast (y-Ax_i)$ can be viewed as a rectification for minimizing the distance between $y$ and $Ax_i$. In the hybridtype Algorithm 5 (complex domain and multi-coils for MRI), the authors start with $\lambda = 1.0$ in the first iteration, and linearly decrease the value to $\lambda=0.2$ at the last iteration.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1706.00051&#34;&gt;GANCS&lt;/a&gt; has an similar rectification $(I-\Phi^{†}\Phi)x+\Phi^{†}y$, where pseudo-inverse $\Phi^{†}$ satisfies $\Phi\Phi^{†}\Phi=\Phi$, but without the scaling factor $\lambda$.  In this paper, the authors view $(I-\Phi^{†}\Phi)x$ as a projection onto the &lt;a href=&#34;https://dx.doi.org/10.1088/1361-6420/aaf14a&#34;&gt;nullspace&lt;/a&gt; of $\Phi$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017182641.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;ddnm&#34; class=&#34;icon-inline&#34; id=&#34;ddnm&#34;&gt;DDNM&lt;a class=&#34;icon-link&#34; href=&#34;#ddnm&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2212.00490&#34;&gt;-&amp;gt; Yinhuai Wang, et al. in ICLR, 2023.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;noise-free&#34; class=&#34;icon-inline&#34; id=&#34;noise-free&#34;&gt;Noise-free&lt;a class=&#34;icon-link&#34; href=&#34;#noise-free&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Similar to &lt;a href=&#34;http://arxiv.org/abs/1706.00051&#34;&gt;GANCS&lt;/a&gt;, DDNM adopts a rectified estimation as:&lt;/p&gt;
&lt;p&gt;$$
x_{0|t}^{\prime}=A^{†}y+(I-A^{†}A)x_{0|t}
$$&lt;/p&gt;
&lt;p&gt;where $x_{0|t}$ is the predicted version at timestep $t$. Based on &lt;a href=&#34;http://arxiv.org/abs/2010.02502&#34;&gt;DDIM&lt;/a&gt;, the generative process can be driven by $x_{t-1}\sim \mathcal{p}(x_{t-1}|x_t,x_{0|t}^{\prime})$. So it might provide a more reliable rectification to human-perceptible $x_{0|t}$, rather than noisy $x_t$ like in &lt;a href=&#34;http://arxiv.org/abs/2010.02502&#34;&gt;Score-MRI&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although this paper mentioned that this consistency mapping was inspired by &lt;strong&gt;range-null space decomposition&lt;/strong&gt; $x=A^{†}Ax+(I-A^{†}A)x$, but it seems to contradict with the subsequent DDNM+ for the noisy inverse problem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;noisy&#34; class=&#34;icon-inline&#34; id=&#34;noisy&#34;&gt;Noisy&lt;a class=&#34;icon-link&#34; href=&#34;#noisy&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$$
x_{0|t}^{\prime}=A^†y+(I−A^†A)x_{0|t} =x_{0|t}-A^†(Ax_{0|t}-y)
$$&lt;/p&gt;
&lt;p&gt;In short, the authors employ two scale factors $\Sigma_t$ &amp;amp; $\Phi_t$ into range-space correction &amp;amp; generative variance, respectively.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
x_{0|t}^{\prime}&amp;amp;= x_{0|t}-\Sigma_tA^†(Ax_{0|t}-y)\\
p^{\prime}(x_{t-1}|x_t,x_{0|t}^{\prime})&amp;amp;=\mathcal{N}(x_{t-1};\mu_t(x_t, x_{0|t}^{\prime}),\Phi_t I)
\end{aligned}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the performance gains from these factor are not clear, due to the absence of ablations experiments. It looks like the &amp;ldquo;Time-Travel&amp;rdquo; (a kind of redundant computing) helped a lot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231017212649.png&#34; loading=&#34;lazy&#34; width=&#34;300&#34;&gt;
&lt;figcaption&gt;
Time-Travel (source from paper)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yang Song and others, ‘Solving Inverse Problems in Medical Imaging with Score-Based Generative Models’  in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2022.&lt;/li&gt;
&lt;li&gt;Hyungjin Chung and Jong Chul Ye, ‘Score-Based Diffusion Models for Accelerated MRI’, &lt;em&gt;Medical Image Analysis&lt;/em&gt;, 80 (2022), 102479.&lt;/li&gt;
&lt;li&gt;Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, ‘Deep Null Space Learning for Inverse Problems: Convergence Analysis and Rates’, &lt;em&gt;Inverse Problems&lt;/em&gt;, 35.2 (2019), 025008.&lt;/li&gt;
&lt;li&gt;Morteza Mardani and others, ‘Deep Generative Adversarial Networks for Compressed Sensing Automates MRI’ arXiv, 2017.&lt;/li&gt;
&lt;li&gt;Yinhuai Wang, Jiwen Yu, and Jian Zhang, ‘Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model’ in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2023.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>DDPM and Early Variants</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212ddpm/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212ddpm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Although Diffusion Model is a new generative framework, it still has many shades of other methods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013140424.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Bayes&amp;rsquo; rule is all you need&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;generation--diffusion&#34; class=&#34;icon-inline&#34; id=&#34;generation--diffusion&#34;&gt;Generation &amp;amp; Diffusion&lt;a class=&#34;icon-link&#34; href=&#34;#generation--diffusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Just like GANs realized the implicit generation through the mapping from a random gaussian vector to a natural image, Diffusion Model is doing the same thing, by multiple mappings, though. This generation can be defined as the following &lt;strong&gt;Markov chain&lt;/strong&gt; with learnable Gaussian transitions:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
p_\theta\left(x_0\right)=\int p_\theta\left(x_{0: T}\right) \mathrm{d} x_{1: T}\\
p_\theta\left(x_{0: T}\right):=p_\theta\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \vert x_t\right)\\
p_\theta\left(x_{t-1} \vert x_t\right):=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_\theta(x_t,t))
\end{align}
$$&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013150347.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;Markov chain: What happens next depends only on the state of affairs now. So we have $p(x_{t-1}\vert x_{t:T})=p(x_{t-1}\vert x_{t})$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similar to VAE, we can use the posterior $q(x_{1:t} \vert x_0)$ to do the estimation for $\theta$. The difference is that $x_1,\dots,x_T$ are the latents of the same size as $x_0$, and the diffusion process (c.t. VAE encoder) $q(x_{1:T} \vert x_0)$ is fixed to a Markov chain without any learnable parameters, which can be designed as Gaussian transitions parameterized by a decreasing sequence $\alpha_{1:T}\in [0,1]^T$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
q(x_{1:T} \vert x_0) := \prod_{t=1}^T q\left(x_{t} \vert x_{t-1}\right) \\
q(x_t \vert x_{t-1}):=N(\frac{\sqrt{\alpha_t}}{\sqrt{\alpha_{t-1}}}x_{t-1}, (1-\frac{\alpha_t}{\alpha_{t-1}})I)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;A nice property of the above design (thank to Gauss.) is that it admits sampling $x_t$ at arbitrary timestep $t$:&lt;/p&gt;
&lt;p&gt;$$
q(x_t\vert x_0)=N(x_t;\sqrt{\alpha_t}x_0, (1-\alpha_t)I)
$$&lt;/p&gt;
&lt;h2 id=&#34;training-objective&#34; class=&#34;icon-inline&#34; id=&#34;training-objective&#34;&gt;Training Objective&lt;a class=&#34;icon-link&#34; href=&#34;#training-objective&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We can use the variational lower bound (appeared in VAE) to maximize the negative log-likelihood:&lt;/p&gt;
&lt;p&gt;$$
\max_{\theta}E_{q}[\log{p_\theta(x_0)}]\leq \max_{\theta}E_{q}[\log{p_{\theta} (x_{0:T})}-\log{q(x_{1:T} \vert x_0)}]
$$&lt;/p&gt;
&lt;p&gt;which also can be driven by Jensen’s inequality as in &lt;a href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34; title=&#34;Lil&#39;log&#34;&gt;Lil&amp;rsquo;log&lt;/a&gt;. And we can further rewrite this object as:&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013165753.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;Using Bayes&amp;rsquo; rule, we can deduce the fact that $q(x_{t-1}\vert x_t, x_0)$ is also a gaussian distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where $L_T$ is constant. Discussing $L_{t-1}$ is one of the key contributions of DDPMs.  &lt;strong&gt;If generative variances is all fixed  $\Sigma_t = \sigma^2_t$&lt;/strong&gt;, using parameterization (fit distribution $\to$ fit mean $\to$ predict noise) and reweighting based on the empirical results, we can simplify this objective as follows:&lt;/p&gt;
&lt;p&gt;$$
L_t=E_{x_0\sim q, \epsilon\sim N(0,1)}\left[|| \epsilon_\theta(\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\epsilon, t)-\epsilon {||}_2^2 \right]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NCSN vs DDPM, different ways lead to almost the same objective!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For last $L_0$, DDPMs treat it as an independent discrete decoder derived from $N(x_0;\mu_{\theta}(x_1,1), 0)$, so it can be trained by the same objective as $L_t$. Notice that this last generative process is set to noiseless to ensure the lossless codelength of discrete data.&lt;/p&gt;
&lt;p&gt;At the end, we can realize the efficient training by optimizing random terms of $L_t$ with stochastic gradient descent (Alg. 1). Correspondingly, the sampling can be exported by $p_\theta(x_{t-1}\vert x_t)$ using predicted $\epsilon_\theta(\cdot)$ (Alg. 2).&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013181724.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;ddpm&#34; class=&#34;icon-inline&#34; id=&#34;ddpm&#34;&gt;DDPM+&lt;a class=&#34;icon-link&#34; href=&#34;#ddpm&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality&#34; class=&#34;icon-inline&#34; id=&#34;finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality&#34;&gt;Finding 1: Why fixing $\sigma^2$ to $\beta$ or$\tilde{\beta}$ achieve similar sample quality?&lt;a class=&#34;icon-link&#34; href=&#34;#finding-1-why-fixing-sigma2-to-beta-ortildebeta-achieve-similar-sample-quality&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013202424.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;As $\beta \approx \tilde{\beta}$ In the early process, the perceptual details generated from these two is very similar. So the other constants might not matter at all for sample quality due to decreasing nature $\frac{\beta_t}{\beta_{t-1}} \to 0$.&lt;/p&gt;
&lt;h3 id=&#34;finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly&#34; class=&#34;icon-inline&#34; id=&#34;finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly&#34;&gt;Finding 2: The diffusion samples from linear schedule lose information very quickly.&lt;a class=&#34;icon-link&#34; href=&#34;#finding-2-the-diffusion-samples-from-linear-schedule-lose-information-very-quickly&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204426.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013204404.png&#34; loading=&#34;lazy&#34; width=&#34;500&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;In the diffusion process, these samples will soon become the noise without any information. Even if the early generation is skipped (~20%), the quality does not get much worse. It suggests that so much full noisy samples might do not contribute to generation.&lt;/p&gt;
&lt;h3 id=&#34;improvements&#34; class=&#34;icon-inline&#34; id=&#34;improvements&#34;&gt;Improvements&lt;a class=&#34;icon-link&#34; href=&#34;#improvements&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Learnable variances with an interpolation between $\beta$ and $\tilde{\beta}$, driven by loss $+ \lambda L_{vlb}$.&lt;/li&gt;
&lt;li&gt;Cosine schedule has a linear drop off in the middle of the process, while changing very little near the start and the end.&lt;/li&gt;
&lt;li&gt;Resampling $L_{vlb}$ to make training stable (like a kind of dynamic weighting)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ddim&#34; class=&#34;icon-inline&#34; id=&#34;ddim&#34;&gt;DDIM&lt;a class=&#34;icon-link&#34; href=&#34;#ddim&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;more-free-diffusion-chain&#34; class=&#34;icon-inline&#34; id=&#34;more-free-diffusion-chain&#34;&gt;More free diffusion chain&lt;a class=&#34;icon-link&#34; href=&#34;#more-free-diffusion-chain&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In DDIM, the authors introduce a extended version $q(x_{t-1}\vert x_t, x_0)$, which has the same marginal noise distribution $q(x_{t}\vert x_0)$：&lt;/p&gt;
&lt;p&gt;$$
q_{\sigma}(x_{t-1} \vert x_t, x_0) = N(x_{t-1}; \sqrt{\alpha_{t-1}} x_0 + \sqrt{1 - \alpha_{t-1} - \sigma_t^2} \frac{x_t - \sqrt{\alpha_t} x_0}{\sqrt{1 - \alpha_t}}, \sigma_t^2 I)
$$&lt;/p&gt;
&lt;p&gt;Therefore, the corresponding generative process can be exported as:
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231013230807.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

If we set $\sigma_t^2=\tilde{\beta}_t$,  the diffusion process becomes Markovian, and the generative process becomes DDPM. And if $\sigma_t=0$, there is a deterministic generation, called DDIM.&lt;/p&gt;
&lt;h3 id=&#34;acceleration-via-subsampling&#34; class=&#34;icon-inline&#34; id=&#34;acceleration-via-subsampling&#34;&gt;Acceleration via subsampling&lt;a class=&#34;icon-link&#34; href=&#34;#acceleration-via-subsampling&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In the generation, we sample a subset of S steps ${\tau_1,\dots,\tau_S}$ to form a new chain as:&lt;/p&gt;
&lt;p&gt;$$
q_{\sigma, \tau}(x_{\tau_{i-1}} \vert x_{\tau_i}, x_0) = N(x_{\tau_{i-1}}; \sqrt{\alpha_{\tau_{i-1}}} x_0 + \sqrt{1 - \alpha_{\tau_{i-1}} - \sigma_t^2} \frac{x_{\tau_i} - \sqrt{\alpha_{\tau_i}} x_0}{\sqrt{1 - \alpha_{\tau_i}}}, \sigma_{\tau_i}^2 I)
$$&lt;/p&gt;
&lt;p&gt;which can still provide high-quality samples using a much fewer number of steps.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Ho, Ajay Jain, and Pieter Abbeel, ‘Denoising Diffusion Probabilistic Models’, in &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2020.&lt;/li&gt;
&lt;li&gt;Alexander Quinn Nichol and Prafulla Dhariwal, ‘Improved Denoising Diffusion Probabilistic Models’, in &lt;em&gt;Proceedings of the 38th International Conference on Machine Learning&lt;/em&gt;, 2021.&lt;/li&gt;
&lt;li&gt;Jiaming Song, Chenlin Meng, and Stefano Ermon, ‘Denoising Diffusion Implicit Models’, in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2021.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Image Generation based on Score Model</title>
      <link>https://yuhaoo00.github.io/posts/snapshots/2212ncsn/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://yuhaoo00.github.io/posts/snapshots/2212ncsn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Both likelihood-based methods and GAN methods have have some intrinsic limitations. Learning and estimating Stein score (the gradient of the log-density function $\nabla_{ x} \log p_{\text {data }}( x)$) may be a better choice than learning the data density directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;score-estimation-for-training&#34; class=&#34;icon-inline&#34; id=&#34;score-estimation-for-training&#34;&gt;Score Estimation (for training)&lt;a class=&#34;icon-link&#34; href=&#34;#score-estimation-for-training&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We want to train a network $s_{\theta}(x)$ to estimate $\nabla_{ x} \log p_{\text {data }}( x)$, but how can we get the ground truth (the real score)? In this paper, the objective $\frac{1}{2} E_{p_{\text{data}}} \lbrack\lVert s_{\theta}( x)-\nabla_{ x} \log p_{\text{data}}( x)\rVert_2^2\rbrack$ is equivalent to the following by Score Matching:&lt;/p&gt;
&lt;p&gt;$$
E_{p_{\text{data}}}\left[tr(\nabla_{ x}s_{\theta}( x))+\frac{1}{2}\left|s_{\theta}( x))\right|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;Unfortunately, it is not easy to compute $tr(\cdot)$ for a large-scale problem. Both &lt;strong&gt;Denoising score matching&lt;/strong&gt; and &lt;strong&gt;Sliced score matching&lt;/strong&gt; are popular methods to deal with this situation. But the sliced one requires 4x computations due to the forward mode auto-differentiation. Instead, Denoising score matching try to estimate the score of noise-perturbed distribution ($q_\sigma$) as follows:&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{2} E_{q_\sigma(\tilde{ x} \mid  x) p_{\text {data }}( x)}\left[\left| s_{\theta}(\tilde{ x})-\nabla_{\tilde{ x}} \log q_\sigma(\tilde{ x} \mid  x)\right|_2^2\right]
$$&lt;/p&gt;
&lt;p&gt;Minimizing this objective, we can get $s_{\theta}( x)=\nabla_{ x} \log q_{\sigma}( x)$. And if the noise is small enough $q_\sigma( x) \approx p_{\text {data }}( x)$, we will get $s_{\theta}( x)=\nabla_{ x} \log q_\sigma( x) \approx \nabla_{ x} \log p_{\text {data }}( x)$.
As $q_\sigma(\tilde{x}| x)$ can be defined as a known simple distribution, this minimization is easier than regular score matching. In this paper, they choose $q_\sigma(\tilde{x}| x) =N\left(\tilde{x} \mid x, \sigma^2 I\right)$, which leads to:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta ; \sigma) \triangleq \frac{1}{2} E_{p_{\text {data }}(x)} E_{\tilde{x} \sim N\left(x, \sigma^2 I\right)}\left[\left|\mathbf{s}_{\theta}(\tilde{x}, \sigma)+\frac{\tilde{x}-x}{\sigma^2}\right|_2^2\right]
$$&lt;/p&gt;
&lt;h2 id=&#34;langevin-dynamics-for-inference&#34; class=&#34;icon-inline&#34; id=&#34;langevin-dynamics-for-inference&#34;&gt;Langevin dynamics (for inference)&lt;a class=&#34;icon-link&#34; href=&#34;#langevin-dynamics-for-inference&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;How can we do sampling from $p_{\text {data }}( x)$ when we get a nice estimation of $\nabla_{ x} \log p_{\text {data }}( x)$? Just along the direction of this gradient? Yes, but plus more tricks.&lt;/p&gt;
&lt;p&gt;$$
x_t = x_{t-1}+\frac{\epsilon}{2} \nabla_{x} \log p\left(x_{t-1}\right)+\sqrt{\epsilon} z_t
$$&lt;/p&gt;
&lt;p&gt;The Annealed Langevin dynamics, which is based on assumptions for particle motion, can provide &lt;em&gt;more stable distribution&lt;/em&gt;! Briefly, the random noise term $\mathbf{z}_t \thicksim N(0,1)$ simulates the random motion of particles. With gradual annealing (the step size $\epsilon \to 0$), the iterative $x_t$ will approach the distribution $p(x)$.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012150556.png&#34; loading=&#34;lazy&#34; width=&#34;400&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;practical-challenges&#34; class=&#34;icon-inline&#34; id=&#34;practical-challenges&#34;&gt;Practical Challenges&lt;a class=&#34;icon-link&#34; href=&#34;#practical-challenges&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;the-manifold-hypothesis&#34; class=&#34;icon-inline&#34; id=&#34;the-manifold-hypothesis&#34;&gt;The manifold hypothesis&lt;a class=&#34;icon-link&#34; href=&#34;#the-manifold-hypothesis&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The score is &lt;em&gt;undefined&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The estimation by Score Matching &lt;em&gt;isn&amp;rsquo;t consistent&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data&lt;/strong&gt; with a small Gaussian noise (make the support of data distribution is the whole space), the loss driven by SlicedScoreMatching (fast &amp;amp; faithful) will converge (Fig. 1).&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192631.png&#34; loading=&#34;lazy&#34;&gt;
&lt;figcaption&gt;
Figure 1: Left: Sliced score matching (SSM) loss w.r.t. iterations. No noise is added to data. Right: Same but data are perturbed with N(0, 0.0001).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;low-data-density-regions&#34; class=&#34;icon-inline&#34; id=&#34;low-data-density-regions&#34;&gt;Low data density regions&lt;a class=&#34;icon-link&#34; href=&#34;#low-data-density-regions&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Our training is based on the data in high density ($\thicksim p_{\text {data }}( x)$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The estimation in low density regions is &lt;em&gt;inaccurate.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Regular Langevin Dynamics &lt;em&gt;can&amp;rsquo;t be able to correctly recover&lt;/em&gt; the relative weights of the multi-modal distribution &lt;em&gt;in reasonable time.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;perturb the data by using multiple noise levels&lt;/strong&gt; (anneal down), we can fill the low density regions.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://image-1300968464.cos.ap-shanghai.myqcloud.com/Obsidian/20231012192901.png&#34; loading=&#34;lazy&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;So naturally, we can kill three birds with Denoising Score Matching! Large-scale estimation, Whole space support, and Filling low density regions.
(Indeed, the authors emphasize that empirically sliced score matching can train NCSNs as well as denoising score matching.)&lt;/p&gt;
&lt;h2 id=&#34;one-more-thing&#34; class=&#34;icon-inline&#34; id=&#34;one-more-thing&#34;&gt;One More Thing&lt;a class=&#34;icon-link&#34; href=&#34;#one-more-thing&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the score matching, the authors approximately have $\left| s_{\theta}( x, \sigma)\right|_2 \propto 1 / \sigma$, so they choose $\lambda (\sigma) = \sigma^2$ to make the order magnitude of loss under various noise levels roughly the same, and independent of $\sigma$.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}\left(\theta ;\lbrace\sigma_i \rbrace_{i=1}^{L}\right) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda\left(\sigma_i\right) \ell\left(\theta ; \sigma_i\right)
$$&lt;/p&gt;
&lt;p&gt;Correspondingly, in the langevin dynamics, they choose $\alpha_i \propto \sigma^2$ to make the order magnitude of &amp;ldquo;signal-to-noise ratio&amp;rdquo; independent of $\sigma$.&lt;/p&gt;
&lt;h1 id=&#34;references&#34; class=&#34;icon-inline&#34; id=&#34;references&#34;&gt;References&lt;a class=&#34;icon-link&#34; href=&#34;#references&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
  &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt;
  &lt;path d=&#34;M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5&#34; /&gt;
  &lt;path d=&#34;M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5&#34; /&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yang Song and Stefano Ermon, ‘Generative Modeling by Estimating Gradients of the Data Distribution’, in &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2019.&lt;/li&gt;
&lt;li&gt;Yang Song and others, ‘Score-Based Generative Modeling through Stochastic Differential Equations’, in &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;, 2021.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    </channel>
</rss>
